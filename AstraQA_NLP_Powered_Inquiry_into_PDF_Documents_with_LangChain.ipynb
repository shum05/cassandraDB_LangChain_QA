{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPYQkQNDSpg7jk2T+yLzO9E",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shum05/cassandraDB_LangChain_QA/blob/main/AstraQA_NLP_Powered_Inquiry_into_PDF_Documents_with_LangChain.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# AstraQA: NLP-Powered Inquiry into PDF Documents with LangChain\"\n",
        "## Project Description\n",
        "This innovative project showcases the power of AstraDB, LangChain, and Vector Search in transforming traditional PDF documents into dynamic sources of information. Seamlessly integrated, our solution allows users to pose questions, receiving accurate and contextually relevant answers drawn from PDF content. Experience the synergy of AstraDB's serverless Cassandra with Vector Search, coupled with LangChain's natural language processing capabilities, as we redefine the way you interact with textual data. Unleash the potential of smart document interrogation through this question-answering journey."
      ],
      "metadata": {
        "id": "MSu_ltRLnt2u"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "enqEkmKQowOt"
      },
      "source": [
        "### **1. Install the required dependencies:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "Uk0qUhJUQrkO"
      },
      "outputs": [],
      "source": [
        "!pip install -q cassio datasets langchain openai tiktoken"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Cassio:** a library used for interacting with Apache Cassandra, to be used for integrating with Astra DB (AstraDB)and it facilitates the integration between your Python code and the AstraDB instance, enabling you to interact with and query the database seamlessly.\n",
        "\n",
        "**Datasets** (Hugging Face):library, often associated with Hugging Face, is a powerful tool for managing and working with various datasets. It provides a standardized interface to access and load datasets for NLP and ML tasks.\n",
        "\n",
        "**Tiktoken:**a library designed to count the number of tokens in a text string without making API calls,useful when working with models that have token-based limits, such as OpenAI's language models.\n"
      ],
      "metadata": {
        "id": "_dirwrBsEG3_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install PyPDF2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WIs76OPQ6JyD",
        "outputId": "c1de1e4d-a27c-4413-ff1b-9cb544490ec3"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: PyPDF2 in /usr/local/lib/python3.10/dist-packages (3.0.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "!pip install nltk\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2HXt0ujUhh3h",
        "outputId": "7806439a-5af9-47c2-9b77-aef4a2c4d49c"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2023.6.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iCeMxTrkkYOC",
        "outputId": "060bf205-bbe2-4768-d730-fcdd457116cd"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XQQN-L2J4Rpq"
      },
      "source": [
        "### **2. Import the packages you'll need:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "V4qBIihE4Rpq"
      },
      "outputs": [],
      "source": [
        "# LangChain components\n",
        "from langchain.vectorstores.cassandra import Cassandra\n",
        "from langchain.indexes.vectorstore import VectorStoreIndexWrapper\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "from langchain.llms import OpenAI\n",
        "from langchain.embeddings import OpenAIEmbeddings\n",
        "\n",
        "# dataset retrieval with Hugging Face\n",
        "from datasets import load_dataset\n",
        "\n",
        "# CassIO for Astra DB integration in LangChain\n",
        "import cassio\n",
        "\n",
        "# to read the content of a PDF file\n",
        "from PyPDF2 import PdfReader\n",
        "from typing_extensions import Concatenate\n",
        "\n",
        "# word_tokenize to split the input text into words\n",
        "from nltk.tokenize import word_tokenize"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **purpose of each tool or module imported:**\n",
        "These tools collectively form a pipeline for querying and analyzing PDF documents using Astra DB, LangChain, and OpenAI-powered language models.\n",
        "\n",
        "**Cassandra:** A vector store implementation designed to work with Cassandra. It allows for storing and retrieving vectorized representations of text data.\\\n",
        "**VectorStoreIndexWrapper:** A wrapper for a vector store that provides additional indexing functionality. It aids in efficiently searching and retrieving relevant information from the vector store.\\\n",
        "**CharacterTextSplitter:** A tool for splitting text into chunks based on characters. In this specific use case, it is configured to split text from a PDF file into manageable chunks.\\\n",
        "**OpenAI** (from langchain.llms): An implementation of a Language Model Microservice (LLM) using OpenAI. It allows for language-related tasks, such as generating responses or embeddings, using OpenAI's language models.\\\n",
        "**OpenAIEmbeddings:** A tool for obtaining embeddings (vector representations) of text using OpenAI's language models. Embeddings capture semantic information about the input text.\\\n",
        "**load_dataset:** A function from the Hugging Face datasets library that facilitates loading and working with various datasets. It's commonly used in natural language processing (NLP) projects for data retrieval and exploration.\\\n",
        "**cassio:**: A module used for initializing and managing the connection to an Astra DB (Database), which is a Cassandra-based NoSQL database. It provides the integration between CassIO and AstraDB in the LangChain project.\\\n",
        "**PdfReader** : A tool for reading the content of PDF files. In the given code, it is used to extract text from a PDF file.\n"
      ],
      "metadata": {
        "id": "0OeMizXRsc1g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **3. Seamless Integration:** Configuring AstraDB and OpenAI Credentials"
      ],
      "metadata": {
        "id": "9JqJbey9t2px"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "OPENAI_API_KEY = \"xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\"\n",
        "\n",
        "ASTRA_DB_ID = \"0xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxb\"\n",
        "ASTRA_DB_APPLICATION_TOKEN = \"AstraCS:xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxd\"\n"
      ],
      "metadata": {
        "id": "hlLazt5uq95u"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **4. Google Drive Integration for Seamless File Access in Colab**\n",
        "- prompted to authenticate and provide an authorization code"
      ],
      "metadata": {
        "id": "DgQrk6NP2xIP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oq7emQLv3IGc",
        "outputId": "049c5390-e939-40a8-8e85-36431a8876e1"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Provide paths to PDF files\n",
        "pdf_file_path1 = '/content/drive/MyDrive/NLP_PDF_Files/about_NLP.pdf'\n",
        "pdf_file_path2 = '/content/drive/MyDrive/NLP_PDF_Files/note_on_NLP.pdf'\n",
        "\n",
        "# Read PDF files\n",
        "pdf_reader1 = PdfReader(pdf_file_path1)\n",
        "pdf_reader2 = PdfReader(pdf_file_path2)\n",
        "\n",
        "# Extract text and merge\n",
        "raw_text1 = ''.join(page.extract_text() for page in pdf_reader1.pages)\n",
        "raw_text2 = ''.join(page.extract_text() for page in pdf_reader2.pages)\n",
        "\n",
        "# Merge the text from both PDF files\n",
        "separator = \" ========== SPACE BETWEEN FILES ========== \"\n",
        "merged_raw_text = f\"{raw_text1}\\n\\n{separator}{separator}\\n\\n{raw_text2}\"\n"
      ],
      "metadata": {
        "id": "36Fb4h8Y3OLa"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "merged_raw_text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        },
        "id": "8pDbgRsO6WsU",
        "outputId": "50e68331-cf4f-428a-f39f-80e84ee699a9"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'1Natural Language Processing\\nCS 6320  \\nLecture 1\\nIntroduction to NLP\\nInstructor: Sanda HarabagiuDefinition\\n•NLP is concerned with the computational techniques used for \\nprocessing human language. It creates and implements computer \\nmodels  for the purpose of performing various natural language tasks. \\n•These tasks include :\\n•Mundane applications , e.g. word counting, spell checking, automatic \\nhyphenation\\n•Cutting edge applications , e.g. automated question answering on the Web, \\nbuilding NL interfaces to databases, machine translation,  and others.\\n•What distinguished these applications from other data processing \\napplications is their use of knowledge of language .\\n•NLP is playing an increasing role in curbing the information  explosion \\non Internet and corporate America.  AI vs. NLP\\n•People refer to many AI techniques – like Chat GPT, \\nwhich are in fact novel NLP methods using GPT3 in an \\ninteractive mode.\\n•GPT4  and GPT3  are Large  Language  Models (LLMs)\\n•LLMs have recently revolutionized NLP\\n•BERT  was another  important  NLP milestone\\n•NLP nowadays uses Deep Learning techniques\\nBut, the understanding  of how language  works, and \\nwhat aspects of natural language processing\\nwe need  to be aware  of still requires  the comprehension \\nof classical NLP technique sRelated areas\\n•NLP is a difficult, and largely unsolved problem. One reason for this \\nis its multidisciplinary  nature: \\n•Linguistics  : How  words, phrases, and  sentences are \\nformed.  \\n•Psycholinguistics  : How people understand and \\ncommunicate  using human language. \\n•Cognitive Modeling:  Deals with models and   computational \\naspects of NL ( e.g. algorithms). Related areas\\n•Philosophy : relates to the semantics of language;  notion of  \\nmeaning, how words identify objects. NLP requires  considerable \\nknowledge about the world. \\n•Computer science :  model formulation and implementation  using \\nmodern methods.  \\n•Artificial intelligence : issues related to knowledge representation \\nand reasoning.\\n•Statistics:  many NLP problems are modeled using probabilistic \\nmodels.\\n•Machine learning:  automatic learning of rules and procedures \\nbased on lexical, syntactic and semantic features.\\n•NL Engineering : implementation of large, realistic systems.  \\nModern software development methods play an important role.Applications of NLP\\n•Text - based applications : \\n•Finding documents on certain topics (document classification)  \\n•Information extraction: extract information related events, relations, \\nconcepts \\n•Complete understanding of texts:  requires a deep structure analysis, \\n•Reading comprehension\\n•Translation from a language to another,  \\n•Summarization,  \\n•Knowledge acquisition,\\n•Question -Answering     \\n•Dialogue - based applications  (involve human - machine \\ncommunication): \\n•Conversational Agents\\n•Tutoring systems \\n•Problem solving.   \\n•Speech processingBasic levels of language processing 1/2\\n1.Phonetic   - how words are related to the sounds that realize \\nthem. Essential for speech processing.  \\n2.Morphological Knowledge  - how words are constructed : e.g  \\nfriend, friendly, unfriendly, friendliness. \\n3.Syntactic Knowledge  - how words can be put together to form  \\ncorrect sentences, and the role each word plays in the sentence. \\ne.g.:\\n          John ate the cake.  \\n1.Semantic Knowledge  - Words and sentence meaning: \\n          They saw a log.\\n          They saw a log yesterday.\\n          He saws a log.Basic levels of language processing 2/2\\n5.Pragmatic  Knowledge - how sentences are used in different \\nsituations(or contexts).  \\n          Mary grabbed her umbrella.   \\n     a)  It is a cloudy day.  \\n     b)  She was afraid of dogs.\\n5.Discourse Knowledge  - how the meaning of words and \\nsentences is effected by the   proceeding sentences; pronoun  \\nresolution.         \\n          John gave his bike to Bill. \\n  He didn\\'t care much for it anyway.\\n5.World Knowledge   - the vast amount of  knowledge necessary \\nto  understand texts. Used to identify beliefs, goals. \\n6.Language generation  - have the machine generate coherent \\ntext  or speech.  Examples of NLP difficulties \\n1.Syntactic ambiguity - when a word has more than one \\npart of  speech:    \\n  Example: Rice flies like sand . \\n Note that these syntactic ambiguities lead to different parse \\nstructures. Sometimes it is possible to use grammar rules \\n(like subject verb  agreement) to disambiguate :   \\n        Flying planes are dangerous.      \\n        Flying planes is dangerous.     \\n2. Semantic ambiguity - when a word has more than one \\npossible  meaning  (or sense):      \\n  John killed  the wolf.       \\n  John killed the project.       \\n  John killed that bottle of wine.        \\n  John killed Jane.  (at tennis , or murdered her)More Examples of NLP difficulties \\n3. Structural ambiguity - when a sentence has more than one \\npossible parse structure; e.g. prepositional attachment .\\n•Example:\\n  John saw the boy in the park with a telescope .  \\nAnother syntactic parse\\nAdditional NLP difficulties \\nAmbiguities of a sentence : \\n \\n Example:      \\n I made her duck.     \\n \\n Possible interpretations:   \\n \\n1.I cooked waterfowl for her.    \\n2.I cooked waterfowl belonging to her.    \\n3.I created the (plaster ?) duck she owns.    \\n4.I caused her to quickly lower her head or body\\n5.I wave my magic wand and turned her into \\nundifferentiated waterfowl.State of the art in NLP Research 1/2\\n•NLP Publications  : \\n•Association of Computational Linguistics (ACL):\\n•Conferences: ACL, HLT -NAACL, EACL, EMNLP  \\n•Journals: Computational Linguistics, TACL\\n• AAAI - every year proceedings.\\n•IJCAI - every year proceedings.\\n•The Web Conference\\n•On the WWWeb: http://aclweb.org\\n•Natural Language Engineering  (journal).•Machine Readable Dictionaries  (MRD)  WordNet, \\nLDOCE.\\n•Large corpora :  \\n•Penn Treebank —contains   2 -3 months of  Wall Street \\nJournal articles (~ .5 million words of English, POS  \\ntagged and parsed),  \\n•Brown corpus,\\n•SemCor.\\n•Google GiGaword\\n•Neural Language ProcessingState of the art in NLP Research 2/2Neural Language Learning\\n•Nowadays, it is the “de facto” way of doing NLP\\n➢BERT: Pre -training of Deep Bidirectional Transformers for \\nLanguage Understanding\\n❑https://www.aclweb.org/anthology/N19 -1423.pdf\\noBERT is designed to pretrain deep bidirectional \\nrepresentations from unlabeled text by jointly conditioning \\non both left and right context in all layers. As a result, the \\npre-trained BERT model can be finetuned with just one \\nadditional output layer to create state -of-the-art models for \\na wide range of tasks, such as question answering and \\nlanguage inference, without substantial task -specific \\narchitecture modifications.Evaluation of NLP systems\\n➢The General Language Understanding Evaluation \\n(GLUE) benchmark (Wang et al., 2018a) is a \\ncollection of diverse natural language understanding \\ntasks.\\nohttps://www.aclweb.org/anthology/W18 -5446.pdf\\noTAKE -HOME LESSON: In order to build neural \\nlanguage processing systems, we rely on vast \\nannotated dataset.\\n➢It is IMPOSSIBLE to build NLP systems without \\nlooking and deeply understanding the texts.\\n➢ANNOTATION experience is KEY!!!\\n\\n ========== SPACE BETWEEN FILES ==========  ========== SPACE BETWEEN FILES ========== \\n\\nNatural Language Processing\\n2004, 8Lectures\\nAnn Copestak e(aac@cl.cam.ac.uk )\\nhttp://www .cl.cam.ac.uk/users/aac/\\nCopyright c\\rAnn Copestak e,2003\\x962004\\nLectur eSynopsis\\nAims\\nThis course aims tointroduce thefundamental techniques ofnatural language processing andtodevelop anunder -\\nstanding ofthelimits ofthose techniques. Itaims tointroduce some current research issues, andtoevaluate some\\ncurrent andpotential applications.\\n\\x0fIntroduction. Brief history ofNLP research, current applications, generic NLP system architecture, knowledge-\\nbased versusprobabilistic approaches.\\n\\x0fFinite-state techniques. In\\x03ectional andderivational morphology ,\\x02nite-state automata inNLP,\\x02nite-state\\ntransducers.\\n\\x0fPrediction andpart-of-speech tagging .Corpora, simple N-grams, wordprediction, stochastic tagging, evalu-\\nating system performance.\\n\\x0fParsing andgeneration. Generati vegrammar ,conte xt-free grammars, parsing andgeneration with conte xt-free\\ngrammars, weights andprobabilities.\\n\\x0fParsing with constraint-based grammars. Constraint-based grammar ,uni\\x02cation.\\n\\x0fCompositional andlexical semantics. Simple compositional semantics inconstraint-based grammar .Semantic\\nrelations, WordNet, wordsenses, wordsense disambiguation.\\n\\x0fDiscourse anddialogue. Anaphora resolution, discourse relations.\\n\\x0fApplications. Machine translation, email response, spok endialogue systems.\\nObjecti ves\\nAttheendofthecourse students should\\n\\x0fbeable todescribe thearchitecture ofandbasic design forageneric NLP system \\x93shell\\x94\\n\\x0fbeable todiscuss thecurrent andlikelyfuture performance ofseveral NLP applications, such asmachine\\ntranslation andemail response\\n\\x0fbeable todescribe brie\\x03y afundamental technique forprocessing language forseveralsubtasks, such asmor-\\nphological analysis, parsing, wordsense disambiguation etc.\\n\\x0funderstand howthese techniques drawonandrelate toother areas of(theoretical) computer science, such as\\nformal language theory ,formal semantics ofprogramming languages, ortheorem proving\\n1Overview\\nNLP isalargeandmultidisciplinary \\x02eld, sothiscourse canonly provide averygeneral introduction. The \\x02rst\\nlecture isdesigned togiveanovervie wofthemain subareas andaverybrief idea ofthemain applications and\\nthemethodologies which havebeen emplo yed. Thehistory ofNLP isbrie\\x03y discussed asawayofputting thisinto\\nperspecti ve.Thenextsixlectures describe some ofthemain subareas inmore detail. Theorganisation isroughly based\\nonincreased `depth\\' ofprocessing, starting with relati velysurface-oriented techniques andprogressing toconsidering\\nmeaning ofsentences andmeaning ofutterances inconte xt.Most lectures willstart offbyconsidering thesubarea as\\nawhole andthen goontodescribe oneormore sample algorithms which tackle particular problems. Thealgorithms\\nhavebeen chosen because theyarerelati velystraightforw ardtodescribe andbecause theyillustrate aspeci\\x02c technique\\nwhich hasbeen showntobeuseful, buttheideaistoexemplify anapproach, nottogiveadetailed survey(which would\\nbeimpossible inthetime available). (Lecture 5isabitdifferent inthatitconcentrates onadata structure instead of\\nanalgorithm.) The\\x02nal lecture brings thepreceding material together inorder todescribe thestate oftheartinthree\\nsample applications.\\nThere arevarious themes running throughout thelectures. One theme istheconnection tolinguistics andthetension\\nthatsometimes exists between thepredominant viewintheoretical linguistics andtheapproaches adopted within NLP.\\nAsome what related theme isthedistinction between knowledge-based andprobabilistic approaches. Evaluation will\\nbediscussed intheconte xtofthedifferent algorithms.\\nBecause NLP issuch alargearea, there aremanytopics that aren\\' ttouched onatallinthese lectures. Speech\\nrecognition andspeech synthesis isalmost totally ignored. Information retrie valandinformation extraction arethe\\ntopic ofaseparate course givenbySimone Teufel, forwhich thiscourse isaprerequisite.\\nFeedback onthehandout, listsoftypos etc,would begreatly appreciated.\\nRecommended Reading\\nRecommended Book:\\nJurafsk y,Daniel andJames Martin, Speec handLangua geProcessing ,Prentice-Hall, 2000 (referenced asJ&M through-\\noutthishandout).\\nBackground:\\nThese books areabout linguistics rather thatNLP/computational linguistics. Theyarenotnecessary tounderstand the\\ncourse, butshould givereaders anidea about some oftheproperties ofhuman languages thatmakeNLP interesting\\nandchallenging, without being technical.\\nPinker,S.,TheLangua geInstinct ,Penguin, 1994.\\nThis isathought-pro voking andsometimes contro versial `popular\\' introduction tolinguistics.\\nMatthe ws,Peter ,Linguistics: avery short introduction ,OUP ,2003.\\nThetitleisaccurate ...\\nBackground/reference:\\nTheInternet Grammar ofEnglish ,http://www .ucl.ac.uk/internet-grammar/home.htm\\nSyntactic concepts andterminology .\\nStudy andSuper vision Guide\\nThe handouts andlectures should contain enough information toenable students toadequately answer theexam\\nquestions, butthehandout isnotintended tosubstitute foratextbook. Inmost cases, J&M gointo aconsiderable\\namount offurther detail: rather than putlotsofsuggestions forfurther reading inthehandout, ingeneral Ihave\\nassumed thatstudents willlook atJ&M, andthen followupthereferences inthere iftheyareinterested. Thenotes at\\ntheendofeach lecture givedetails ofthesections ofJ&M thatarerelevantanddetails ofanydiscrepancies with these\\nnotes.\\nSupervisors ought tofamiliarise themselv eswith therelevantparts ofJurafsk yandMartin (seenotes attheendofeach\\nlecture). However,good students should \\x02nd itquite easy tocome upwith questions thatthesupervisors (and the\\n2lecturer) can\\'tanswer! Language islikethat...\\nGenerally I\\'mtaking arather informal/e xample-based approach toconcepts such as\\x02nite-state automata, conte xt-free\\ngrammars etc. PartIIstudents should havealready gottheformal background thatenables them tounderstand the\\napplication toNLP.Diploma andPartII(General) students may nothavecovered allthese concepts before, butthe\\nexpectation isthattheexamples arestraightforw ardenough sothatthiswon\\'tmatter toomuch.\\nThis course inevitably assumes some verybasic linguistic knowledge, such asthedistinction between themajor parts\\nofspeech. Itintroduces some linguistic concepts thatwon\\'tbefamiliar toallstudents: since I\\'llhavetogothrough\\nthese quickly ,reading the\\x02rstfewchapters ofanintroductory linguistics textbook may help students understand the\\nmaterial. The idea istointroduce justenough linguistics tomotivatetheapproaches used within NLP rather than\\ntoteach thelinguistics foritsownsake.Attheendofthishandout, there aresome mini-e xercises tohelp students\\nunderstand theconcepts: itwould beveryuseful ifthese were attempted before thelectures asindicated. There are\\nalsosome suggested post-lecture exercises.\\nExam questions won\\'trely onstudents remembering thedetails ofanyspeci\\x02c linguistic phenomenon. Asfaras\\npossible, exam questions will besuitable forpeople who speak English asasecond language. Forinstance, ifa\\nquestion relied onknowledge oftheambiguity ofaparticular English word, agloss oftherelevantsenses would be\\ngiven.\\nOfcourse, I\\'llbehapp ytotryandanswer questions about thecourse ormore general NLP questions, preferably by\\nemail.\\n31Lectur e1:Introduction toNLP\\nThe aimofthislecture istogivestudents some idea oftheobjecti vesofNLP.The main subareas ofNLP will be\\nintroduced, especially those which willbediscussed inmore detail intherestofthecourse. There willbeapreliminary\\ndiscussion ofthemain problems involvedinlanguage processing bymeans ofexamples takenfrom NLP applications.\\nThis lecture also introduces some methodological distinctions andputs theapplications andmethodology intosome\\nhistorical conte xt.\\n1.1 What isNLP?\\nNatural language processing (NLP) canbede\\x02ned astheautomatic (orsemi-automatic) processing ofhuman language.\\nTheterm `NLP\\' issometimes used rather more narro wlythan that, often excluding information retrie valandsometimes\\nevenexcluding machine translation. NLP issometimes contrasted with `computational linguistics\\', with NLP being\\nthought ofasmore applied. Nowadays, alternati veterms areoften preferred, like`Language Technology\\' or`Language\\nEngineering\\'. Language isoften used incontrast with speech (e.g., Speech andLanguage Technology). ButI\\'mgoing\\ntosimply refer toNLP andusetheterm broadly .\\nNLP isessentially multidisciplinary: itisclosely related tolinguistics (although theextent towhich NLP overtly draws\\nonlinguistic theory varies considerably). Italsohaslinks toresearch incogniti vescience, psychology ,philosoph yand\\nmaths (especially logic). Within CS,itrelates toformal language theory ,compiler techniques, theorem proving, ma-\\nchine learning andhuman-computer interaction. Ofcourse itisalsorelated toAI,though nowadays it\\'snotgenerally\\nthought ofaspartofAI.\\n1.2 Some linguistic terminology\\nThecourse isorganised sothatthere aresixlectures corresponding todifferent NLP subareas, moving from relati vely\\n`shallo w\\'processing toareas which involvemeaning andconnections with therealworld. These subareas loosely\\ncorrespond tosome ofthestandard subdi visions oflinguistics:\\n1.Morphology: thestructure ofwords. Forinstance, unusually canbethought ofascomposed ofapre\\x02x un-,a\\nstem usual ,andanaf\\x02x-ly.composed iscompose plus thein\\x03ectional af\\x02x-ed:aspelling rulemeans weend\\nupwith composed rather than composeed .Morphology willbediscussed inlecture 2.\\n2.Syntax: thewaywords areused toform phrases. e.g., itispart ofEnglish syntax thatadeterminer such as\\nthewillcome before anoun, andalso thatdeterminers areoblig atory with certain singular nouns. Formal and\\ncomputational aspects ofsyntax willbediscussed inlectures 3,4and5.\\n3.Semantics. Compositional semantics istheconstruction ofmeaning (generally expressed aslogic) based on\\nsyntax. This iscontrasted tolexical semantics, i.e.,themeaning ofindividual words. Compositional andlexical\\nsemantics isdiscussed inlecture 6.\\n4.Pragmatics: meaning inconte xt.This will come into lecture 7,although linguistics andNLP generally have\\nverydifferent perspecti veshere.\\n1.3 Whyislanguage processing dif\\x02cult?\\nConsider trying tobuildasystem thatwould answer email sentbycustomers toaretailer selling laptops andaccessories\\nviatheInternet. This might beexpected tohandle queries such asthefollowing:\\n\\x0fHasmyorder number 4291 been shipped yet?\\n\\x0fIsFD5 compatible with a505G?\\n\\x0fWhat isthespeed ofthe505G?\\n4Assume thequery istobeevaluated against adatabase containing product andorder information, with relations such\\nasthefollowing:\\nORDER\\nOrder number Date ordered Date shipped\\n4290 2/2/02 2/2/02\\n4291 2/2/02 2/2/02\\n4292 2/2/02\\nUSER: Hasmyorder number 4291 been shipped yet?\\nDBQUER Y:order(number=4291,date shipped=?)\\nRESPONSE TOUSER: Order number 4291 wasshipped on2/2/02\\nItmight look quite easy towrite patterns forthese queries, butverysimilar strings canmean verydifferent things,\\nwhile verydifferent strings canmean much thesame thing. 1and2belowlook verysimilar butmean something\\ncompletely different, while 2and3look verydifferent butmean much thesame thing.\\n1.Howfastisthe505G?\\n2.Howfastwillmy505G arrive?\\n3.Please tellmewhen Icanexpect the505G Iordered.\\nWhile some tasks inNLP canbedone adequately without having anysortofaccount ofmeaning, others require that\\nwecanconstruct detailed representations which willre\\x03ect theunderlying meaning rather than thesuper\\x02cial string.\\nInfact,innatural languages (asopposed toprogramming languages), ambiguity isubiquitous, soexactly thesame\\nstring might mean different things. Forinstance inthequery:\\nDoyousellSonylaptops anddisk drives?\\ntheuser may ormay notbeasking about Sonydisk drives.This particular ambiguity may berepresented bydifferent\\nbrack etings:\\nDoyousell(Sonylaptops) and(disk drives)?\\nDoyousell(Sony(laptops anddisk drives))?\\nWe\\'llseelotsofexamples ofdifferent types ofambiguity inthese lectures.\\nOften humans haveknowledge oftheworld which resolv esapossible ambiguity ,probably without thespeak eror\\nhearer evenbeing awarethatthere isapotential ambiguity .1Buthand-coding such knowledge inNLP applications\\nhasturned outtobeimpossibly hard todoformore than verylimited domains: theterm AI-complete issometimes\\nused (byanalogy toNP-complete), meaning thatwe\\'dhavetosolvetheentire problem ofrepresenting theworld\\nandacquiring world knowledge.2Theterm AI-complete isintended jokingly ,butconveyswhat\\' sprobably themost\\nimportant guiding principle incurrent NLP: we\\'relooking forapplications which don\\'trequire AI-complete solutions:\\ni.e.,ones where wecanworkwith verylimited domains orapproximate fullworld knowledge byrelati velysimple\\ntechniques.\\n1.4 Some NLP applications\\nThefollowing listisnotcomplete, butuseful systems havebeen builtfor:\\n1I\\'llusehearergenerally tomean theperson who isontherecei ving end, regardless ofthemodality ofthelanguage transmission: i.e.,regardless\\nofwhether it\\'sspok en,signed orwritten. Similarly ,I\\'llusespeak erfortheperson generating thespeech, textetcandutterance tomean thespeech\\nortextitself. This isthestandard linguistic terminology ,which recognises thatspok enlanguage isprimary andtextisalater development.\\n2Inthiscourse, Iwillusedomain tomean some circumscribed body ofknowledge: forinstance, information about laptop orders constitutes a\\nlimited domain.\\n5\\x0fspelling andgrammar checking\\n\\x0foptical character recognition (OCR)\\n\\x0fscreen readers forblind andpartially sighted users\\n\\x0faugmentati veandalternati vecommunication (i.e., systems toaidpeople who havedif\\x02culty communicating\\nbecause ofdisability)\\n\\x0fmachine aided translation (i.e., systems which help ahuman translator ,e.g., bystoring translations ofphrases\\nandproviding online dictionaries integrated with wordprocessors, etc)\\n\\x0flexicographers\\' tools\\n\\x0finformation retrie val\\n\\x0fdocument classi\\x02cation (\\x02ltering, routing)\\n\\x0fdocument clustering\\n\\x0finformation extraction\\n\\x0fquestion answering\\n\\x0fsummarization\\n\\x0ftextsegmentation\\n\\x0fexam marking\\n\\x0freport generation (possibly multilingual)\\n\\x0fmachine translation\\n\\x0fnatural language interf aces todatabases\\n\\x0femail understanding\\n\\x0fdialogue systems\\nSeveralofthese applications arediscussed brie\\x03y below.Roughly speaking, theyareordered according tothecom-\\nplexityofthelanguage technology required. Theapplications towards thetopofthelistcanbeseen simply asaidsto\\nhuman users, while those atthebottom arepercei vedasagents intheir ownright. Perfect performance onanyofthese\\napplications would beAI-complete, butperfection isn\\'tnecessary forutility: inmanycases, useful versions ofthese\\napplications hadbeen builtbythelate70s. Commercial success hasoften been harder toachie ve,however.\\n1.5 Spelling andgrammar checking\\nAllspelling check erscan\\x03agwords which aren\\' tinadictionary .\\n(1) *Theneccesary steps areobvious.\\n(2) Thenecessary steps areobvious.\\nIftheuser canexpand thedictionary ,orifthelanguage hascomple xproducti vemorphology (seex2.1), then asimple\\nlistofwords isn\\'tenough todothisandsome morphological processing isneeded.3\\nMore subtle cases involvewords which arecorrect inisolation, butnotinconte xt.Syntax could sortsome ofthese\\ncases out.Forinstance, possessi veitsgenerally hastobeimmediately followed byanoun orbyoneormore adjecti ves\\nwhich areimmediately infront ofanoun:\\n3Note theuseof*(`star\\') above:thisnotation isused inlinguistics toindicate asentence which isjudged (bytheauthor ,atleast) tobeincorrect.\\n?isgenerally used forasentence which isquestionable, oratleast doesn\\' thavetheintended interpretation. #isused forapragmatically anomalous\\nsentence.\\n6(3) *Itsafairexchange.\\n(4) It\\'safairexchange.\\n(5) *Thedogcame intotheroom, it\\'stailwagging.\\n(6) Thedogcame intotheroom, itstailwagging.\\nBut, itsometimes isn\\'tlocally clear what theconte xtis:e.g.fairisambiguous between anoun andanadjecti ve.\\n(7) *`Itsfair\\', wasallKim said.\\n(8) `It\\'sfair\\', wasallKim said.\\n(9) *Everyvillage hasanannual fair,except Kimbolton: it\\'sfairisheld twice ayear.\\n(10) Everyvillage hasanannual fair,except Kimbolton: itsfairisheld twice ayear.\\nThemost elaborate spelling/grammar check erscangetsome ofthese cases right, butnone areanywhere near perfect.\\nSpelling correction canrequire aform ofwordsense disambiguation :\\n(11) #Thetree\\'sbowswere heavywith snow.\\n(12) Thetree\\'sboughs were heavywith snow.\\nGetting thisright requires anassociation between treeandbough .Inthepast, attempts might havebeen made to\\nhand-code thisinterms ofgeneral knowledge oftrees andtheir parts. Howeverthissortofhand-coding isnotsuitable\\nforapplications thatworkonunbounded domains. These days machine learning techniques aregenerally used to\\nderivewordassociations from corpora:4thiscanbeseen asasubstitute forthefully detailed world knowledge, but\\nmay actually beamore realistic model ofhowhumans dowordsense disambiguation. However,commercial systems\\ndon\\'t(yet) dothissystematically .\\nSimple subject verbagreement canbecheck edautomatically:5\\n(13) Myfriends likepizza.\\n(14) Myfriend likespizza.\\n(15) *Myfriends likespizza.\\n(16) *Myfriend likepizza.\\n(17) Myfriends were unhapp y.\\n(18) *Myfriend were unhapp y.\\nButthisisn\\'tasstraightforw ardasitmay seem:\\n(19) Anumber ofmyfriends were unhapp y.\\n(20) Thenumber ofmyfriends who were unhapp ywasamazing.\\n(21) Myfamily were unhapp y.\\nWhether thelastexample isgrammatical ornotdepends onyour dialect ofEnglish: itisgrammatical formost British\\nEnglish speak ers,butnotformanyAmericans.\\nChecking punctuation canbehard (evenAI-complete):\\nBBC NewsOnline, 3October ,2001\\n4Acorpus isabody oftextthathasbeen collected forsome purpose, seex3.1.\\n5InEnglish, thesubject ofasentence isgenerally anoun phrase which comes before theverb,incontrast totheobject, which followstheverb.\\n7Students atCambridge University ,who come from lessaf\\x03uent backgrounds, arebeing offered upto\\n1,000 ayear under abursary scheme.\\nThis sentence contains anon-r estrictive relative clause :who come fromlessaf\\x03uent backgrounds .This isaform\\nofparenthetical comment. The sentence implies thatmost/all students atCambridge come from lessaf\\x03uent back-\\ngrounds. What thereporter probably meant wasarestricti verelati ve,which should nothavecommas round it:\\nStudents atCambridge University who come from lessaf\\x03uent backgrounds arebeing offered upto1,000\\nayear under abursary scheme.\\nArestricti verelati veisatype ofmodi\\x02er :thatis,itfurther speci\\x02es theentities under discussion. Thus itrefers toa\\nsubset ofthestudents.\\n1.6 Information retrie val,information extraction andquestion answering\\nInformation retrie valinvolvesreturning asetofdocuments inresponse toauser query: Internet search engines area\\nform ofIR.However,onechange from classical IRisthatInternet search nowuses techniques thatrank documents\\naccording tohowmanylinks there aretothem (e.g., Google\\' sPageRank) aswell asthepresence ofsearch terms.\\nInformation extraction involvestrying todisco verspeci\\x02c information from asetofdocuments. The information\\nrequired canbedescribed asatemplate. Forinstance, forcompan yjoint ventures, thetemplate might haveslots for\\nthecompanies, thedates, theproducts, theamount ofmone yinvolved.Theslot\\x02llers aregenerally strings.\\nQuestion answering attempts to\\x02ndaspeci\\x02c answer toaspeci\\x02c question from asetofdocuments, oratleast ashort\\npiece oftextthatcontains theanswer .\\n(22) What isthecapital ofFrance?\\nParishasbeen theFrench capital formanycenturies.\\nThere aresome question-answering systems ontheWeb,butmost useverybasic techniques. Forinstance, AskJeeves\\nrelies onafairly largestaffofpeople who search theweb to\\x02ndpages which areanswers topotential questions. The\\nsystem performs verylimited manipulation ontheinput tomap toaknownquestion. Thesame basic technique isused\\ninmanyonline help systems.\\n1.7 Machine translation\\nMTworkstarted intheUSintheearly \\x02fties, concentrating onRussian toEnglish. Aprototype system waspublicly\\ndemonstrated in1954 (remember thatthe\\x02rstelectronic computer hadonly been builtafewyears before that). MT\\nfunding gotdrastically cutintheUSinthemid-60s andceased tobeacademically respectable insome places, but\\nSystran wasproviding useful translations bythelate60s. Systran isstillgoing (updating itovertheyears isanamazing\\nfeatofsoftw areengineering): Systran nowpowers AltaV ista\\'sBabelFish\\nhttp://world.altavista.com/\\nandmanyother translation services ontheweb.\\nUntil the80s, theutility ofgeneral purpose MTsystems wasseverely limited bythefactthattextwasnotavailable in\\nelectronic form: Systran used teams ofskilled typists toinput Russian documents.\\nSystran andsimilar systems arenotasubstitute forhuman translation: theyareuseful because theyallowpeople to\\ngetanidea ofwhat adocument isabout, andmaybe decide whether itisinteresting enough togettranslated properly .\\nThis ismuch more relevantnowthatdocuments etcareavailable ontheWeb.Badtranslation isalso, apparently ,good\\nenough forchatrooms.\\nSpok enlanguage translation isviable forlimited domains: research systems include Verbmobil, SLTandCSTAR.\\n81.8 Natural language interfaces anddialogue systems\\nNatural language interf aces were the`classic\\' NLP problem inthe70sand80s. LUN ARistheclassic example of\\nanatural language interf acetoadatabase (NLID): itsdatabase concerned lunar rock samples brought back from the\\nApollo missions. LUN ARisdescribed byWoods (1978) (butnote most oftheworkwasdone severalyears earlier): it\\nwascapable oftranslating elaborate natural language expressions intodatabase queries.\\nSHRDLU (Winograd, 1973) wasasystem capable ofparticipating inadialogue about amicro world(theblocks world)\\nandmanipulating thisworldaccording tocommands issued inEnglish bytheuser.SHRDLU hadabigimpact onthe\\nperception ofNLP atthetime since itseemed toshowthat computers could actually `understand\\' language: the\\nimpossibility ofscaling upfrom themicro worldwasnotrealised.\\nLUN ARandSHRDLU both exploited thelimitations ofoneparticular domain tomakethenatural language under -\\nstanding problem tractable, particularly with respect toambiguity .Totakeatrivialexample, ifyouknowyour database\\nisabout lunar rock, youdon\\'tneed toconsider themusic ormovement senses ofrockwhen you\\'reanalysing aquery .\\nThere havebeen manyadvances inNLP since these systems were built: systems havebecome much easier tobuild,\\nandsome what easier touse,buttheystillhaven\\'tbecome ubiquitous. Natural Language interf aces todatabases were\\ncommercially available inthelate1970s, butlargely died outbythe1990s: porting tonewdatabases andespecially to\\nnewdomains requires veryspecialist skills andisessentially tooexpensi ve(automatic porting wasattempted butnever\\nsuccessfully developed). Users generally preferred graphical interf aces when these became available. Speech input\\nwould makenatural language interf aces much more useful: unfortunately ,speak er-independent speech recognition\\nstill isn\\'tgood enough foreven1970s scale NLP toworkwell. Techniques fordealing with misrecognised data\\nhaveprovedhard todevelop. Inmanyways,current commercially-deplo yedspok endialogue systems areusing pre-\\nSHRDLU technology .\\n1.9 Some morehistory\\nBefore the1970s, most NLP researchers were concentrating onMTasanapplication (see above).NLP wasavery\\nearly application ofCSandstarted about thesame time asChomsk ywaspublishing his\\x02rstmajor works informal\\nlinguistics (Chomsk yanlinguistics quickly became dominant, especially intheUS). Inthe1950s andearly 1960s,\\nideas about formal grammar were being workedoutinlinguistics andalgorithms forparsing natural language were\\nbeing developed atthesame time asalgorithms forparsing programming languages. However,most linguists were\\nuninterested inNLP andtheapproach thatChomsk ydeveloped turned outtobeonly some what indirectly useful for\\nNLP.\\nNLP inthe1970s and\\x02rsthalfofthe1980s waspredominantly based onaparadigm where extensi velinguistic and\\nreal-w orld knowledge washand-coded. There wascontro versy about howmuch linguistic knowledge wasnecessary\\nforprocessing, with some researchers downplaying syntax, inparticular ,infavourofworld knowledge. NLP re-\\nsearchers were verymuch partoftheAIcommunity (especially intheUSandtheUK), andthedebate thatwent onin\\nAIabout theuseoflogic vsother meaning representations (`neat\\' vs`scruf fy\\')alsoaffected NLP.Bythe1980s, several\\nlinguistic formalisms hadappeared which were fully formally grounded andreasonably computationally tractable, and\\nthelinguistic/logical paradigm inNLP was\\x02rmly established. Unfortunately ,thisdidn\\' tlead tomanyuseful systems,\\npartly because manyofthedif\\x02cult problems (disambiguation etc)were seen assomebody else\\'sjob(and mainstream\\nAIwasnotdeveloping adequate knowledge representation techniques) andpartly because most researchers were con-\\ncentrating onthe`agent-lik e\\'applications andneglecting theuser aids. Although thesymbolic, linguistically-based\\nsystems sometimes workedquite well asNLIDs, theyprovedtobeoflittle usewhen itcame toprocessing lessre-\\nstricted text,forapplications such asIE.Italso became apparent thatlexical acquisition wasaserious bottleneck for\\nserious development ofsuch systems.\\nStatistical NLP became themost common paradigm inthe1990s, atleast intheresearch community .Speech recog-\\nnition haddemonstrated thatsimple statistical techniques worked,givenenough training data. NLP systems were\\nbuiltwhich required verylimited hand-coded knowledge, apart from initial training material. Most applications were\\nmuch shallo werthan theearlier NLIDs, buttheswitch tostatistical NLP coincided with achange inUSfunding,\\nwhich started toemphasise speech-based interf aces andIE.There wasalso ageneral realization oftheimportance\\nofserious evaluation andofreporting results inawaythatcould bereproduced byother researchers. USfunding\\nemphasised competitions with speci\\x02c tasks andsupplied testmaterial, which encouraged this, although there wasa\\ndownside inthatsome ofthetechniques developed were verytask-speci\\x02c. Itshould beemphasised thatthere had\\n9been computational workoncorpora formanyyears (much ofitbylinguists): itbecame much easier todocorpus\\nworkbythelate1980s asdisk space became cheap andmachine-readable textbecame ubiquitous. Despite theshift\\ninresearch emphasis tostatistical approaches, most commercial systems remained primarily based onhand-coded\\nlinguistic information.\\nMore recently thesymbolic/statistical split hasbecome lesspronounced, since most researchers areinterested inboth.6\\nThere isconsiderable emphasis onmachine learning ingeneral, including machine learning forsymbolic processing.\\nLinguistically-based NLP hasmade something ofacomeback, with increasing availability ofopen source resources,\\nandtherealisation thatatleast some oftheclassic statistical techniques seem tobereaching limits onperformance,\\nespecially because ofdif\\x02culties inadapting tonewtypes oftext.However,themodern linguistically-based approaches\\naremaking useofmachine learning andstatistical processing. Thedotcom boom andbusthasconsiderably affected\\nNLP,butit\\'stooearly tosaywhat thelong-term implications are.Theubiquity oftheInternet hascertainly changed\\nthespace ofinteresting NLP applications, andthevastamount oftextavailable canpotentially beexploited, especially\\nforstatistical techniques.\\n1.10 Generic `deep\\' NLP application architectur e\\nManyNLP applications canbeadequately implemented with relati velyshallo wprocessing. Forinstance, spelling\\nchecking only requires awordlistandsimple morphology tobeuseful. I\\'llusetheterm `deep\\' NLP forsystems that\\nbuildameaning representation (oranelaborate syntactic representation), which isgenerally agreed toberequired for\\napplications such asNLIDs, email question answering andgood MT.\\nThemost important principle inbuilding asuccessful NLP system ismodularity .NLP systems areoften bigsoftw are\\nengineering projects \\x97success requires thatsystems canbeimpro vedincrementally .\\nThe input toanNLP system could bespeech ortext.Itcould also begesture (multimodal input orperhaps aSign\\nLanguage). Theoutput might benon-linguistic, butmost systems need togivesome sortoffeedback totheuser,even\\niftheyaresimply performing some action (issuing aticket,paying abill, etc). However,often thefeedback canbe\\nveryformulaic.\\nThere\\' sgeneral agreement thatthefollowing system components canbedescribed semi-independently ,although as-\\nsumptions about thedetailed nature oftheinterf aces between them differ.Notallsystems haveallofthese components:\\n\\x0finput preprocessing: speech recogniser ortextpreprocessor (non-tri vialinlanguages likeChinese orforhighly\\nstructured textforanylanguage) orgesture recogniser .Such system might themselv esbeverycomple x,butI\\nwon\\'tdiscuss them inthiscourse \\x97we\\'llassume thattheinput tothemain NLP component issegmented text.\\n\\x0fmorphological analysis: thisisrelati velywell-understood forthemost common languages thatNLP hasconsid-\\nered, butiscomplicated formanylanguages (e.g., Turkish, Basque).\\n\\x0fpartofspeech tagging: notanessential partofmost deep processing systems, butsometimes used asawayof\\ncutting downparser search space.\\n\\x0fparsing: thisincludes syntax andcompositional semantics, which aresometimes treated asseparate components\\n\\x0fdisambiguation: thiscanbedone aspartofparsing, or(partially) lefttoalater phase\\n\\x0fconte xtmodule: thismaintains information about theconte xt,foranaphora resolution, forinstance.\\n\\x0ftextplanning: thepartoflanguage generation that\\'sconcerned with deciding what meaning toconvey(Iwon\\'t\\ndiscuss thisinthiscourse)\\n\\x0ftactical generation: convertsmeaning representations tostrings. This may usethesame grammar andlexicon7\\nastheparser .\\n\\x0fmorphological generation: aswith morphological analysis, thisisrelati velystraightforw ardforEnglish.\\n6Atleast, there areonly afewresearchers who avoidstatistical techniques asamatter ofprinciple andallstatistical systems haveasymbolic\\ncomponent!\\n7Theterm lexicon isgenerally used forthepartoftheNLP system thatcontains dictionary-lik einformation \\x97i.e.information about individual\\nwords.\\n10\\x0foutput processing: text-to-speech, textformatter ,etc. Aswith input processing, thismay becomple x,butfor\\nnowwe\\'llassume thatwe\\'reoutputting simple text.\\nApplication speci\\x02c components, forinstance:\\n1.ForNLinterf aces, email answering andsoon,weneed aninterf acebetween semantic representation (expressed\\nassome form oflogic, forinstance) andtheunderlying knowledge base.\\n2.ForMTbased ontransfer ,weneed acomponent thatmaps between semantic representations.\\nItisalso veryimportant todistinguish between theknowledge sources andtheprograms thatusethem. Forinstance,\\namorphological analyser hasaccess toalexicon andasetofmorphological rules: themorphological generator might\\nshare these knowledge sources. Thelexicon forthemorphology system may bethesame asthelexicon fortheparser\\nandgenerator .\\nOther things might berequired inorder toconstruct thestandard components andknowledge sources:\\n\\x0flexicon acquisition\\n\\x0fgrammar acquisition\\n\\x0facquisition ofstatistical information\\nForacomponent tobeatruemodule, itobviously needs awell-de\\x02ned setofinterf aces. What\\' slessobvious isthatit\\nneeds itsownevaluation strate gyandtestsuites: developers need tobeable toworksome what independently .\\nInprinciple, atleast, components arereusable invarious ways: forinstance, aparser could beused with multiple\\ngrammars, thesame grammar canbeprocessed bydifferent parsers andgenerators, aparser/grammar combination\\ncould beused inMTorinanatural language interf ace. However,foravariety ofreasons, itisnoteasy toreuse\\ncomponents likethis, andgenerally alotofworkisrequired foreach newapplication, evenifit\\'sbased onanexisting\\ngrammar orthegrammar isautomatically acquired.\\nWecandrawschematic diagrams forapplications showing howthemodules \\x02ttogether .\\n1.11 Natural language interface toaknowledge base\\nKB\\n*\\nKBINTERF ACE/CONTEXT\\n6\\nPARSING\\n6\\nMORPHOLOGY\\n6\\nINPUT PROCESSING\\n6\\nuser inputj\\nKBOUTPUT/TEXT PLANNING\\n?\\nTACTICAL GENERA TION\\n?\\nMORPHOLOGY GENERA TION\\n?\\nOUTPUT PROCESSING\\n?\\noutput\\n11Insuch systems, theconte xtmodule generally getsincluded aspartoftheKBinterf acebecause thediscourse state is\\nquite simple, andconte xtual resolution isdomain speci\\x02c. Similarly ,there\\' soften noelaborate textplanning require-\\nment, though thisdepends verymuch ontheKBandtype ofqueries involved.\\nInlectures 2\\x967, various algorithms will bediscussed which could beparts ofmodules inthisgeneric architecture,\\nalthough most arealsouseful inlesselaborate conte xts.Lecture 8willdiscuss thearchitecture andrequirements ofa\\nfewapplications inabitmore detail.\\n1.12 General comments\\n\\x0fEven`simple\\' NLP applications, such asspelling check ers,need comple xknowledge sources forsome prob-\\nlems.\\n\\x0fApplications cannot be100% perfect, because fullrealworldknowledge isnotpossible.\\n\\x0fApplications thatarelessthan 100% perfect canbeuseful (humans aren\\' t100% perfect anyway).\\n\\x0fApplications thataidhumans aremuch easier toconstruct than applications which replace humans. Itisdif\\x02cult\\ntomakethelimitations ofsystems which accept speech orlanguage obvious tonaivehuman users.\\n\\x0fNLP interf aces arenearly alwayscompeting with anon-language based approach.\\n\\x0fCurrently nearly allapplications either dorelati velyshallo wprocessing onarbitrary input ordeep processing on\\nnarro wdomains. MTcanbedomain-speci\\x02c tovarying extents: MTonarbitrary textisn\\'tverygood, buthas\\nsome applications.\\n\\x0fLimited domain systems require extensi veandexpensi veexpertise toport. Research thatrelies onextensi ve\\nhand-coding ofknowledge forsmall domains isnowgenerally regarded asadead-end, though reusable hand-\\ncoding isadifferent matter .\\n\\x0fThedevelopment ofNLP hasmainly been drivenbyhardw areandsoftw areadvances, andsocietal andinfras-\\ntructure changes, notbygreat newideas. Impro vements inNLP techniques aregenerally incremental rather\\nthan revolutionary .\\n122Lectur e2:Mor phology and\\x02nite-state techniques\\nThis lecture starts with abrief discussion ofmorphology ,concentrating mainly onEnglish morphology .Theconcept\\nofalexicon inanNLP system isdiscussed with respect tomorphological processing. Spelling rules areintroduced\\nandtheuseof\\x02nite state transducers toimplement spelling rules isexplained. The lecture concludes with abrief\\novervie wofsome other uses of\\x02nite state techniques inNLP.\\n2.1 Averybrief andsimpli\\x02ed introduction tomorphology\\nMorphology concerns thestructure ofwords. Words areassumed tobemade upofmorpheme s,which aretheminimal\\ninformation carrying unit. Morphemes which canonly occur inconjunction with other morphemes areaf\\x02xes:words\\naremade upofastem (more than oneinthecase ofcompounds) andzero ormore af\\x02xes.Forinstance, dogisastem\\nwhich may occur with theplural suf\\x02x+si.e.,dogs.English only hassuf\\x02xes(af\\x02xeswhich come after astem) and\\npre\\x02x es(which come before thestem \\x97inEnglish these arelimited toderivational morphology), butother languages\\nhavein\\x02xes (af\\x02xeswhich occur inside thestem) andcircum\\x02x es(af\\x02xeswhich goaround astem). Forinstance,\\nArabic hasstems (root forms) such asktb,which arecombined with in\\x02xestoform words (e.g., kataba ,hewrote;\\nkotob,books). Some English irregular verbs showarelic ofin\\x03ection byin\\x02xation (e.g. sing,sang ,sung )butthis\\nprocess isnolonger productive (i.e., itwon\\'tapply toanynewwords, such asping).8\\n2.2 In\\x03ectional vsderivational morphology\\nIn\\x03ectional andderivational morphology canbedistinguished, although thedividing lineisn\\'talwayssharp. The\\ndistinction isofsome importance inNLP,since itmeans different representation techniques may beappropriate.\\nIn\\x03ectional morphology canbethought ofassetting values ofslots insome paradigm .In\\x03ectional morphology\\nconcerns properties such astense, aspect, number ,person, gender ,andcase, although notalllanguages code allof\\nthese: English, forinstance, hasverylittle morphological marking ofcase andgender .Derivational af\\x02xes,such as\\nun-,re-,anti- etc,haveabroader range ofsemantic possibilities anddon\\'t\\x02tintoneat paradigms. In\\x03ectional af\\x02xes\\nmay becombined (though notinEnglish). However,there arealwaysobvious limits tothis, since once allthepossible\\nslotvalues are`set\\', nothing elsecanhappen. Incontrast, there arenoobvious limitations onthenumber ofderivational\\naf\\x02xes(antidisestablishmentarianism ,antidisestablishmentarianismization )andtheymay evenbeapplied recursi vely\\n(antiantimissile ).Insome languages, such asInuit, derivational morphology isoften used where English would use\\nadjecti valmodi\\x02cation orother syntactic means. This leads toverylong `words\\' occurring naturally andispresumably\\nresponsible fortheclaim that`Eskimo\\' hashundreds ofwords forsnow.\\nIn\\x03ectional morphology isgenerally close tofully producti ve,inthesense that awordofaparticular class will\\ngenerally showallthepossible in\\x03ections although theactual af\\x02xused may vary.Forinstance, anEnglish verbwill\\nhaveapresent tense form, a3rdperson singular present tense form, apastparticiple andapassi veparticiple (thelatter\\ntwobeing thesame forregular verbs). This willalso apply toanynewwords which enter thelanguage: e.g., textas\\naverb\\x97texts,texted.Derivational morphology islessproducti veandtheclasses ofwords towhich anaf\\x02xapplies\\nislessclearcut. Forinstance, thesuf\\x02x-eeisrelati velyproducti ve(textee sounds plausible, meaning therecipient\\nofatextmessage, forinstance), butdoesn\\' tapply toallverbs (?snoree,?jogee,?dropee ).Derivational af\\x02xesmay\\nchange thepartofspeech ofaword(e.g., -ise/-izeconvertsnouns intoverbs: plural,pluralise ).However,there are\\nalso examples ofwhat issometimes called zeroderivation ,where asimilar effectisobserv edwithout anaf\\x02x:e.g.\\ntango ,waltz etcarewords which arebasically nouns butcanbeused asverbs.\\nStems andaf\\x02xescanbeindividually ambiguous. There isalsopotential forambiguity inhowawordform issplit into\\nmorphemes. Forinstance, unionised could beunion -ise-edor(inchemistry) un-ion-ise-ed.This sortofstructural\\nambiguity isn\\'tnearly ascommon inEnglish morphology asinsyntax, however.Note thatun-ionisnotapossible\\nform (because un-can\\'tattach toanoun). Furthermore, although there isapre\\x02x un-thatcanattach toverbs, itnearly\\nalwaysdenotes areversal ofaprocess (e.g., untie ),whereas theun-thatattaches toadjecti vesmeans `not\\', which is\\nthemeaning inthecase ofun-ion-ise-ed.Hence theinternal structure ofun-ion-ise-edhastobe(un- ((ion -ise)\\n-ed)) .\\n8Arguably ,though, spok enEnglish hasoneproducti vein\\x02xation process, exempli\\x02ed byabsobloodylutely .\\n132.3 Spelling rules\\nEnglish morphology isessentially concatenati ve:i.e.,wecanthink ofwords asasequence ofpre\\x02x es,stems and\\nsuf\\x02xes.Some words haveirregular morphology andtheir in\\x03ectional forms simply havetobelisted. However,in\\nother cases, there areregular phonological orspelling changes associated with af\\x02xation. Forinstance, thesuf\\x02x-sis\\npronounced differently when itisadded toastem which ends ins,xorzandthespelling re\\x03ects thiswith theaddition\\nofane(boxes etc). Forthepurposes ofthiscourse, we\\'lljusttalkabout spelling effects rather than phonological\\neffects: these effects canbecaptured byspelling rules (also knownasortho graphic rules ).\\nEnglish spelling rules canbedescribed independently oftheparticular stems andaf\\x02xesinvolved,simply interms of\\ntheaf\\x02xboundary .The`e-insertion\\' rulecanbedescribed asfollows:\\n\"!e=8\\n<\\n:s\\nx\\nz9\\n=\\n;^s\\nInsuch rules, themapping isalwaysgivenfrom the`underlying\\' form tothesurfaceform, themapping isshownto\\ntheleftoftheslash andtheconte xttotheright, with the indicating theposition inquestion.\"isused fortheempty\\nstring and^fortheaf\\x02xboundary .This particular ruleisread assaying thattheempty string maps to`e\\'intheconte xt\\nwhere itispreceded byans,x,orzandanaf\\x02xboundary andfollowed byans.Forinstance, thismaps box^stoboxes .\\nThis rulemight look asthough itiswritten inaconte xtsensiti vegrammar formalism, butactually we\\'llseeinx2.7\\nthatitcorresponds toa\\x02nite state transducer .Because theruleisindependent oftheparticular af\\x02x,itapplies equally\\ntotheplural form ofnouns andthe3rdperson singular present form ofverbs. Other spelling rules inEnglish include\\nconsonant doubling (e.g., rat,ratted ,though note, not*auditted )andy/ieconversion (party ,parties ).\\n2.4 Applications ofmorphological processing\\nItispossible touseafull-form lexicon forEnglish NLP: i.e.,tolistallthein\\x03ected forms andtotreat derivational\\nmorphology asnon-producti ve.However,when anewwordhastobetreated (generally because theapplication is\\nexpanded butinprinciple because anewwordhasentered thelanguage) itisredundant tohavetospecify (orlearn)\\nthein\\x03ected forms aswell asthestem, since thevastmajority ofwords inEnglish haveregular morphology .Soa\\nfull-form lexicon isbestregarded asaform ofcompilation. Manyother languages havemanymore in\\x03ectional forms,\\nwhich increases theneed todomorphological analysis rather than full-form listing.\\nIRsystems usestemming rather than fullmorphological analysis. ForIR,what isrequired istorelate forms, notto\\nanalyse them compositionally ,andthiscanmost easily beachie vedbyreducing allmorphologically comple xforms\\ntoacanonical form. Although thisisreferred toasstemming, thecanonical form may notbethelinguistic stem.\\nThemost commonly used algorithm isthePorter stemmer ,which uses aseries ofsimple rules tostrip endings (see\\nJ&M, section 3.4) without theneed foralexicon. However,stemming does notnecessarily help IR.Search engines\\nsometimes doin\\x03ectional morphology ,butthiscanbedangerous. Forinstance, onesearch engine searches forcorpus\\naswell ascorpor awhen giventhelatter asinput, resulting inalargenumber ofspurious results involving Corpus\\nChristi andsimilar terms.\\nInmost NLP applications, however,morphological analysis isaprecursor tosome form ofparsing. Inthiscase, the\\nrequirement istoanalyse theform into astem andaf\\x02xessothatthenecessary syntactic (and possibly semantic)\\ninformation canbeassociated with it.Morphological analysis isoften called lemmatization .Forinstance, forthepart\\nofspeech tagging application which wewilldiscuss inthenextlecture, muggedwould beassigned apartofspeech\\ntagwhich indicates itisaverb, though mug isambiguous between verbandnoun. Forfullparsing, asdiscussed\\ninlectures 4and5,we\\'llneed more detailed syntactic andsemantic information. Morphological generation takesa\\nstem andsome syntactic information andreturns thecorrect form. Forsome applications, there isarequirement that\\nmorphological processing isbidir ectional :thatis,canbeused foranalysis andgeneration. The\\x02nite state transducers\\nwewilllook atbelowhavethisproperty .\\n2.5 Lexical requir ements formorphological processing\\nThere arethree sorts oflexical information thatareneeded forfull, high precision morphological processing:\\n14\\x0faf\\x02xes,plus theassociated information conveyedbytheaf\\x02x\\n\\x0firregular forms, with associated information similar tothatforaf\\x02xes\\n\\x0fstems with syntactic categories (plus more detailed information ifderivational morphology istobetreated as\\nproducti ve)\\nOne approach toanaf\\x02xlexicon isforittoconsist ofapairing ofaf\\x02xandsome encoding ofthesyntactic/semantic\\neffectoftheaf\\x02x.9Forinstance, consider thefollowing fragment ofasuf\\x02xlexicon (wecanassume there isaseparate\\nlexicon forpre\\x02x es):\\nedPAST_VERB\\nedPSP_VERB\\nsPLURAL_NOUN\\nHerePAST_VERB ,PSP_VERB andPLURAL_NOUN areabbre viations forsome bundle ofsyntactic/semantic infor -\\nmation: we\\'lldiscuss thisbrie\\x03y inx5.7.\\nAlexicon ofirregular forms isalso needed. One approach isforthistojustbeatriple consisting ofin\\x03ected form,\\n`af\\x02xinformation\\' andstem, where `af\\x02xinformation\\' corresponds towhate verencoding isused fortheregular af\\x02x.\\nForinstance:\\nbeganPAST_VERB begin\\nbegunPSP_VERB begin\\nNote thatthisinformation canbeused forgeneration aswell asanalysis, ascantheaf\\x02xlexicon.\\nInmost cases, English irregular forms arethesame forallsenses ofaword. Forinstance, ranisthepast ofrun\\nwhether wearetalking about athletes, politicians ornoses. This argues forassociating irregularity with particular\\nwordforms rather than particular senses, especially since compounds also tend tofollowtheirregular spelling, even\\nnon-producti velyformed ones (e.g., theplural ofdormouse isdormice ).However,there areexceptions: e.g., The\\nwashing washung/*hang edouttodryvsthemurdererwashang ed.\\nMorphological analysers also generally haveaccess toalexicon ofregular stems. This isneeded forhigh precision:\\ne.g.toavoidanalysing corpus ascorpu -sweneed toknowthatthere isn\\'tawordcorpu .There arealso cases where\\nhistorically awordwasderived,butwhere thebase form isnolonger found inthelanguage: wecanavoidanalysing\\nunkempt asun-kempt ,forinstance, simply bynothaving kempt inthestem lexicon. Ideally thislexicon should have\\nsyntactic information: forinstance, feed could befee-ed,butsince feeisanoun rather than averb,thisisn\\'tapossible\\nanalysis. However,intheapproach we\\'llassume, themorphological analyser issplit into twostages. The \\x02rst of\\nthese only concerns morpheme forms andreturns both fee-edandfeed giventheinput feed.Asecond stage which is\\nclosely coupled tothesyntactic analysis then rules outfee-edbecause theaf\\x02xandstem syntactic information arenot\\ncompatible (seex5.7foroneapproach tothis).\\nIfmorphology waspurely concatenati ve,itwould beverysimple towrite analgorithm tosplit offaf\\x02xes.Spelling\\nrules complicate thissome what: infact,it\\'sstillpossible todoareasonable jobforEnglish with adhoccode, buta\\ncleaner andmore general approach istouse\\x02nite state techniques.\\n2.6 Finite state automata forrecognition\\nThe approach tospelling rules thatwe\\'lldescribe involvestheuseof\\x02nite state transducers (FSTs). Rather than\\njumping straight intothis, we\\'llbrie\\x03y consider thesimpler \\x02nite state automata andhowtheycanbeused inasimple\\nrecogniser .Suppose wewanttorecognise dates (just dayandmonth pairs) written intheformat day/month. Theday\\nandthemonth may beexpressed asoneortwodigits (e.g. 11/2, 1/12 etc). This format corresponds tothefollowing\\nsimple FSA, where each character corresponds toonetransition:\\n9J&M describe analternati veapproach which istomakethesyntactic information correspond toalevelina\\x02nite state transducer .However,at\\nleast forEnglish, thisconsiderably complicates thetransducers.\\n150,1,2,3 digit / 0,1 0,1,2\\ndigit digit1 2 3 4 5 6\\nAccept states areshownwith adouble circle. This isanon-deterministic FSA: forinstance, aninput starting with the\\ndigit 3will movetheFSA toboth state 2andstate 3.This corresponds toalocal ambiguity :i.e.,onethatwill be\\nresolv edbysubsequent conte xt.Byconvention, there must beno`left over\\'characters when thesystem isinthe\\x02nal\\nstate.\\nTomakethisabitmore interesting, suppose wewanttorecognise acomma-separated listofsuch dates. TheFSA,\\nshownbelow,nowhasacycle andcanaccept asequence ofinde\\x02nite length (note thatthisisiteration andnotfull\\nrecursion, however).\\n0,1,2,3 digit / 0,1 0,1,2\\ndigit digit\\n,1 2 3 4 5 6\\nBoth these FSAs willaccept sequences which arenotvaliddates, such as37/00. Conversely ,ifweusethem togenerate\\n(random) dates, wewillgetsome invalidoutput. Ingeneral, asystem which generates output which isinvalidissaid\\ntoovergenerate.Infact,inmanylanguage applications, some amount ofovergeneration canbetolerated, especially if\\nweareonly concerned with analysis.\\n2.7 Finite state transducers\\nFSAs canbeused torecognise particular patterns, butdon\\'t,bythemselv es,allowforanyanalysis ofwordforms.\\nHence formorphology ,weuse\\x02nite state transducers (FSTs) which allowthesurfacestructure tobemapped intothe\\nlistofmorphemes. FSTs areuseful forboth analysis andgeneration, since themapping isbidirectional. This approach\\nisknownastwo-le velmorpholo gy.\\nToillustrate two-levelmorphology ,consider thefollowing FST,which recognises theaf\\x02x-sallowing forenviron-\\nments corresponding tothee-insertion spelling ruleshowninx2.3.10\\n10Actually ,I\\'vesimpli\\x02ed thisslightly sothecorrespondence tothespelling ruleisnotexact: J&M giveamore comple xtransducer which isan\\naccurate re\\x03ection ofthespelling rule.\\n161other :other\\n\":^\\n2s:s\\n3\\n4other :others:sx:xz:z e:^\\ns:sx:xz:z\\nTransducers map between tworepresentations, soeach transition corresponds toapair ofcharacters. Aswith the\\nspelling rule, weusethespecial character `\"\\'tocorrespond totheempty character and`^\\'tocorrespond toanaf\\x02x\\nboundary .Theabbre viation `other :other\\' means thatanycharacter notmentioned speci\\x02cally intheFST maps to\\nitself. Aswith theFSA example, weassume thattheFST only accepts aninput iftheendoftheinput corresponds to\\nanaccept state (i.e., no`left-o ver\\'characters areallowed).\\nForinstance, with thisFST,`dogs\\'maps to`dog\\x88s\\',`foxes\\'maps to`fox\\x88s\\'and`buzzes\\'maps to`bu\\nzz\\x88s\\'.When thetransducer isruninanalysis mode, thismeans thesystem candetect anaf\\x02xboundary (and hence\\nlook upthestem andtheaf\\x02xintheappropriate lexicons). Ingeneration mode, itcanconstruct thecorrect string. This\\nFST isnon-deterministic.\\nSimilar FSTs canbewritten fortheother spelling rules forEnglish (although todoconsonant doubling correctly ,in-\\nformation about stress andsyllable boundaries isrequired andthere arealsodifferences between British andAmerican\\nspelling conventions which complicate matters). Morphology systems areusually implemented sothatthere isone\\nFST perspelling ruleandthese operate inparallel.\\nOne issue with thisuseofFSTs isthattheydonotallowforanyinternal structure ofthewordform. Forinstance, we\\ncanproduce asetofFSTs which willresult inunionised being mapped intoun^ion^ise^ed,butaswe\\'veseen, the\\naf\\x02xesactually havetobeapplied intheright order andthisisn\\'tmodelled bytheFSTs.\\n2.8 Some other uses of\\x02nite state techniques inNLP\\n\\x0fGrammars forsimple spok endialogue systems. Finite state techniques arenotadequate tomodel grammars\\nofnatural languages: we\\'lldiscuss thisalittle inx4.12. However,forverysimple spok endialogue systems,\\na\\x02nite-state grammar may beadequate. More comple xgrammars canbewritten asCFGs andcompiled into\\n\\x02nite state approximations.\\n\\x0fPartial grammars fornamed entity recognition (brie\\x03y discussed inx4.12).\\n\\x0fDialogue models forspok endialogue systems (SDS). SDS usedialogue models foravariety ofpurposes: in-\\ncluding controlling thewaythattheinformation acquired from theuser isinstantiated (e.g., theslots thatare\\n\\x02lled inanunderlying database) andlimiting thevocabulary toachie vehigher recognition rates. FSAs canbe\\nused torecord possible transitions between states inasimple dialogue. Forinstance, consider theproblem of\\n17obtaining adate expressed asadayandamonth from auser.There arefour possible states, corresponding to\\ntheuser input recognised sofar:\\n1.Noinformation. System prompts formonth andday.\\n2.Month only isknown.System prompts forday.\\n3.Day only isknown.System prompts formonth.\\n4.Month anddayknown.\\nTheFSA isshownbelow.Theloops thatstayinasingle state correspond touserresponses thataren\\' trecognised\\nascontaining therequired information (mumble istheterm generally used foranunrecognised input).\\n1mumble\\nmonth day\\nday&month2mumble\\nday3mumble\\nmonth\\n4\\n2.9 Probabilistic FSAs\\nInmanycases, itisuseful toaugment theFSA with information about transition probabilities. Forinstance, inthe\\nSDS system described above,itismore likelythatauser willspecify amonth alone than adayalone. Aprobabilistic\\nFSA fortheSDS isshownbelow.Note thattheprobabilities ontheoutgoing arcsfrom each state must sum to1.\\n10.1\\n0.5 0.1\\n0.3 20.1\\n0.930.2\\n0.8\\n4\\n2.10 Further reading\\nChapters 2and3ofJ&M. Much ofChapter 2should befamiliar from other courses intheCST (atleast toPartII\\nstudents). Chapter 3uses more elaborate transducers than I\\'vediscussed.\\n183Lectur e3:Prediction andpart-of-speech tagging\\nThis lecture introduces some simple statistical techniques andillustrates their useinNLP forprediction ofwords and\\npart-of-speech categories. Itstarts with adiscussion ofcorpora, then introduces wordprediction. Wordprediction can\\nbeseen asawayof(crudely) modelling some syntactic information (i.e., wordorder). Similar statistical techniques\\ncanalsobeused todisco verparts ofspeech foruses ofwords inacorpus. Thelecture concludes with some discussion\\nofevaluation.\\n3.1 Corpora\\nAcorpus (corpora istheplural) issimply abody oftextthat hasbeen collected forsome purpose. Abalanced\\ncorpus contains textswhich represent different genres (newspapers, \\x02ction, textbooks, parliamentary reports, cooking\\nrecipes, scienti\\x02c papers etcetc): early examples were theBrowncorpus (USEnglish) andtheLancaster -Oslo-Ber gen\\n(LOB) corpus (British English) which areeach about 1million words: themore recent British National Corpus (BNC)\\ncontains approx 100million words andincludes 20million words ofspok enEnglish. Corpora areimportant formany\\ntypes oflinguistic research, although mainstream linguists havetended todismiss their useinfavourofreliance on\\nintuiti vejudgements. Corpora areessential formuch modern NLP research, though NLP researchers haveoften used\\nnewspaper text(particularly theWallStreet Journal) rather than balanced corpora.\\nDistrib uted corpora areoften annotated insome way:themost important type ofannotation forNLP ispart-of-speech\\ntagging (POS tagging), which we\\'lldiscuss further below.\\nCorpora may also becollected foraspeci\\x02c task. Forinstance, when implementing anemail answering application,\\nitisessential tocollect samples ofrepresentati veemails. Forinterf aceapplications inparticular ,collecting acorpus\\nrequires asimulation oftheactual application: generally thisisdone byaWizardofOzexperiment, where ahuman\\npretends tobeacomputer .\\nCorpora areneeded inNLP fortworeasons. Firstly ,wehavetoevaluate algorithms onreallanguage: corpora are\\nrequired forthispurpose foranystyle ofNLP.Secondly ,corpora provide thedata source formanymachine-learning\\napproaches.\\n3.2 Prediction\\nTheessential idea ofprediction isthat, givenasequence ofwords, wewanttodetermine what\\' smost likelytocome\\nnext.There areanumber ofreasons towanttodothis: themost important isasaform oflangua gemodelling for\\nautomatic speech recognition. Speech recognisers cannot accurately determine awordfrom thesound signal forthat\\nwordalone, andtheycannot reliably tellwhere each wordstarts and\\x02nishes.11Sothemost probable wordischosen\\nonthebasis ofthelanguage model, which predicts themost likelyword,giventheprior conte xt.Thelanguage models\\nwhich arecurrently most effectiveworkonthebasis ofN-grams(atype ofMark ovchain),where thesequence ofthe\\npriorn\\x001words isused topredict thenext.Trigram models usethepreceding 2words, bigram models thepreceding\\nwordandunigram models usenoconte xtatall,butsimply workonthebasis ofindividual wordprobabilities. Bigrams\\narediscussed below,though Iwon\\'tgointodetails ofexactly howtheyareused inspeech recognition.\\nWordprediction isalsouseful incommunication aids: i.e.,systems forpeople who can\\'tspeak because ofsome form\\nofdisability .People who usetext-to-speech systems totalkbecause ofanon-linguistic disability usually havesome\\nform ofgeneral motor impairment which also restricts their ability totype atnormal rates (strok e,ALS, cerebral\\npalsy etc). Often theyusealternati veinput devices, such asadapted keyboards, pufferswitches, mouth sticks or\\neyetrack ers. Generally such users canonly construct textatafewwords aminute, which istooslowforanything\\nlikenormal communication tobepossible (normal speech isaround 150words perminute). Asapartial aid,aword\\nprediction system issometimes helpful: thisgivesalistofcandidate words thatchanges astheinitial letters areentered\\nbytheuser.Theuser chooses thedesired wordfrom amenu when itappears. Themain dif\\x02culty with using statistical\\nprediction models insuch applications isin\\x02nding enough data: tobeuseful, themodel really hastobetrained onan\\nindividual speak er\\'soutput, butofcourse verylittle ofthisislikelytobeavailable.\\n11Infact,although humans arebetter atdoing thisthan speech recognisers, wealso need conte xttorecognise words, especially words likethe\\nanda.\\n19Prediction isimportant inestimation ofentrop y,including estimations oftheentrop yofEnglish. Thenotion ofentrop y\\nisimportant inlanguage modelling because itgivesametric forthedif\\x02culty oftheprediction problem. Forinstance,\\nspeech recognition ismuch easier insituations where thespeak erisonly saying twoeasily distinguishable words than\\nwhen thevocabulary isunlimited: measurements ofentrop ycanquantify this, butwon\\'tbediscussed further inthis\\ncourse.\\nOther applications forprediction include optical character recognition (OCR), spelling correction andtextsegmen-\\ntation forlanguages such asChinese, which areconventionally written without explicit wordboundaries. Some ap-\\nproaches towordsense disambiguation, tobediscussed inlecture 6,canalsobetreated asaform ofprediction.\\n3.3 bigrams\\nAbigram model assigns aprobability toawordbased ontheprevious word: i.e.P(wnjwn\\x001)wherewnisthenth\\nwordinsome string. Theprobability ofsome string ofwordsP(Wn\\n1)isthus approximated bytheproduct ofthese\\nconditional probabilities:\\nP(Wn\\n1)\\x19nY\\nk=1P(wkjwk\\x001)\\nForexample, suppose wehavethefollowing tinycorpus ofutterances:\\ngood morning\\ngood afternoon\\ngood afternoon\\nitisverygood\\nitisgood\\nWe\\'llusethesymbolhsitoindicate thestart ofanutterance, sothecorpus really looks like:\\nhsigood morninghsigood afternoonhsigood afternoonhsiitisverygoodhsiitisgoodhsi\\nThebigram probabilities aregivenas\\nC(wn\\x001wn)P\\nwC(wn\\x001w)\\ni.e.thecount ofaparticular bigram, normalised bydividing bythetotal number ofbigrams starting with thesame\\nword(which isequivalent tothetotal number ofoccurrences ofthatword, except inthecase ofthelasttoken,a\\ncomplication which canbeignored forareasonable sizeofcorpus).\\nsequence count bigramprobability\\n<s> 5\\n<s>good 3 .6\\n<s>it 2 .4\\ngood 5\\ngoodmorning 1 .2\\ngoodafternoon 2 .4\\ngood<s> 2 .4\\nmorning 1\\nmorning <s> 1 1\\nafternoon 2\\nafternoon <s>2 1\\nitis 2 1\\nisvery 1 .5\\nisgood 1 .5\\nverygood 1 1\\n20This yields aprobability of0.24 forthestring `hsigoodhsi\\'which isthehighest probability utterance thatwecan\\nconstruct onthebasis ofthebigrams from thiscorpus, ifweimpose theconstraint thatanutterance must beginwith\\nhsiandendwithhsi.\\nForapplication tocommunication aids, wearesimply concerned with predicting thenextword: once theuser has\\nmade their choice, thewordcan\\'tbechanged. Forspeech recognition, theN-gram approach isapplied tomaximise\\nthelikelihood ofasequence ofwords, hence we\\'relooking to\\x02ndthemost likelysequence overall. Notice thatwe\\ncanregardbigrams ascomprising asimple deterministic weighted FSA. TheViterbi algorithm ,anef\\x02cient method of\\napplying N-grams inspeech recognition andother applications, isusually described interms ofanFSA.\\nTheprobability of`hsiverygood\\' based onthiscorpus is0,since theconditional probability of`very\\'given`hsi\\'is0\\nsince wehaven\\'tfound anyexamples ofthisinthetraining data. Ingeneral, thisisproblematic because wewillnever\\nhaveenough data toensure thatwewillseeallpossible eventsandsowedon\\'twanttoruleoutunseen eventsentirely .\\nToallowforsparsedata wehavetousesmoothing ,which simply means thatwemakesome assumption about the\\n`real\\' probability ofunseen orveryinfrequently seen events anddistrib utethatprobability appropriately .Acommon\\napproach issimply toaddonetoallcounts: thisisadd-one smoothing which isnotsound theoretically ,butissimple\\ntoimplement. Abetter approach inthecase ofbigrams istobackofftotheunigram probabilities: i.e.,todistrib ute\\ntheunseen probability mass sothatitisproportional totheunigram probabilities. This sortofestimation isextremely\\nimportant togetgood results from N-gram techniques, butwewon\\'tdiscuss thedetails inthiscourse.\\n3.4 Partofspeech tagging\\nPrediction techniques canbeused forwordclasses, rather than justindividual words. One important application isto\\npart-of-speech tagging (POS tagging), where thewords inacorpus areassociated with atagindicating some syntactic\\ninformation thatapplies tothatparticular useoftheword. Forinstance, consider theexample sentence below:\\nTheycan\\x02sh.\\nThis hastworeadings: one(themost likely)about ability to\\x02shandother about putting \\x02shincans. \\x02shisambiguous\\nbetween asingular noun, plural noun andaverb,while canisambiguous between singular noun, verb(the `put in\\ncans\\' use) andmodal verb.However,theyisunambiguously apronoun. (Iamignoring some lesslikelypossibilities,\\nsuch asproper names.) These distinctions canbeindicated byPOS tags:\\ntheyPNP\\ncanVM0VVBVVINN1\\nfishNN1NN2VVBVVI\\nThere areseveralstandard tagsets used incorpora andinPOS tagging experiments. TheoneI\\'musing fortheexamples\\ninthislecture isCLA WS5(C5) which isgiveninfullinappendix CinJ&M. Themeaning ofthetagsaboveis:\\nNN1singular noun\\nNN2pluralnoun\\nPNPpersonal pronoun\\nVM0modalauxiliary verb\\nVVBbaseformofverb(except infinitive)\\nVVIinfinitive formofverb(i.e.occurswith`to\\')\\nAPOS tagger resolv esthelexical ambiguities togivethemost likelysetoftagsforthesentence. Inthiscase, theright\\ntagging islikelytobe:\\nTheyPNP canVM0 \\x02shVVB .PUN\\nNote thetagforthefullstop: punctuation istreated asunambiguous. POS tagging canberegarded asaform ofvery\\nbasic wordsense disambiguation.\\nTheother syntactically possible reading is:\\nTheyPNP canVVB \\x02shNN2 .PUN\\n21However,POS taggers (unlik efullparsers) don\\'tattempt toproduce globally coherent analyses. Thus aPOS tagger\\nmight return:\\nTheyPNP canVM0 \\x02shNN2 .PUN\\ndespite thefactthatthisdoesn\\' tcorrespond toapossible reading ofthesentence.\\nPOS tagging isuseful asawayofannotating acorpus because itmakesiteasier toextract some types ofinformation\\n(forlinguistic research orNLP experiments). Italso actsasabasis formore comple xforms ofannotation. Named\\nentity recognisers (discussed inlecture 4)aregenerally runonPOS-tagged data. POS taggers aresometimes runas\\npreprocessors tofullparsing, since thiscancutdownthesearch space tobeconsidered bytheparser .\\n3.5 Stochastic POS tagging\\nOne form ofPOS tagging applies theN-gram technique thatwesawabove,butinthiscase itapplies tothePOS\\ntags rather than theindividual words. Themost common approaches depend onasmall amount ofmanually tagged\\ntraining data from which POS N-grams canbeextracted.12I\\'llillustrate thiswith respect toanother trivialcorpus:\\nTheyused tocan\\x02shinthose towns. Butnowfewpeople \\x02shthere.\\nThis might betagged asfollows:\\nThey_PNP used_VVD to_TO0can_VVI fish_NN2 in_PRPthose_DT0 towns_NN2 ._PUN\\nBut_CJC now_AV0 few_DT0 people_NN2 fish_VVB in_PRPthese_DT0 areas_NN2 ._PUN\\nThis yields thefollowing counts andprobabilities:\\nsequence count bigramprobability\\nAV0 1\\nAV0DT0 1 1\\nCJC 1\\nCJCAV0 1 1\\nDT0 3\\nDT0NN2 3 1\\nNN2 4\\nNN2PRP 1 0.25\\nNN2PUN 2 0.5\\nNN2VVB 1 0.25\\nPNP 1\\nPNPVVD 1 1\\nPRP 1\\nPRPDT0 2 1\\nPUN 1\\nPUNCJC 1 1\\nTO0 1\\n12Itispossible tobuildPOS taggers thatworkwithout ahand-tagged corpus, buttheydon\\'tperform aswell asasystem trained onevena1,000\\nwordcorpus which canbetagged inafewhours. Furthermore, these algorithms stillrequire alexicon which associates possible tagswith words.\\n22TO0VVI 1 1\\nVVB 1\\nVVBPRP 1 1\\nVVD 1\\nVVDTO0 1 1\\nVVI 1\\nVVINN2 1 1\\nWecanalsoobtain alexicon from thetagged data:\\nwordtagcount\\ntheyPNP1\\nusedVVD1\\ntoTO01\\ncanVVI1\\nfishNN21\\nVVB1\\ninPRP2\\nthoseDT01\\ntownsNN21\\n.PUN1\\nbutCJC1\\nnowAV01\\nfewDT01\\npeopleNN21\\ntheseDT01\\nareasNN21\\nTheidea ofstochastic POS tagging isthatthetagcanbeassigned based onconsideration ofthelexical probability\\n(howlikelyitisthatthewordhasthattag), plus thesequence ofprior tags. Forabigram model, weonly look at\\nasingle previous tag. This isslightly more complicated than thewordprediction case because wehavetotakeinto\\naccount both words andtags.\\nWearetrying toestimate theprobability ofasequence oftags givenasequence ofwords:P(TjW).ByBayes\\ntheorem:\\nP(TjW)=P(T)P(WjT)\\nP(W)\\nSince we\\'relooking atassigning tags toaparticular sequence ofwords,P(W)isconstant, soforarelati vemeasure\\nofprobability wecanuse:\\nP(TjW)=P(T)P(WjT)\\nWenowhavetoestimateP(T)andP(WjT).Ifwemakethebigram assumption, P(T)isapproximated byP(tijti\\x001)\\n\\x97i.e.,theprobability ofsome taggiventheimmediately preceding tag.Weapproximate P(WjT)asP(wijti).These\\nvalues canbeestimated from thecorpus frequencies.\\nNote thatweendupmultiplying P(tijti\\x001)withP(wijti)(the probability ofthewordgiventhetag) rather than\\nP(tijwi)(the probability ofthetaggiventheword). Forinstance, ifwe\\'retrying tochoose between thetags NN2\\nandVVB for\\x02shinthesentence they\\x02sh,wecalculateP(NN2ijPNPi\\x001),P(\\x02shijNN2i),P(VVBijPNPi\\x001)and\\nP(\\x02shijVVBi).\\nInfact,POS taggers generally usetrigrams rather than bigrams \\x97therelevantequations aregiveninJ&M, page 306.\\nAswith wordprediction, backoffandsmoothing arecrucial forreasonable performance.\\nWhen aPOS tagger sees awordwhich wasnotinitstraining data, weneed some wayofassigning possible tagstothe\\nword. One approach issimply touseallpossible open class tags, with probabilities based ontheunigram probabilities\\n23ofthose tags. Open class words areones forwhich wecannevergiveacomplete listforaliving language, since words\\narealwaysbeing added: i.e.,verbs, nouns, adjecti vesandadverbs. The restareconsidered closed class. Abetter\\napproach istouseamorphological analyser torestrict thisset:e.g., words ending in-edarelikelytobeVVD (simple\\npast) orVVN (past participle), butcan\\'tbeVVG(-ing form).\\n3.6 Evaluation ofPOS tagging\\nPOS tagging algorithms areevaluated interms ofpercentage ofcorrect tags. Thestandard assumption isthatevery\\nwordshould betagged with exactly onetag,which isscored ascorrect orincorrect: there arenomarks fornear\\nmisses. Generally there aresome words which canbetagged inonly oneway,soareautomatically counted ascorrect.\\nPunctuation isgenerally givenanunambiguous tag. Therefore thesuccess rates ofover95% which aregenerally\\nquoted forPOS tagging arealittle misleading: thebaseline ofchoosing themost common tagbased onthetraining\\nsetoften gives90% accurac y.Some POS taggers returns multiple tagsincases where more than onetaghasasimilar\\nprobability .\\nItisworth noting thatincreasing thesize ofthetagset does notnecessarily result indecreased performance: this\\ndepends onwhether thetagsthatareadded cangenerally beassigned unambiguously ornot.Potentially ,adding more\\n\\x02ne-grained tags could increase performance. Forinstance, suppose wewanted todistinguish between present tense\\nverbs according towhether theywere 1st,2ndor3rdperson. WiththeC5tagset, andthestochastic tagger described,\\nthiswould beimpossible todowith high accurac y,because allpronouns aretagged PRP,hence theyprovide no\\ndiscriminating power.Ontheother hand, ifwetagged IandweasPRP1, youasPRP2 andsoon,theN-gram approach\\nwould allowsome discrimination. Ingeneral, predicting onthebasis ofclasses means wehavelessofasparse data\\nproblem than when predicting onthebasis ofwords, butwealso lose discriminating power.There isalso something\\nofatradeof fbetween theutility ofasetoftagsandtheir usefulness inPOS tagging. Forinstance, C5assigns separate\\ntags forthedifferent forms ofbe,which isredundant formanypurposes, buthelps makedistinctions between other\\ntagsintagging models where theconte xtisgivenbyatagsequence alone (i.e., rather than considering words prior to\\nthecurrent one).\\nPOS tagging exempli\\x02es some general issues inNLP evaluation:\\nTraining data andtestdata The assumption inNLP isalwaysthatasystem should workonnoveldata, therefore\\ntestdata must bekeptunseen.\\nFormachine learning approaches, such asstochastic POS tagging, theusual technique istospilt adata setinto\\n90% training and10% testdata. Care needs tobetakenthatthetestdata isrepresentati ve.\\nForanapproach thatrelies onsigni\\x02cant hand-coding, thetestdata should beliterally unseen bytheresearchers.\\nDevelopment cycles involvelooking atsome initial data, developing thealgorithm, testing onunseen data,\\nrevising thealgorithm andtesting onanewbatch ofdata. Theseen data iskeptforregression testing.\\nBaselines Evaluation should bereported with respect toabaseline, which isnormally what could beachie vedwith a\\nverybasic approach, giventhesame training data. Forinstance, thebaseline forPOS tagging with training data\\nistochoose themost common tagforaparticular wordonthebasis ofthetraining data (and tosimply choose\\nthemost frequent tagofallforunseen words).\\nCeiling Itisoften useful totryandcompute some sortofceiling fortheperformance ofanapplication. This isusually\\ntakentobehuman performance onthattask, where theceiling isthepercentage agreement found between two\\nannotators (interannotator agreement ).ForPOS tagging, thishasbeen reported as96% (which makesexisting\\nPOS taggers look impressi ve).Howeverthisraises lotsofquestions: relati velyuntrained human annotators\\nworking independently often havequite lowagreement, buttrained annotators discussing results canachie ve\\nmuch higher performance (approaching 100% forPOS tagging). Human performance varies considerably be-\\ntween individuals. Inanycase, human performance may notbearealistic ceiling onrelati velyunnatural tasks,\\nsuch asPOS tagging.\\nErroranalysis Theerror rateonaparticular problem willbedistrib uted veryunevenly.Forinstance, aPOS tagger\\nwillneverconfuse thetagPUN with thetagVVN (past participle), butmight confuse VVN with AJ0(adjecti ve)\\nbecause there\\' sasystematic ambiguity formanyforms (e.g., given ).Foraparticular application, some errors\\n24may bemore important than others. Forinstance, ifoneislooking forrelati velylowfrequenc ycases ofde-\\nnominal verbs (that isverbs derivedfrom nouns \\x97e.g., canoe ,tango ,forkused asverbs), then POS tagging is\\nnotdirectly useful ingeneral, because averbal usewithout acharacteristic af\\x02xislikelytobemistagged. This\\nmakesPOS-tagging lessuseful forlexicographers, who areoften speci\\x02cally interested in\\x02nding examples of\\nunusual worduses. Similarly ,intextcategorisation, some errors aremore important than others: e.g. treating\\nanincoming order foranexpensi veproduct asjunk email isamuch worse error than theconverse.\\nRepr oducibility Ifatallpossible, evaluation should bedone onagenerally available corpus sothatother researchers\\ncanreplicate theexperiments.\\n3.7 Further reading\\nThis lecture hasskimmed overmaterial thatiscovered inseveralchapters ofJ&M. See5.9fortheViterbi algorithm,\\nChapter 6forN-grams (especially 6.3,6.4and6.7), 7.1-7.3 forspeech recognition andChapter 8onPOS tagging.\\n254Lectur e4:Parsing andgeneration\\nInthislecture, we\\'lldiscuss syntax inawaywhich ismuch closer tothestandard notions informal linguistics than\\nPOS-tagging is.Tostart with, we\\'llbrie\\x03y motivatetheidea ofagenerati vegrammar inlinguistics, reviewthenotion\\nofaconte xt-free grammar andthen showaconte xt-free grammar foratinyfragment ofEnglish. We\\'llthen show\\nhowconte xtfreegrammars canbeused toimplement generators andparsers, anddiscuss chart parsing, which allows\\nef\\x02cient processing ofstrings containing ahigh degree ofambiguity .Finally we\\'llbrie\\x03y touch onprobabilistic\\nconte xt-free approaches.\\n4.1 Generati vegrammar\\nSince Chomsk y\\'sworkinthe1950s, much workinformal linguistics hasbeen concerned with thenotion ofagenera-\\ntivegrammar \\x97i.e.,aformally speci\\x02ed grammar thatcangenerate allandonly theacceptable sentences ofanatural\\nlanguage. It\\'simportant torealise thatnobody hasactually written such agrammar foranynatural language oreven\\ncome close todoing so:what most linguists arereally interested inistheprinciples thatunderly such grammars, espe-\\ncially totheextent thattheyapply toallnatural languages. NLP researchers, ontheother hand, areatleast sometimes\\ninterested inactually building andusing large-scale detailed grammars.\\nTheformalisms which areofinterest tousformodelling syntax assign internal structure tothestrings ofalanguage,\\nwhich canberepresented bybrack eting. Wealready sawsome evidence ofthisinderivational morphology (the\\nunionised example), buthere weareconcerned with thestructure ofphrases. Forinstance, thesentence:\\nthedogslept\\ncanbebrack eted\\n((the (bigdog)) slept)\\nThephrase, bigdog,isanexample ofaconstituent (i.e. something thatisenclosed inapairofbrack ets): thebigdog\\nisalso aconstituent, butthebigisnot. Constituent structure isgenerally justi\\x02ed byarguments about substitution\\nwhich Iwon\\'tgointohere: J&M discuss thisbrie\\x03y ,butseeanintroductory syntax book forafulldiscussion. Inthis\\ncourse, Iwillsimply givebrack eted structures andhope thattheconstituents makesense intuiti vely,rather than trying\\ntojustify them.\\nTwogrammars aresaid tobeweakly-equivalent iftheygenerate thesame strings. Twogrammars arestrongly-\\nequivalent iftheyassign thesame brack etings toallstrings theygenerate.\\nInmost, butnotall,approaches, theinternal structures aregivenlabels. Forinstance, thebigdogisanoun phrase\\n(abbre viated NP), slept ,slept inthepark andlickedSandy areverb phrases(VPs). Thelabels such asNPandVPcor-\\nrespond tonon-terminal symbols inagrammar .Inthislecture, we\\'lldiscuss theuseofsimple conte xt-free grammars\\nforlanguage description, moving onto amore expressi veformalism inlecture 5.\\n4.2 Context freegrammars\\nTheidea ofaconte xt-free grammar (CFG) should befamiliar from formal language theory .ACFG hasfour compo-\\nnents, described here astheyapply togrammars ofnatural languages:\\n1.asetofnon-terminal symbols (e.g., S,VP), conventionally written inuppercase;\\n2.asetofterminal symbols (i.e., thewords), conventionally written inlowercase;\\n3.asetofrules (productions), where thelefthand side (themother) isasingle non-terminal andtheright hand\\nsideisasequence ofoneormore non-terminal orterminal symbols (thedaughters);\\n4.astart symbol, conventionally S,which isamember ofthesetofnon-terminal symbols.\\n26The formal description ofaCFG generally allowsproductions with anempty righthandside (e.g., Det!\").Itis\\nconvenient toexclude these however,since theycomplicate parsing algorithms, andaweakly-equi valent grammar can\\nalwaysbeconstructed thatdisallo wssuch empty productions .\\nAgrammar inwhich allnonterminal daughters aretheleftmost daughter inarule(i.e., where allrules areoftheform\\nX!Ya\\x03),issaid tobeleft-associative .Agrammar where allthenonterminals arerightmost isright-associative .\\nSuch grammars areweakly-equi valent toregular grammars (i.e., grammars thatcanbeimplemented byFSAs), but\\nnatural languages seem torequire more expressi vepowerthan this(seex4.12).\\n4.3 Asimple CFG forafragment ofEnglish\\nThefollowing tinyfragment isintended toillustrate some oftheproperties ofCFGs sothatwecandiscuss parsing\\nandgeneration. Ithassome serious de\\x02ciencies asarepresentation ofeventhisfragment, which we\\'llignore fornow,\\nthough we\\'lldiscuss some ofthem inlecture 5.\\nS->NPVP\\nVP->VPPP\\nVP->V\\nVP->VNP\\nVP->VVP\\nNP->NPPP\\nPP->PNP\\n;;;lexicon\\nV->can\\nV->fish\\nNP->fish\\nNP->rivers\\nNP->pools\\nNP->December\\nNP->Scotland\\nNP->it\\nNP->they\\nP->in\\nTherules with terminal symbols ontheRHS correspond tothelexicon. Here andbelow,comments arepreceded by\\n;;;\\nHere aresome strings which thisgrammar generates, along with their brack etings:\\nthey\\x02sh\\n(S(NPthey)(VP(V\\x02sh)))\\ntheycan\\x02sh\\n(S(NPthey)(VP(Vcan) (VP(V\\x02sh))))\\n;;;themodal verb`areable to\\'reading\\n(S(NPthey)(VP(Vcan) (NP\\x02sh)))\\n;;;thelessplausible, put\\x02shincans, reading\\nthey\\x02shinrivers\\n(S(NPthey)(VP(VP(V\\x02sh)) (PP(Pin)(NPrivers))))\\nthey\\x02shinriversinDecember\\n(S(NPthey)(VP(VP(V\\x02sh)) (PP(Pin)(NP(NPrivers)(PP(Pin)(NPDecember))))))\\n;;;i.e.theimplausible reading where theriversareinDecember\\n;;;(cfriversinScotland)\\n(S(NPthey)(VP(VP(VP(V\\x02sh)) (PP(Pin)(NP(NPrivers)))) (PP(Pin)(NPDecember))))\\n;;;i.e.the\\x02shing isdone inDecember\\n27One important thing tonotice about these examples isthatthere\\' slotsofpotential forambiguity .Inthetheycan\\x02sh\\nexample, thisisduetolexical ambiguity (itarises from thedual lexical entries ofcanand\\x02sh),butthelastexample\\ndemonstrates purely structur alambiguity .Inthiscase, theambiguity arises from thetwopossible attac hments ofthe\\nprepositional phrase (PP) inDecember :itcanattach totheNP(river s)ortotheVP.These attachments correspond\\ntodifferent semantics, asindicated bytheglosses. PPattachment ambiguities areamajor headache inparsing, since\\nsequences offour ormore PPsarecommon inrealtextsandthenumber ofreadings increases astheCatalan series,\\nwhich isexponential. Other phenomena havesimilar properties: forinstance, compound nouns (e.g. long-stay car\\npark shuttle bus).\\nNotice that\\x02shcould havebeen entered inthelexicon directly asaVP,butthatthiswould cause problems ifwewere\\ndoing derivational morphology ,because wewanttosaythatsuf\\x02xeslike-edapply toVs.Making river setcNPs rather\\nthan nouns isasimpli\\x02cation I\\'veadopted here tokeepthegrammar smaller .\\n4.4 Parse trees\\nParsetrees areequivalent tobrack eted structures, butareeasier toread forcomple xcases. Aparse treeandbrack eted\\nstructure foronereading oftheycan\\x02shinDecember isshownbelow.Thecorrespondence should beobvious.\\nS\\nNP VP\\nthey V VP\\ncan VP PP\\nV\\n\\x02shP NP\\nin December\\n(S(NPthey)(VP(Vcan) (VP(VP(V\\x02sh)) (PP(Pin)(NPDecember)))))\\n4.5 Using agrammar asarandom generator\\nThefollowing simple algorithm illustrates howagrammar canbeused togenerate sentences.\\nExpand catcategory sentence-r ecord:\\nLetpossibilities beasetcontaining alllexical items which match category andallrules with left-hand sidecategory\\nIfpossibilities isempty ,\\nthen fail\\nelse\\nRandomly select apossibility chosen from possibilities\\nIfchosen islexical,\\nthen append ittosentence-r ecord\\nelseexpand catoneach rhscategory inchosen (left toright) with theupdated sentence-r ecord\\nreturn sentence-r ecord\\nForinstance:\\nExpand catS()\\n28possibilities =S->NPVP\\nchosen =S->NPVP\\nExpand catNP()\\npossibilities =it,they,\\x02sh\\nchosen =\\x02sh\\nsentence-record =(\\x02sh)\\nExpand catVP(\\x02sh)\\npossibilities =VP->V,VP->VVP,VP->VNP\\nchosen =VP->V\\nExpand catV(\\x02sh)\\npossibilities =\\x02sh, can\\nchosen =\\x02sh\\nsentence-record =(\\x02sh \\x02sh)\\nObviously ,thestrings generated could bearbitrarily long. Ifinthisnaivegeneration algorithm, weexplored allthe\\nsearch space rather than randomly selecting apossible expansion, thealgorithm wouldn\\' tterminate.\\nReal generation operates from semantic representations, which aren\\' tencoded inthisgrammar ,soinwhat follows\\nwe\\'llconcentrate ondescribing parsing algorithms instead. However,it\\'simportant torealise thatCFGs are,inprin-\\nciple, bidirectional.\\n4.6 Chart parsing\\nInorder toparse with reasonable ef\\x02cienc y,weneed tokeeparecord oftherules thatwehaveapplied sothatwedon\\'t\\nhavetobacktrack andredo workthatwe\\'vedone before. This works forparsing with CFGs because therules are\\nindependent oftheir conte xt:aVPcanalwaysexpand asaVandanNPregardless ofwhether ornotitwaspreceded\\nbyanNPoraV,forinstance. (Insome cases wemay beable toapply techniques thatlook attheconte xttocutdown\\nthesearch space, because wecantellthataparticular ruleapplication isnevergoing tobepartofasentence, butthisis\\nstrictly a\\x02lter: we\\'renevergoing togetincorrect results byreusing partial structures.) This record keeping strate gyis\\nanapplication ofdynamic programming which isused inprocessing formal languages too. InNLP thedata structure\\nused forrecording partial results isgenerally knownasachart andalgorithms forparsing using such structures are\\nreferred toaschart parsers.\\nAchart isalistofedges.Inthesimplest version ofchart parsing, each edge records aruleapplication andhasthe\\nfollowing structure:\\n[id,leftverte x,right verte x,mother category,daughter s]\\nAvertexisanintegerrepresenting apoint intheinput string, asillustrated below:\\n.they.can.fish.\\n0123\\nmother category refers totherulethathasbeen applied tocreate theedge. daughter sisalistoftheedges thatacted\\nasthedaughters forthisparticular ruleapplication: itisthere purely forrecord keeping sothattheoutput ofparsing\\ncanbealabelled brack eting.\\nForinstance, thefollowing edges would beamong those found onthechart after acomplete parse oftheycan\\x02sh\\naccording tothegrammar givenabove(idnumbering isarbitrary):\\nidleftrightmother daughters\\n31 2V (can)\\n42 3NP (fish)\\n52 3V (fish)\\n62 3VP (5)\\n71 3VP (35)\\n81 3VP (34)\\n29Thedaughters fortheterminal ruleapplications aresimply theinput wordstrings. Note thatlocal ambiguities cor-\\nrespond tosituations where aparticular span hasmore than oneassociated edge. We\\'llseebelowthatwecanpack\\nstructures sothatweneverhavetwoedges with thesame category andthesame span, butwe\\'llignore thisforthe\\nmoment (seex4.9). Also, inthischart we\\'reonly recording complete ruleapplications: thisispassive chart parsing.\\nThemore ef\\x02cient active chart isdiscussed below,inx4.10.\\n4.7 Abottom-up passi vechart parser\\nThe following pseudo-code sketch isforaverysimple chart parser .The main function isAdd new edge which is\\ncalled foreach wordintheinput going lefttoright. Add new edge recursi velyscans backw ards looking forother\\ndaughters.\\nParse:\\nInitialise thechart (i.e., clear previous results)\\nForeach wordwordintheinput sentence, letfrombetheleftvertex,tobetheright vertexanddaughter sbe(word)\\nForeach category category thatislexically associated with word\\nAdd new edge from,to,category,daughter s\\nOutput results forallspanning edges\\n(i.e., ones thatcovertheentire input andwhich haveamother corresponding totherootcategory)\\nAdd new edge from,to,category,daughter s:\\nPutedge inchart: [id,from,to,category,daughter s]\\nForeach ruleinthegrammar ofform lhs->cat1...catn\\x001,category\\nFind setoflistsofcontiguous edges [id1,from1,to1,cat1,daughter s1]...[idn\\x001,fromn\\x001,from,catn\\x001,daughter sn\\x001]\\n(such thatto1=from2etc)\\nForeach listofedges, Add new edge from1,to,lhs,(id1...id)\\nNotice thatthismeans thatthegrammar rules areindexedbytheir rightmost category,andthattheedges inthechart\\nmust beindexedbytheir tovertex(because wescan backw ardfrom therightmost category). Consider:\\n.they.can.fish.\\n0123\\nThefollowing diagram showsthechart edges astheyareconstructed inorder (when there isachoice, taking rules in\\napriority order according totheorder theyappear inthegrammar):\\nidleftrightmother daughters\\n10 1NP (they)\\n21 2V (can)\\n31 2VP (2)\\n40 2S (13)\\n52 3V (fish)\\n62 3VP (5)\\n71 3VP (26)\\n80 3S (17)\\n92 3NP (fish)\\n101 3VP (29)\\n110 3S (110)\\nThespanning edges are11and8:theoutput routine togivebrack eted parses simply outputs aleftbrack et,outputs\\nthecategory,recurses through each ofthedaughters andthen outputs aright brack et.So,forinstance, theoutput from\\nedge 11is:\\n(S(NPthey)(VP(Vcan)(NPfish)))\\n304.8 Adetailed trace ofthesimple chart parser\\nParse\\nword=they\\ncategories =NP\\nAdd new edge 0,1,NP,(they)\\nthey can \\x02sh1\\nMatching grammar rules are:\\nVP->VNP\\nPP->PNP\\nNomatching edges corresponding toVorP\\nword=can\\ncategories =V\\nAdd new edge 1,2,V,(can)\\nthey can \\x02sh1 2\\nMatching grammar rules are:\\nVP->V\\nsetofedge lists=f(2)g\\nAdd new edge 1,2,VP,(2)\\nthey can \\x02sh1 23\\nMatching grammar rules are:\\nS->NPVP\\nVP->VVP\\nsetofedge listscorresponding toNPVP=f(1;3)g\\nAdd new edge 0,2,S,(1,3)\\nthey can \\x02sh1 234\\nNomatching grammar rules forS\\nNoedges matching VVP\\n31word=\\x02sh\\ncategories =V,NP\\nAdd new edge 2,3,V,(\\x02sh)\\nthey can \\x02sh1 234\\n5\\nMatching grammar rules are:\\nVP->V\\nsetofedge lists=f(5)g\\nAdd new edge 2,3,VP,(5)\\nthey can \\x02sh1 234\\n56\\nMatching grammar rules are:\\nS->NPVP\\nVP->VVP\\nNoedges match NP\\nsetofedge listsforVVP=f(2;6)g\\nAdd new edge 1,3,VP,(2,6)\\nthey can \\x02sh1 234\\n567\\nMatching grammar rules are:\\nS->NPVP\\nVP->VVP\\nsetofedge listsforNPVP=f(1;7)g\\nAdd new edge 0,3,S,(1,7)\\nthey can \\x02sh1 234\\n5678\\nNomatching grammar rules forS\\nNoedges matching V\\n32Add new edge 2,3,NP,(\\x02sh)\\nthey can \\x02sh1 234\\n5678 9\\nMatching grammar rules are:\\nVP->VNP\\nPP->PNP\\nsetofedge listscorresponding toVNP=f(2;9)g\\nAdd new edge 1,3,VP,(2,9)\\nthey can \\x02sh1 234\\n5678 910\\nMatching grammar rules are:\\nS->NPVP\\nVP->VVP\\nsetofedge listscorresponding toNPVP=f(1;10)g\\nAdd new edge 0,3,S,(1,10)\\nthey can \\x02sh1 234\\n5678 91011\\nNomatching grammar rules forS\\nNoedges corresponding toVVP\\nNoedges corresponding toPNP\\nNofurther words ininput\\nSpanning edges are8and11:Output results for8\\n33(S(NPthey)(VP(Vcan)(VP(Vfish))))\\nOutput results for11\\n(S(NPthey)(VP(Vcan)(NPfish)))\\n4.9 Packing\\nThe algorithm givenaboveisexponential inthecase where there areanexponential number ofparses. The body\\nofthealgorithm canbemodi\\x02ed sothatitruns incubic time, though producing theoutput isstillexponential. The\\nmodi\\x02cation issimply tochange thedaughters value onanedge tobeasetoflistsofdaughters andtomakeanequality\\ncheck before adding anedge sowedon\\'taddonethat\\'sequivalent toanexisting one. That is,ifweareabout toadd\\nanedge:\\n[id,leftverte x,right verte x,mother category,daughter s]\\nandthere isanexisting edge:\\n[id-old ,leftverte x,right verte x,mother category,daughter s-old ]\\nwesimply modify theoldedge torecord thenewdaughters:\\n[id-old ,leftverte x,right verte x,mother category,daughter s-oldtdaughter s]\\nThere isnoneed torecurse with thisedge, because wecouldn\\' tgetanynewresults.\\nFortheexample above,everything proceeds asbefore uptoedge 9:\\nidleftrightmother daughters\\n10 1NP {(they)}\\n21 2V {(can)}\\n31 2VP {(2)}\\n40 2S {(13)}\\n52 3V {(fish)}\\n62 3VP {(5)}\\n71 3VP {(26)}\\n80 3S {(17)}\\n92 3NP {(fish)}\\nHowever,rather than addedge 10,which would be:\\n101 3VP (29)\\nwematch thiswith edge 7,andsimply addthenewdaughters tothat.\\n71 3VP {(26),(29)}\\nThe algorithm then terminates. Weonly haveonespanning edge (edge 8)butthedisplay routine ismore comple x\\nbecause wehavetoconsider thealternati vesetsofdaughters foredge 7.(Youshould gothrough thistoconvince\\nyourself thatthesame results areobtained asbefore.) Although inthiscase, theamount ofprocessing savedissmall,\\ntheeffects aremuch more important with longer sentences (consider hebelie vestheycan\\x02sh,forinstance).\\n4.10 Activechart parsing\\nAmore minor ef\\x02cienc yimpro vement isobtained bystoring theresults ofpartial rule applications. This isactive\\nchart parsing, socalled because thepartial edges areconsidered tobeactive:i.e.they`want\\'more input tomakethem\\ncomplete. Anactiveedge records theinput itexpects aswell asthedaughters ithasalready seen. Forinstance, with\\nanactivechart parser ,wemight havethefollowing edges after seeing they:\\n34idleftrightmother expected daughters\\n10 1NP {(they)}\\n20 1S VP {(1?)}\\nThedaughter mark edas?willbeinstantiated bytheedge corresponding totheVPwhen itisfound.\\n4.11 Ordering thesearchspace\\nInthepseudo-code above,theorder ofaddition ofedges tothechart wasdetermined bytherecursion. Ingeneral,\\nchart parsers makeuseofanagenda ofedges, sothatthenextedges tobeoperated onaretheones thatare\\x02rstonthe\\nagenda. Different parsing algorithms canbeimplemented bymaking thisagenda astack oraqueue, forinstance.\\nSofar,we\\'veconsidered bottom upparsing: analternati veistopdown parsing, where theinitial edges aregivenby\\ntherules whose mother corresponds tothestart symbol.\\nSome ef\\x02cienc yimpro vements canbeobtained byordering thesearch space appropriately ,though which version is\\nmost ef\\x02cient depends onproperties oftheindividual grammar .However,themost important reason touseanexplicit\\nagenda iswhen wearereturning parses insome sortofpriority order ,corresponding toweights ondifferent grammar\\nrules orlexical entries.\\nWeights canbemanually assigned torules andlexical entries inamanually constructed grammar .However,in\\nthelastdecade, alotofworkhasbeen done onautomatically acquiring probabilities from acorpus annotated with\\ntrees (atreebank ),either aspartofageneral process ofautomatic grammar acquisition, orasautomatically acquired\\nadditions toamanually constructed grammar .Probabilistic CFGs (PCFGs) canbede\\x02ned quite straightforw ardly ,if\\ntheassumption ismade thattheprobabilities ofrules andlexical entries areindependent ofoneanother (ofcourse\\nthisassumption isnotcorrect, buttheorderings givenseem toworkquite well inpractice). Theimportance ofthisis\\nthatwerarely wanttoreturn allparses inarealapplication, butinstead wewanttoreturn those which aretop-rank ed:\\ni.e.,themost likelyparses. This isespecially truewhen weconsider thatrealistic grammars caneasily return many\\nthousands ofparses forsentences ofquite moderate length (20words orso).Ifedges areprioritised byprobability ,very\\nlowpriority edges canbecompletely excluded from consideration ifthere isacut-of fsuch thatwecanbereasonably\\ncertain thatnoedges with alowerpriority than thecut-of fwillcontrib utetothehighest-rank edparse. Limiting the\\nnumber ofanalyses under consideration isknownasbeam search(theanalogy isthatwe\\'relooking within abeam of\\nlight, corresponding tothehighest probability edges). Beam search islinear rather than exponential orcubic. Justas\\nimportantly ,agood priority ordering from aparser reduces theamount ofworkthathastobedone to\\x02lter theresults\\nbywhate versystem isprocessing theparser\\' soutput.\\n4.12 Whycan\\'t weuseFSAs tomodel thesyntax ofnatural languages?\\nInthislecture, westarted using CFGs. This raises thequestion ofwhyweneed thismore expressi ve(and hence\\ncomputationally expensi ve)formalism, rather than modelling syntax with FSAs. One reason isthatthesyntax of\\nnatural languages cannot bedescribed byanFSA, eveninprinciple, duetothepresence ofcentr e-embedding ,i.e.\\nstructures which map to:\\nA!\\x0bA\\x0c\\nandwhich generate grammars oftheformanbn.Forinstance:\\nthestudents thepolice arrested complained\\nhasacentre-embedded structure. However,humans havedif\\x02culty processing more than twolevelsofembedding:\\n?thestudents thepolice thejournalists criticised arrested complained\\nIftherecursion is\\x02nite (nomatter howdeep), then thestrings ofthelanguage canbegenerated byanFSA. Soit\\'snot\\nentirely clear whether formally anFSA might notsuf\\x02ce.\\nThere\\' safairly extensi vediscussion ofthese issues inJ&M ,butthere aretwoessential points forourpurposes:\\n351.Grammars written using \\x02nite state techniques alone areveryhighly redundant, which makesthem verydif\\x02cult\\ntobuildandmaintain.\\n2.Without internal structure, wecan\\'tbuildupgood semantic representations.\\nHence theuseofmore powerful formalisms: inthenextlecture, we\\'lldiscuss theinadequacies ofsimple CFGs from\\nasimilar perspecti ve.\\nHowever,FSAs areveryuseful forpartial grammars which don\\'trequire fullrecursion. Inparticular ,forinformation\\nextraction, weneed torecognise named entities :e.g. Professor Smith, IBM, 101Dalmatians, theWhite House, the\\nAlps andsoon.Although NPs areingeneral recursi ve(theman who likesthedogwhichbites postmen ),relati ve\\nclauses arenotgenerally partofnamed entities. Also theinternal structure ofthenames isunimportant forIE.Hence\\nFSAs canbeused, with sequences such as`title surname\\', `DT0 PNP\\' etc\\nCFGs canbeautomatically compiled intoapproximately equivalent FSAs byputting bounds ontherecursion. This is\\nparticularly important inspeech recognition engines.\\n4.13 Further reading\\nThis lecture hascovered material which J&M discuss inchapters 9and10,though wealsotouched onPCFGs (covered\\nintheir chapter 12)andissues oflanguage comple xitywhich theydiscuss inchapter 13.J&M\\' sdiscussion coversthe\\nEarle yalgorithm, which canbethought ofasaform ofactivetop-do wnchart parsing. Ichose toconcentrate on\\nbottom-up parsing inthislecture, mainly because I\\x02nditeasier todescribe, butalsobecause itiseasier toseehowto\\nextend thistoPCFGs. Bottom-up parsing also seems tohavebetter practical performance with thesortofgrammars\\nwe\\'lllook atinlecture 5.\\nThere arealargenumber ofintroductory linguistics textbooks which coverelementary syntax anddiscuss concepts\\nsuch asconstituenc y.Forinstance, students could usefully look atthe\\x02rst\\x02vechapters ofTallerman (1998):\\nTallerman, Maggie, Under standing Syntax ,Arnold, London, 1998\\nAnalternati vewould bethe\\x02rsttwochapters ofSagandWasow(1999) \\x97copies should beintheComputer Lab-\\noratory library .This hasanarro werfocus than most other syntax books, butcoversamuch more detailed grammar\\nfragment. Thelater chapters (particularly 3and4)arerelevantforlecture 5.\\nSag, IvanA.andThomas Wasow,Syntactic Theory \\x97aformal introduction ,CSLI Publications, Stanford, CA, USA,\\n1999\\n365Lectur e5:Parsing with constraint-based grammars\\nTheCFG approach which we\\'velookedatsofarhassome serious de\\x02ciencies asamodel ofnatural language. Inthis\\nlecture, I\\'lldiscuss some ofthese andgiveanintroduction toamore expressi veformalism which iswidely used in\\nNLP,againwith thehelp ofasample grammar .Inthe\\x02rstpartofthenextlecture, Iwillalso sketch howwecanuse\\nthisapproach todocompositional semantics.\\n5.1 De\\x02ciencies inatomic category CFGs\\nIfweconsider thegrammar wesawinthelastlecture, severalproblems areapparent. One isthatthere isnoaccount\\nofagreement, so,forinstance, *it\\x02shisallowed bythegrammar aswell asthey\\x02sh.13\\nWecould, ofcourse, allowforagreement byincreasing thenumber ofatomic symbols intheCFG, introducing NP-sg,\\nNP-pl, VP-sg andVP-pl, forinstance. Butthisapproach would soon become verytedious:\\nS->NP-sgVP-sg\\nS->NP-plVP-pl\\nVP-sg->V-sgNP-sg\\nVP-sg->V-sgNP-pl\\nVP-pl->V-plNP-sg\\nVP-pl->V-plNP-pl\\nNote thatwehavetoexpand outthesymbols evenwhen there\\' snoconstraint onagreement, since wehavenowayof\\nsaying thatwedon\\'tcare about thevalue ofnumber foracategory.\\nAnother linguistic phenomenon thatwearefailing todeal with issubcate gorization .This isthelexical property that\\ntells ushowmanyargument saverbcanhave(among other things). Averbsuch asadore,forinstance, istransiti ve:a\\nsentence such as*Kim adoredisstrange, while Kim adoredSandy isusual. Averbsuch asgive isditransitive :Kim\\ngave Sandy anapple (orKim gave anapple toSandy ).Without going intodetails ofexactly howsubcate gorization is\\nde\\x02ned, orwhat anargument is,itshould beintuiti velyobvious thatwe\\'renotencoding thisproperty with ourCFG.\\nThegrammar allowsthefollowing, forinstance:\\nthey\\x02sh\\x02shit\\n(S(NPthey)(VP(V\\x02sh) (VP(V\\x02sh) (NPit))))\\nAgainthiscould bedealt with bymultiplying outsymbols (V-intrans, V-ditrans etc), butthegrammar becomes ex-\\ntremely cumbersome.\\nFinally ,consider thephenomenon oflong-distance dependencies ,exempli\\x02ed, forinstance, by:\\nwhich problem didyousayyoudon\\'tunderstand?\\nwho doyouthink Kim askedSandy tohit?\\nwhich kids didyousaywere making allthatnoise?\\nTraditionally ,these sentences aresaid tocontain `gap\\'s,corresponding totheplace where thenoun phrase would\\nnormally appear: thegapsaremark edbyunderscores below:\\nwhich problem didyousayyoudon\\'tunderstand ?\\nwho doyouthink Kim askedSandy tohit?\\nwhich kids didyousay were making allthatnoise?\\nNotice that, inthethird example, theverbwereshowsplural agreement.\\nDoing thisinstandard CFGs ispossible, butextremely verbose, potentially leading tomillions ofrules. Instead of\\nhaving simple atomic categories intheCFG, wewanttoallowforfeatures onthecategories, which canhavevalues\\nindicating things likeplurality .Asthelong-distance dependenc yexamples should indicate, thefeatures need tobe\\ncomple x-valued. Forinstance,\\n13There wasalso noaccount ofcase:thisisonly re\\x03ected inafewplaces inmodern English, but*theycantheyisclearly ungrammatical (as\\nopposed totheycanthem ,which isgrammatical with thetransiti veverbuseofcan).\\n37*what kiddidyousay were making allthatnoise?\\nisnotgrammatical. The analysis needs tobeable torepresent theinformation thatthegapcorresponds toaplural\\nnoun phrase.\\nInwhat follows,Iwillillustrate asimple constr aint-based grammar formalism, using featur estructur es.Aconstraint-\\nbased grammar describes alanguage using asetofindependently stated constraints, without imposing anyconditions\\nonprocessing orprocessing order .ACFG canbetakenasanexample ofaconstraint-based grammar ,butusually the\\nterm isreserv edforricher formalisms. Thesimplest waytothink offeature structures (FSs) isthatwe\\'rereplacing the\\natomic categories ofaCFG with more comple xdata structures. I\\'ll\\x02rstillustrate thisidea intuiti vely,using agrammar\\nfragment liketheoneinlecture 4butenforcing agreement. I\\'llthen gothrough thefeature structure formalism inmore\\ndetail. This isfollowed byanexample ofamore comple xgrammar ,which allowsforsubcate gorization (Iwon\\'tshow\\nhowcase andlong-distance dependencies aredealt with).\\n5.2 Averysimple FSgrammar encoding agreement\\nInaFSgrammar ,rules aredescribed asrelating FSs: i.e.,lexical entries andphrases areFSs. Inthese formalisms,\\ntheterm sign isoften used torefer tolexical entries andphrases collecti vely.Infact,rules themselv escanbetreated\\nasFSs. Feature structures aresingly-rooted directed acyclic graphs, with arcslabelled byfeatures andterminal nodes\\nassociated with values. Aparticular feature inastructure may beatomic-valued ,meaning itpoints toaterminal node\\ninthegraph, orcomple x-valued ,meaning itpoints toanon-terminal node. Asequence offeatures isknownasapath.\\nForinstance, inthestructure below,there aretwoarcs, labelled with CATand AGR,andthree nodes, with thetwo\\nterminal nodes having values noun andsg.Each ofthefeatures isthus atomic-v alued.\\nCAT-noun\\nAGR\\njsg\\nInthegraph below,thefeature HEADiscomple x-valued, andthevalue ofAGR(i.e., thevalue ofthepath HEADAGR)\\nisunspeci\\x02ed:\\nHEAD-CAT-noun\\nAGR\\nj\\nFSsareusually drawnasattrib ute-value matrices orAVMs. TheAVMs corresponding tothetwoFSsaboveareas\\nfollows:\\n\\x14\\nCATnoun\\nAGRsg\\x15\\n\"\\nHEAD\\x14\\nCATnoun\\nAGR\\x02\\x03\\x15#\\nSince FSsaregraphs, rather than trees, aparticular node may beaccessed from therootbymore than onepath: thisis\\nknownasreentr ancy .InAVMs, reentranc yisconventionally indicated byboxedintegers, with node identity indicated\\nbyintegeridentity .Theactual integers used arearbitrary .This isillustrated with anabstract example using features F\\nand Gbelow:\\n38Graph AVM\\nNon-reentrantaF:\\nG- a\\x14\\nFa\\nGa\\x15\\nReentrantF\\nzG- a\\x14\\nF0a\\nG 0\\x15\\nWhen using FSsingrammars, structures arecombined byuni\\x02cation .This means thatalltheinformation inthetwo\\nstructures iscombined. Theempty square brack ets(\\x02\\x03)inanAVM indicate thatavalue isunspeci\\x02ed: i.e.thisisa\\nnode which canbeuni\\x02ed with aterminal node (i.e., anatomic value) oracomple xvalue. More details ofuni\\x02cation\\naregivenbelow.\\nWhen FSsareused inaparticular grammar ,allsigns will haveasimilar setoffeatures (although sometimes there\\naredifferences between lexical andphrasal signs). Feature structure grammars canbeused toimplement avariety of\\nlinguistic frame works. Forthe\\x02rstexample ofaFSgrammar ,we\\'lljustconsider howagreement could beencoded.\\nSuppose wearetrying tomodel agrammar which isweakly equivalent totheCFG fragment below:\\nS->NP-sgVP-sg\\nS->NP-plVP-pl\\nVP-sg->V-sgNP-sg\\nVP-sg->V-sgNP-pl\\nVP-pl->V-plNP-sg\\nVP-pl->V-plNP-pl\\nV-pl->like\\nV-sg->likes\\nNP-sg->it\\nNP-pl->they\\nNP-sg->fish\\nNP-pl->fish\\nTheFSequivalent shownbelowsplits upthecategories sothatthemain category andtheagreement values aredistinct.\\nInthegrammar below,Ihaveused thearrownotation forrules asanabbre viation: Iwilldescribe theactual FSencoding\\nofrules shortly .TheFSgrammar justneeds tworules. There isasingle rulecorresponding totheS->NPVPrule,\\nwhich enforces identity ofagreement values between theNPandtheVPbymeans ofreentranc y(indicated bythetag\\n1).Therulecorresponding toVP->VNPsimply makestheagreement values oftheVandtheVPthesame but\\nignores theagreement value ontheNP.14Thelexicon speci\\x02es agreement values forit,they,likeandlikes,butleaves\\ntheagreement value for\\x02shuninstantiated (i.e., underspeci\\x02ed). Note thatthegrammar alsohasarootFS:astructure\\nonly counts asavalidparse ifitisuni\\x02able with theroot.\\nFSgrammar fragment encoding agreement\\nGrammar rules\\nRule1\\x14\\nCATS\\nAGR1\\x15\\n!\\x14\\nCATNP\\nAGR1\\x15\\n,\\x14\\nCATVP\\nAGR1\\x15\\nRule2\\x14\\nCATVP\\nAGR1\\x15\\n!\\x14\\nCATV\\nAGR1\\x15\\n,\\x14\\nCATNP\\nAGR\\x02\\x03\\x15\\nLexicon:\\n;;;noun phrases\\nthey\\x14\\nCATnoun\\nAGRpl\\x15\\n14Note thatthereentranc yindicators arelocal toeach rule: the 1inrule1isnotthesame structure asthe 1inrule2.\\n39\\x02sh\"\\x14\\nCATnoun\\nAGR\\x02\\x03\\x15#\\nit\\x14\\nCATnoun\\nAGRsg\\x15\\n;;;verbs\\nlike\\x14\\nCATverb\\nAGRpl\\x15\\nlikes\\x14\\nCATverb\\nAGRsg\\x15\\nRoot structure:\\x02\\nCATS\\x03\\nConsider parsing theylikeitwith thisgrammar .Thelexical structures forlikeanditareuni\\x02ed with thecorresponding\\nstructure totheright hand side ofrule2.Both uni\\x02cations succeed, andthestructure corresponding tothemother of\\ntheruleis:\\n\\x14\\nCATVP\\nAGRpl\\x15\\nThe agreement value isplbecause ofthecoinde xation with theagreement value oflike.This structure canunify\\nwith therightmost daughter ofrule1.Thestructure fortheyisuni\\x02ed with theleftmost daughter .Rule 1says that\\nboth daughters havetohavethesame agreement value, which isthecase inthisexample. Rule application therefore\\nsucceeds andsince theresult uni\\x02es with therulestructure, there isavalidparse.\\nToseewhat isgoing onabitmore precisely ,weneed toshowtherules asFSs. There areseveralwaysofencoding\\nthis, butforcurrent purposes Iwillassume thatrules havefeatures MOTHER,DTR1,DTR2...DTRN.SoRule2, which\\nIinformally wrote as:\\n\\x14\\nCATVP\\nAGR1\\x15\\n!\\x14\\nCATV\\nAGR1\\x15\\n,\\x14\\nCATNP\\nAGR\\x02\\x03\\x15\\nisactually:\\n2\\n66666664MOTHER\\x14\\nCATVP\\nAGR1\\x15\\nDTR1\\x14\\nCATV\\nAGR1\\x15\\nDTR2\\x14\\nCATNP\\nAGR\\x02\\x03\\x153\\n77777775\\nThestructure forlikecanbeuni\\x02ed with thevalue ofDTR1intherule. Uni\\x02cation means allinformation isretained,\\nsotheresult includes theagreement value from like:\\n2\\n66666664MOTHER\\x14\\nCATVP\\nAGR1pl\\x15\\nDTR1\\x14\\nCATV\\nAGR1\\x15\\nDTR2\\x14\\nCATNP\\nAGR\\x02\\x03\\x153\\n77777775\\nThestructure foritisuni\\x02ed with thevalue forDTR2:\\n402\\n66666664MOTHER\\x14\\nCATVP\\nAGR1pl\\x15\\nDTR1\\x14\\nCATV\\nAGR1\\x15\\nDTR2\\x14\\nCATNP\\nAGRsg\\x153\\n77777775\\nTheruleapplication thus succeeds. The MOTHERvalue actsastheDTR2ofRule 1.That is:\\n\\x14\\nCATVP\\nAGRpl\\x15\\nisuni\\x02ed with theDTR2value of:\\n2\\n66666664MOTHER\\x14\\nCATS\\nAGR1\\x15\\nDTR1\\x14\\nCATNP\\nAGR1\\x15\\nDTR2\\x14\\nCATVP\\nAGR1\\x153\\n77777775\\nThis gives:\\n2\\n66666664MOTHER\\x14\\nCATS\\nAGR1pl\\x15\\nDTR1\\x14\\nCATNP\\nAGR1\\x15\\nDTR2\\x14\\nCATVP\\nAGR1\\x153\\n77777775\\nTheFSfortheyis:\\n\\x14\\nCATNP\\nAGRpl\\x15\\nTheuni\\x02cation ofthiswith thevalue ofDTR1succeeds butadds nonewinformation:\\n2\\n66666664MOTHER\\x14\\nCATS\\nAGR1pl\\x15\\nDTR1\\x14\\nCATNP\\nAGR1\\x15\\nDTR2\\x14\\nCATVP\\nAGR1\\x153\\n77777775\\nSimilarly ,thisstructure uni\\x02es with therootstructure, sothisisavalidparse.\\nNote however,thatifwehadtried toparse itlikeit,auni\\x02cation failure would haveoccurred, since the AGRonthe\\nlexical entry forithasthevalue sgwhich clashes with thevalue pl.\\nIhavedescribed these uni\\x02cations asoccurring inaparticular order ,butitisveryimportant tonote thatorder isnot\\nsigni\\x02cant andthatthesame overall result would havebeen obtained ifanother order hadbeen used. This means that\\ndifferent parsing algorithms areguaranteed togivethesame result. Theoneproviso isthatwith some FSgrammars,\\njustlikeCFGs, some algorithms may terminate while others donot.\\n5.3 Featur estructur esindetail\\nSofar,Ihavebeen using arather informal description ofFSs. Thefollowing section givesmore formal de\\x02nitions.\\n41FSscanbethought ofasgraphs which havelabelled arcs connecting nodes (except forthecase ofthesimplest FSs,\\nwhich consist ofasingle node with noarcs) Thelabels onthearcs arethefeatures. Arcs areregarded ashaving a\\ndirection, conventionally regarded aspointing intothestructure, awayfrom thesingle root node. Thesetoffeatures\\nandthesetofatomic values areassumed tobe\\x02nite.\\nProperties ofFSs\\nConnectedness andunique rootAFSmust haveaunique rootnode: apart from therootnode, allnodes haveoneor\\nmore parent nodes.\\nUnique featur esAnynode may havezero ormore arcs leading outofit,butthelabel oneach (that is,thefeature)\\nmust beunique.\\nNocycles Nonode may haveanarcthatpoints back totheroot node ortoanode thatinterv enes between itandthe\\nrootnode. (Some variants ofFSformalisms allowcycles.)\\nValues Anode which does nothaveanyarcsleading outofitmay haveanassociated atomic value.\\nFiniteness AnFSmust havea\\x02nite number ofnodes.\\nSequences offeatures areknownaspaths.\\nFeature structures canberegarded asbeing ordered byinformation content \\x97anFSissaidtosubsume another ifthe\\nlatter carries extrainformation. This isimportant because wede\\x02ne uni\\x02cation interms ofsubsumption.\\nProperties ofsubsumption FS1subsumes FS2ifandonly ifthefollowing conditions hold:\\nPathvalues Foreverypath PinFS1there isapath PinFS2. IfPhasavalue tinFS1, then Palsohasvalue tinFS2.\\nPathequivalences Everypairofpaths PandQwhich arereentrant inFS1 (i.e., which lead tothesame node inthe\\ngraph) arealsoreentrant inFS2.\\nUni\\x02cation corresponds toconjunction ofinformation, andthus canbede\\x02ned interms ofsubsumption, which isa\\nrelation ofinformation containment. Theuni\\x02cation oftwoFSsisde\\x02ned tobethemost general FSwhich contains\\nalltheinformation inboth oftheFSs. Uni\\x02cation willfailifthetwoFSscontain con\\x03icting information. Aswesaw\\nwith thesimple grammar above,thisprevented itlikeitgetting ananalysis, because theAGRvalues con\\x03icted.\\nProperties ofuni\\x02cation Theuni\\x02cation oftwoFSs, FS1andFS2, isthemost general FSwhich issubsumed byboth\\nFS1andFS2, ifitexists.\\n5.4 Agrammar enforcing subcategorization\\nAlthough thegrammar shownaboveimpro vesonthesimple CFG, itstill doesn\\' tencode subcate gorization. The\\ngrammar shownoverleaf does this. Itmovesfurther awayfrom theCFG. Inparticular ,intheprevious grammar thecat\\nfeature encoded both thepart-of-speech (i.e., noun orverb)andthedistinction between thelexical sign andthephrase\\n(i.e., NvsNPandVvsVP). Inthegrammar below,theCATfeature justencodes themajor category (noun vsverb)and\\nthephrasal distinction isencoded interms ofwhether thesubcate gorization requirements havebeen satis\\x02ed. The CAT\\nand AGRfeatures arenowinside another feature head .Signs havethree features atthetop-le vel:HEAD,COMPand\\nSPR.This re\\x03ects aportion ofalinguistic frame workwhich isdescribed ingreat detail inSagandWasow(1999).15\\nBrie\\x03y ,HEADcontains information which isshared between thelexical entries andphrases ofthesame category:\\ne.g., nouns share thisinformation with thenoun phrase which dominates them inthetree, while verbs share head\\ninformation with verbphrases andsentences. SoHEADisused foragreement information andforcategory information\\n(i.e. noun, verbetc). Incontrast, COMPand SPRareabout subcate gorization: theycontain information about what\\ncancombine with thissign. Inthegrammar below,thespeci\\x02er isthesubject ofaverb,butinalargergrammar a\\ndeterminer would bethespeci\\x02er ofanoun. Forinstance, anintransiti veverbwillhaveaSPRcorresponding toits\\nsubject `slot\\' andavalue of\\x02lled foritsCOMP.16\\n15Youaren\\' texpected toknowanydetails ofthelinguistic frame workfortheexam, andyoudonothavetoremember thefeature structure\\narchitecture described here. Thepoint ofgiving thismore complicated grammar isthatitstarts todemonstrate thepowerofthefeature structure\\nframe work,inawaythatthesimple grammar using agreement does not.\\n16There\\' samore elegantwayofdoing thisusing lists, butsince thiscomplicates thegrammar quite alot,Iwon\\'tshowthishere.\\n42Thegrammar belowhasjusttworules, oneforcombining asign with itscomplement, another forcombining asign\\nwith itsspeci\\x02er .Rule 1says that, when building thephrase, the COMPvalue ofthe\\x02rstdaughter istobeequated\\n(uni\\x02ed) with thewhole structure ofthesecond daughter (indicated by2).Thehead ofthemother isequated with\\nthehead ofthe\\x02rstdaughter (1).The SPRofthemother isalsoequated with theSPRofthe\\x02rstdaughter (3).The\\nCOMPvalue ofthemother isstipulated asbeing \\x02lled :thismeans themother can\\'tactasthe\\x02rstdaughter inanother\\napplication oftherule, since \\x02lled won\\'tunify with acomple xfeature structure. Thespeci\\x02er ruleisfairly similar ,in\\nthataSPR`slot\\' isbeing instantiated, although inthiscase it\\'sthesecond daughter thatcontains theslotandissharing\\nitshead information with themother .The rule also stipulates thatthe AGRvalues ofthetwodaughters havetobe\\nuni\\x02ed andthatthespeci\\x02er daughter hastohavea\\x02lled complement. These rules arecontrolled bythelexical entries\\ninthesense thatit\\'sthelexical entries which determine therequired complements andspeci\\x02er ofaword.\\nAsanexample, consider analysing they\\x02sh.Theverbentry for\\x02shcanbeuni\\x02ed with thesecond daughter position\\nofrule2,giving thefollowing partially instantiated rule:2\\n64HEAD 1\\x14\\nCATverb\\nAGR3pl\\x15\\nCOMP\\x02lled\\nSPR\\x02lled3\\n75! 22\\n64HEAD\\x14\\nCATnoun\\nAGR3\\x15\\nCOMP\\x02lled\\nSPR\\x02lled3\\n75,\"\\nHEAD 1\\nCOMP\\x02lled\\nSPR2#\\nThe\\x02rstdaughter ofthisresult canbeuni\\x02ed with thestructure forthey,which inthiscase returns thesame structure,\\nsince itadds nonewinformation. Theresult canbeuni\\x02ed with therootstructure, sothisisavalidparse.\\nOntheother hand, thelexical entry forthenoun \\x02shdoes notunify with thesecond daughter position ofrule2. The\\nentry fortheydoes notunify with the\\x02rstdaughter position ofrule1. Hence there isnoother parse.\\nSimple FSgrammar fragment encoding subcategorization\\nRule1 ;;;comp \\x02lling\"\\nHEAD 1\\nCOMP\\x02lled\\nSPR3#\\n!\"\\nHEAD 1\\nCOMP2\\nSPR3#\\n,2\\x02\\nCOMP\\x02lled\\x03\\nRule2 ;;;spr\\x02lling:\"\\nHEAD 1\\nCOMP\\x02lled\\nSPR\\x02lled#\\n! 22\\n4HEAD\\x02\\nAGR3\\x03\\nCOMP\\x02lled\\nSPR\\x02lled3\\n5,2\\n4HEAD 1\\x02\\nAGR3\\x03\\nCOMP\\x02lled\\nSPR23\\n5\\nLexicon:\\n;;;noun phrases\\nthey2\\n64HEAD\\x14\\nCATnoun\\nAGRpl\\x15\\nCOMP\\x02lled\\nSPR\\x02lled3\\n75\\n\\x02sh2\\n64HEAD\\x14\\nCATnoun\\nAGR\\x02\\x03\\x15\\nCOMP\\x02lled\\nSPR\\x02lled3\\n75\\nit2\\n64HEAD\\x14\\nCATnoun\\nAGRsg\\x15\\nCOMP\\x02lled\\nSPR\\x02lled3\\n75\\n;;;verbs\\n\\x02sh2\\n6664HEAD\\x14\\nCATverb\\nAGRpl\\x15\\nCOMP\\x02lled\\nSPRh\\nHEAD\\x02\\nCATnoun\\x03i3\\n7775\\n43can2\\n666664HEAD\\x14\\nCATverb\\nAGR\\x02\\x03\\x15\\nCOMPh\\nHEAD\\x02\\nCATverb\\x03i\\nSPRh\\nHEAD\\x02\\nCATnoun\\x03i3\\n777775;;;auxiliary verb\\ncan2\\n6666664HEAD\\x14\\nCATverb\\nAGRpl\\x15\\nCOMP\\x14\\nHEAD\\x02\\nCATnoun\\x03\\nCOMP\\x02lled\\x15\\nSPRh\\nHEAD\\x02\\nCATnoun\\x03i3\\n7777775;;;transiti veverb\\nRoot structure:2\\n4HEAD\\x02\\nCATverb\\x03\\nCOMP\\x02lled\\nSPR\\x02lled3\\n5\\n5.5 Parsing with featur estructur egrammars\\nFormally wecantreat feature structure grammars interms ofsubsumption. Iwon\\'tgivedetails here, buttheintuition\\nisthattherule FSs, thelexical entry FSsandtheroot FSallactasconstraints ontheparse, which havetobesat-\\nis\\x02ed simultaneously .This means thesystem hastobuildaparse structure which issubsumed byalltheapplicable\\nconstraints. However,thisdescription ofwhat itmeans forsomething tobeavalidparse doesn\\' tgiveanyhintofa\\nsensible algorithm.\\nThestandard approach toimplementation istousechart parsing, asdescribed intheprevious lecture, butthenotion\\nofagrammar rulematching anedge inthechart ismore comple x.Inanaiveimplementation, when application ofa\\ngrammar ruleischeck ed,allthefeature structures intheedges inthechart thatcorrespond tothepossible daughters\\nhavetobecopied, andthegrammar rule feature structure itself isalso copied. The copied daughter structures are\\nuni\\x02ed with thedaughter positions inthecopyoftherule, andifuni\\x02cation succeeds, thecopied structure isassociated\\nwith anewedge onthechart.\\nThe need forcopying isoften discussed interms ofthedestructi venature ofthestandard algorithm foruni\\x02cation\\n(which Iwon\\'tdescribe here), butthisisperhaps alittle misleading. Uni\\x02cation, howeverimplemented, involves\\nsharing information between structures. Assume, forinstance, thattheFSrepresenting thelexical entry ofthenoun\\nfor\\x02shisunderspeci\\x02ed fornumber agreement. When weparse asentence like:\\nthe\\x02shswims\\nthepartoftheFSintheresult thatcorresponds totheoriginal lexical entry willhaveitsAGRvalue instantiated. This\\nmeans thatthestructure corresponding toaparticular edge cannot bereused inanother analysis, because itwillcontain\\n`extra\\' information. Consider ,forinstance, parsing:\\nthe\\x02shinthelakewhich isnear thetownswim\\nApossible analysis of:\\n\\x02shinthelakewhich isnear thetown\\nis:\\n(\\x02sh (inthelake)(which isnear thetown))\\ni.e.,the\\x02sh(sg)isnear thetown.Ifweinstantiate theAGRvalue intheFSfor\\x02shassgwhile constructing thisparse,\\nandthen trytoreuse thatsame FSfor\\x02shintheother parses, analysis willfail.Hence theneed forcopying, sowecan\\nuseafresh structure each time. Copying ispotentially extremely expensi ve,because realistic grammars involveFSs\\nwith manyhundreds ofnodes.\\n44So,although uni\\x02cation isverynear tolinear incomple xity,naiveimplementations ofFSformalisms areveryin-\\nef\\x02cient. Furthermore, packing isnotstraightforw ard, because twostructures arerarely identical inrealgrammars\\n(especially ones thatencode semantics).\\nReasonably ef\\x02cient implementations ofFSformalisms cannevertheless bedeveloped. Copying canbegreatly re-\\nduced:\\n1.bydoing anef\\x02cient pretest before uni\\x02cation, sothatcopies areonly made when uni\\x02cation islikelytosucceed\\n2.bysharing parts ofFSsthataren\\' tchanged\\n3.bytaking advantage oflocality principles inlinguistic formalisms which limit theneed topercolate information\\nthrough structures\\nPacking canalsobeimplemented: thetesttoseeifanewedge canbepackedinvolvessubsumption rather than equality .\\nAswith CFGs, forrealef\\x02cienc yweneed tocontrol thesearch space soweonly getthemost likelyanalyses. De\\x02ning\\nprobabilistic FSgrammars inawaywhich istheoretically well-moti vated ismuch more dif\\x02cult than de\\x02ning aPCFG.\\nPractically itseems toturnoutthattreating aFSgrammar much asthough itwere aCFG works fairly well, butthisis\\nanactiveresearch issue.\\n5.6 Templates\\nThelexicon outlined abovehasthepotential tobeveryredundant. Forinstance, aswell astheintransiti veverb\\x02sh,\\nafulllexicon would haveentries forsleep ,snoreandsoon,which would beessentially identical. Weavoidthis\\nredundanc ybyassociating names with particular feature structures andusing those names inlexical entries. For\\ninstance:\\n\\x02shINTRANS VERB\\nsleep INTRANS VERB\\nsnore INTRANS VERB\\nwhere thetemplate isspeci\\x02ed as:\\nINTRANS VERB2\\n6664HEAD\\x14\\nCATverb\\nAGRpl\\x15\\nCOMP\\x02lled\\nSPRh\\nHEAD\\x02\\nCATnoun\\x03i3\\n7775\\nThelexical entry may havesome speci\\x02c information associated with it(e.g., semantic information, seenextlecture)\\nwhich willbeexpressed asaFS:inthiscase, thetemplate andthelexical feature structure arecombined byuni\\x02cation.\\n5.7 Interface tomorphology\\nSofarwehaveassumed afull-form lexicon, butwecannowreturn totheapproach tomorphology thatwesawin\\nlecture 2,andshowhowthisrelates tofeature structures. Recall thatwehavespelling rules which canbeused to\\nanalyse awordform toreturn astem andlistofaf\\x02xesandthateach af\\x02xisassociated with anencoding ofthe\\ninformation itcontrib utes. Forinstance, theaf\\x02xsisassociated with thetemplatePLURAL_NOUN ,which would\\ncorrespond tothefollowing information inourgrammar fragment:\\n\"\\nHEAD\\x14\\nCATnoun\\nAGRpl\\x15#\\nAstem foranoun isgenerally assumed tobeuninstantiated fornumber (i.e., neutral between sgandpl).Sothelexical\\nentry forthenoun doginourfragment would be:\\n2\\n64HEAD\\x14\\nCATnoun\\nAGR\\x02\\x03\\x15\\nCOMP\\x02lled\\nSPR\\x02lled3\\n75\\n45One simple wayofimplementing in\\x03ectional morphology inFSsissimply tounify thecontrib ution oftheaf\\x02xwith\\nthatofthestem. Ifweunify theFScorresponding tothestem fordogtotheFSforPLURAL_NOUN ,weget:\\n2\\n64HEAD\\x14\\nCATnoun\\nAGRpl\\x15\\nCOMP\\x02lled\\nSPR\\x02lled3\\n75\\nThis approach assumes thatwealsohaveatemplateSINGULAR_NOUN ,where thisisassociated with a`null\\' af\\x02x.\\nInthecase ofanexample such asfeed incorrectly analysed asfee-ed,discussed inx2.5,theaf\\x02xinformation willfail\\ntounify with thestem, ruling outthatanalysis.\\nThere areother waysofencoding in\\x03ectional morphology with FS,which Iwon\\'tdiscuss here. Note thatthissimple\\napproach isnot,ingeneral, adequate forderivational morphology .Forinstance, theaf\\x02x-ize,which combines with\\nanoun toform averb(e.g., lemmatization ),cannot berepresented simply byuni\\x02cation, because ithastochange a\\nnominal form intoaverbal one. This canbeimplemented bysome form oflexical rule(which areessentially grammar\\nrules with single daughters), butIwon\\'tdiscuss thisinthiscourse. Note, however,thatthisre\\x03ects thedistinction\\nbetween in\\x03ectional andderivational morphology thatwesawinx2.2:while in\\x03ectional morphology canbeseen as\\nsimple addition ofinformation, derivational morphology convertsfeature structures into newstructures. However,\\nderivational morphology isoften nottreated asproducti ve,especially inlimited domain systems.\\n5.8 Further reading\\nJ&M describe feature structures asaugmenting aCFG rather than replacing it,butmost oftheir discussion applies\\nequally totheFSformalism I\\'veoutlined here.\\nLinGO (Linguistic Grammars Online:http://lingo.stanford.edu) distrib utes Open Source FSgrammars foravariety\\noflanguages. The LinGO English Resource Grammar (ERG) isprobably thelargest freely available bidirectional\\ngrammar .\\n466Lectur e6:Compositional andlexical semantics\\nThis lecture willgivearather super\\x02cial account ofsemantics andsome ofitscomputational aspects:\\n1.Compositional semantics infeature structure grammars\\n2.Meaning postulates\\n3.Classical lexical relations: hypon ymy,meron ymy,synon ymy,anton ymy\\n4.Taxonomies andWordNet\\n5.Classes ofpolysemy: homon ymy,regular polysemy ,vagueness\\n6.Wordsense disambiguation\\n6.1 Simple semantics infeatur estructur es\\nThegrammar fragment belowisbased ontheoneintheprevious lecture. Itisintended asarough indication ofhow\\nitispossible tobuildupsemantic representations using feature structures. Thelexical entries havebeen augmented\\nwith pieces offeature structure re\\x03ecting predicate-ar gument structure. Withthisgrammar ,theFSfortheycan\\x02sh\\nwillhaveaSEMvalue of: 2\\n666666666664PREDand\\nARG1\\x14\\nPREDpron\\nARG11\\x15\\nARG22\\n666664PREDand\\nARG1\"\\nPREDcanv\\nARG11\\nARG22#\\nARG2\\x14\\nPRED\\x02shn\\nARG12\\x153\\n7777753\\n777777777775\\nThis canbetakentobeequivalent tothelogical expression pron(x)^(canv(x;y)^\\x02shn(y))bytranslating the\\nreentranc ybetween argument positions intovariable equivalence.\\nThe most important thing tonotice ishowthesyntactic argument positions inthelexical entries arelinkedtotheir\\nsemantic argument positions. This means, forinstance, thatforthetransiti veverbcan,thesyntactic subject willalways\\ncorrespond tothe\\x02rstargument position, while thesyntactic object willcorrespond tothesecond position.\\nSimple FSgrammar with crude semantic composition\\nRule1 ;;;comp \\x02lling2\\n6664HEAD 1\\nCOMP\\x02lled\\nSPR3\\nSEM\"\\nPREDand\\nARG14\\nARG25#3\\n7775!2\\n4HEAD 1\\nCOMP2\\nSPR3\\nSEM 43\\n5,2\\x14\\nCOMP\\x02lled\\nSEM 5\\x15\\nRule2 ;;;spr\\x02lling:2\\n6664HEAD 1\\nCOMP\\x02lled\\nSPR\\x02lled\\nSEM\"\\nPREDand\\nARG14\\nARG25#3\\n7775! 22\\n64HEAD\\x02\\nAGR3\\x03\\nCOMP\\x02lled\\nSPR\\x02lled\\nSEM 43\\n75,2\\n64HEAD 1\\x02\\nAGR3\\x03\\nCOMP\\x02lled\\nSPR2\\nSEM 53\\n75\\nLexicon:\\n47can2\\n66666666666666664HEAD\\x14\\nCATverb\\nAGRpl\\x15\\nCOMP2\\n4HEAD\\x02\\nCATnoun\\x03\\nCOMP\\x02lled\\nSEM\\x02\\nINDEX 2\\x033\\n5\\nSPR\"\\nHEAD\\x02\\nCATnoun\\x03\\nSEM\\x02\\nINDEX 1\\x03#\\nSEM\"\\nPREDcanv\\nARG11\\nARG22#3\\n77777777777777775;;;transiti veverb\\n\\x02sh2\\n6666664HEAD\\x14\\nCATnoun\\nAGR\\x15\\nCOMP\\x02lled\\nSPR\\x02lled\\nSEM\"\\nINDEX 1\\nPRED\\x02shn\\nARG11#3\\n7777775;;;noun phrase\\nthey2\\n6666664HEAD\\x14\\nCATnoun\\nAGRpl\\x15\\nCOMP\\x02lled\\nSPR\\x02lled\\nSEM\"\\nINDEX 1\\nPREDpron\\nARG11#3\\n7777775;;;noun phrase\\nAnalternati veapproach toencoding semantics istowrite thesemantic composition rules inaseparate formalism\\nsuch astyped lambda calculus .This corresponds more closely totheapproach most commonly assumed informal\\nlinguistics: variants oflambda calculus aresometimes used inNLP,butIwon\\'tdiscuss thisfurther here.\\nIngeneral, asemantic representation constructed forasentence iscalled thelogical form ofthesentence. The se-\\nmantics shownabovecanbetakentobeequivalent toaform ofpredicate calculus without variables orquanti\\x02ers:\\ni.e.the`variables\\' intherepresentation actually correspond toconstants. Itturns outthatthisveryimpo verished\\nform ofsemantic representation isadequate formanyNLP applications: template representations, used ininformation\\nextraction orsimple dialogue systems canbethought ofasequivalent tothis. Butforafully adequate representation\\nweneed something richer \\x97forinstance, todonegation properly .Minimally weneed full\\x02rst-order predicate cal-\\nculus (FOPC). FOPC logical forms canbepassed totheorem-pro versinorder todoinference about themeaning ofa\\nsentence. However,although thisapproach hasbeen extensi velyexplored inresearch work,especially inthe1980s, it\\nhasn\\' tsofarledtopractical systems. There aremanyreasons forthis, butperhaps themost important isthedif\\x02culty\\nofacquiring detailed domain knowledge expressed inFOPC. There isalsoatheoretical AIproblem, because weseem\\ntoneed some form ofprobabilistic reasoning formanyapplications. So,although most researchers who areworking in\\ncomputational compositional semantics takesupport forinference asadesideratum, manysystems actually usesome\\nform ofshallo winference (e.g., semantic transfer inMT,mentioned inlecture 8).\\nFOPC alsohasthedisadv antage thatitforces quanti\\x02ers tobeinaparticular scopal relationship, andthisinformation\\nisnot(generally) overtinNLsentences. One classic example is:\\nEveryman lovesawoman\\nwhich isambiguous between:\\n8x[man0(x))9y[woman0(y)^love0(x;y)]]\\nandtheless-lik ely,`one speci\\x02c woman\\' reading:\\n9y[woman0(y)^8x[man0(x))love0(x;y)]]\\nMost current systems construct anunderspeci\\x02ed representation which isneutral between these readings, ifthey\\nrepresent quanti\\x02er scope atall.There areseveraldifferent alternati veformalisms forunderspeci\\x02cation.\\n486.2 Generation\\nWecangenerate from asemantic representation with asuitable FSgrammar .Producing anoutput string givenaninput\\nlogical form isgenerally referred toastactical generation orrealization ,asopposed tostrategicgeneration ortext\\nplanning ,which concerns howyoumight buildthelogical form inthe\\x02rstplace. Strate gicgeneration isanopen-ended\\nproblem: itdepends verymuch ontheapplication andIwon\\'thavemuch tosayabout ithere. Tactical generation is\\nmore tractable, andisuseful without astrate giccomponent insome conte xts,such asthesemantic transfer approach\\ntoMT,which I\\'llbrie\\x03y discuss inlecture 8.\\nTactical generation canusesimilar techniques toparsing: forinstance oneapproach ischart generation which uses\\nmanyofthesame techniques aschart parsing. There hasbeen much lessworkongeneration than onparsing ingeneral,\\nandbuilding bidirectional grammars ishard: most grammars forparsing allowthrough manyungrammatical strings.\\nRecently there hasbeen some workonstatistical generation, where n-grams areused tochoose between realisations\\nconstructed byagrammar thatovergenerates. Butevenrelati vely`tight\\' bidirectional grammars may need touse\\nstatistical techniques inorder togenerate natural sounding utterances.\\n6.3 Meaning postulates\\nInference rules canbeused torelate open class predicates: i.e.,predicates thatcorrespond toopen class words. This is\\ntheclassic wayofrepresenting lexical meaning informal semantics within linguistics:17\\n8x[bachelor(x)$man(x)^unmarried(x)]\\nLinguistically andphilosophically ,thisgets pretty dubious. Isthecurrent Pope abachelor? Technically presumably\\nyes, butbachelor seems toimply someone who could bemarried: it\\'sastrange wordtoapply tothePope under\\ncurrent assumptions about celibac y.Meaning postulates arealso toounconstrained: Icould construct apredicate\\n`bachelor -weds-thurs\\' tocorrespond tosomeone who wasunmarried onWednesday andmarried onThursday ,butthis\\nisn\\'tgoing tocorrespond toawordinanynatural language. Inanycase, veryfewwords areassimple tode\\x02ne as\\nbachelor :consider howyoumight start tode\\x02ne table ,tomato orthought ,forinstance.18\\nForcomputational semantics, perhaps thebestwayofregarding meaning postulates issimply asonereasonable wayof\\nlinking compositionally constructed semantic representations toaspeci\\x02c domain. InNLP,we\\'renormally concerned\\nwith implication rather than de\\x02nition andthisislessproblematic philosophically:\\n8x[bachelor(x)!man(x)^unmarried(x)]\\nHowever,thebigcomputational problems with meaning postulates aretheir acquisition andthecontrol ofinference\\nonce theyhavebeen obtained. Building meaning postulates foranything other than asmall, bounded domain isan\\nAI-complete problem.\\nThemore general, shallo wer,relationships thatareclassically discussed inlexical semantics arecurrently more useful\\ninNLP,especially forbroad-co verage processing.\\n6.4 Hyponymy: IS-A\\nHypon ymy istheclassical IS-A relation: e.g. dogisahyponym ofanimal .That is,therelevantsense ofdogisthe\\nhypon ymofanimal :asnearly everything said inthislecture isabout wordsenses rather than words, Iwill avoid\\nexplicitly qualifying allstatements inthisway,butthisshould beglobally understood.\\nanimal isthehypernym ofdog.Hypon yms canbearranged intotaxonomies :classically these aretree-structured: i.e.,\\neach term hasonly onehypernym .\\nDespite thefactthathypon ymy isbyfarthemost important meaning relationship assumed inNLP,manyquestions\\narise which don\\'tcurrently haveverygood answers:\\n17Generally ,linguists don\\'tactually write meaning postulates foropen-class words, butthisisthestandard assumption about howmeaning would\\nberepresented ifanyone could bebothered todoit!\\n18There hasbeen acourt case thathinged ontheprecise meaning oftable andalso onethatdepended onwhether tomatoes were fruits or\\nvegetables.\\n491.What classes ofwords canbecategorised byhypon ymy? Some nouns, classically biological taxonomies, but\\nalsohuman artifacts, professions etcworkreasonably well. Abstract nouns, such astruth ,don\\'treally workvery\\nwell (theyareeither notinhypon ymic relationships atall,orveryshallo wones). Some verbs canbetreated as\\nbeing hypon yms ofoneanother \\x97e.g. murderisahyponym ofkill,butthisisnotnearly asclear asitisfor\\nconcrete nouns. Event-denoting nouns aresimilar toverbs inthisrespect. Hypon ymy isessentially useless for\\nadjecti ves.\\n2.Dodifferences inquantisation andindividuation matter? Forinstance, ischair ahypon ymoffurnitur e?isbeer\\nahypon ymofdrink ?iscoin ahypon ymofmone y?\\n3.Ismultiple inheritance allowed? Intuiti vely,multiple parents might bepossible: e.g. coin might bemetal (or\\nobject ?)andalso mone y.Artif acts ingeneral canoften bedescribed either interms oftheir form ortheir\\nfunction.\\n4.What should thetopofthehierarch ylook like?Thebest answer seems tobetosaythatthere isnosingle top\\nbutthatthere areaseries ofhierarchies.\\n6.5 Other lexical semantic relations\\nMeronymy i.e.,PART-OF\\nThe standard examples ofmeron ymy apply tophysical relationships: e.g., arm ispart ofabody (arm isa\\nmeronym ofbody );steering wheel isameron ymofcar.Note thedistinction between `part\\' and`piece\\': ifI\\nattack acarwith achainsa w,Igetpieces rather than parts!\\nSynonymy i.e.,twowords with thesame meaning (ornearly thesame meaning)\\nTruesynon yms arerelati velyuncommon: most cases oftruesynon ymy arecorrelated with dialect differences\\n(e.g., eggplant /auber gine,boot /trunk ).Often synon ymy involvesregister distinctions, slang orjargons: e.g.,\\npoliceman ,cop,rozzer ...Near -synon yms conveynuances ofmeaning: thin,slim,slender ,skinny .\\nAntonymy i.e.,opposite meaning\\nAnton ymy ismostly discussed with respect toadjecti ves:e.g., big/little,though it\\'sonly relevantforsome\\nclasses ofadjecti ves.\\n6.6 WordNet\\nWordNet isthemain resource forlexical semantics forEnglish thatisused inNLP \\x97primarily because ofitsvery\\nlargecoverage andthefactthatit\\'sfreely available. WordNets areunder development formanyother languages,\\nthough sofarnone areasextensi veastheoriginal.\\nTheprimary organisation ofWordNet isintosynsets :synon ymsets(near -synon yms). Toillustrate this, thefollowing\\nispartofwhat WordNet returns asan`overvie w\\'ofred:\\nwnred-over\\nOverview ofadjred\\nTheadjredhas6senses(first5fromtaggedtexts)\\n1.(43)red,reddish, ruddy,blood-red, carmine,\\ncerise, cherry, cherry-red, crimson, ruby,ruby-red,\\nscarlet --(having anyofnumerous brightorstrong\\ncolorsreminiscent ofthecolorofbloodorcherries\\nortomatoes orrubies)\\n2.(8)red,reddish --((usedofhairorfur)ofa\\nreddish browncolor;\"reddeer\";reddish hair\")\\n50Nouns inWordNet areorganised byhypon ymy,asillustrated bythefragment below:\\nSense6\\nbigcat,cat\\n=>leopard, Panthera pardus\\n=>leopardess\\n=>panther\\n=>snowleopard, ounce,Panthera uncia\\n=>jaguar, panther, Panthera onca,Felisonca\\n=>lion,kingofbeasts, Panthera leo\\n=>lioness\\n=>lionet\\n=>tiger,Panthera tigris\\n=>Bengaltiger\\n=>tigress\\n=>liger\\n=>tiglon, tigon\\n=>cheetah, chetah, Acinonyx jubatus\\n=>saber-toothed tiger,sabertooth\\n=>Smiledon californicus\\n=>falsesaber-toothed tiger\\nThefollowing isanovervie woftheinformation available inWordNet forthevarious POS classes:\\n\\x0fallclasses\\n1.synon yms (ordered byfrequenc y)\\n2.familiarity /polysemy count\\n3.compound words (done byspelling)\\n\\x0fnouns\\n1.hypon yms /hypern yms (also sisters)\\n2.holon yms /meron yms\\n\\x0fadjecti ves\\n1.anton yms\\n\\x0fverbs\\n1.anton yms\\n2.hypon yms /hypern yms (also sisters)\\n3.syntax (verysimple)\\n\\x0fadverbs\\nTaxonomies havealso been automatically orsemi-automatically extracted from machine-readable dictionaries, but\\nthese arenotdistrib uted. Microsoft\\' sMindNet isthebest knownexample (ithasmanymore relationships than just\\nhypon ymy). There areother collections ofterms, generally hierarchically ordered, especially medical ontologies.\\nThere havebeen anumber ofattempts tobuildanontology forworldknowledge: none ofthemore elaborate ones are\\ngenerally available. There isanongoing attempt atstandardisation ofontologies. Ontology support isanimportant\\ncomponent ofthesemantic web.\\n516.7 Using lexical semantics\\nByfarthemost commonly used lexical relation ishypon ymy.Hypon ymy relations canbeused inmanyways:\\n\\x0fSemantic classi\\x02cation: e.g., forselectional restrictions (e.g., theobject ofeathastobesomething edible) and\\nfornamed entity recognition\\n\\x0fShallo winference: `Xmurdered Y\\'implies `Xkilled Y\\'etc\\n\\x0fBack-of ftosemantic classes insome statistical approaches\\n\\x0fWord-sense disambiguation\\n\\x0fMT:ifyoucan\\'ttranslate aterm, substitute ahypern ym\\n\\x0fQuery expansion forinformation retrie val:ifasearch doesn\\' treturn enough results, oneoption istoreplace an\\nover-speci\\x02c term with ahypern ym\\nSynon ymy ornear-synon ymy isrelevantforsome ofthese reasons andalsoforgeneration. (Howeverdialect andreg-\\nister haven\\'tbeen investig ated much inNLP,sothepossible relevance ofdifferent classes ofsynon ymforcustomising\\ntexthasn\\' treally been lookedat.)\\n6.8 Polysemy\\nPolysemy refers tothestate ofawordhaving more than onesense: thestandard example isbank (riverbank) vsbank\\n(\\x02nancial institution).\\nThis ishomonymy \\x97thetwosenses areunrelated (not entirely trueforbank ,actually ,buthistorical relatedness isn\\'t\\nactually important \\x97it\\'swhether ordinary speak ersofthelanguage feelthere\\' sarelationship). Homon ymy isthe\\nmost obvious case ofpolysemy ,butisactually relati velyinfrequent compared touses which havedifferent butrelated\\nmeanings, such asbank (\\x02nancial institution) vsbank (inacasino).\\nIfpolysemy were alwayshomon ymy,wordsenses would bediscrete: twosenses would benomore likelytoshare\\ncharacteristics than would morphologically unrelated words. Butmost senses areactually related. Regular orsys-\\ntematic polysemy concerns related butdistinct usages ofwords, often with associated syntactic effects. Forinstance,\\nstrawberry ,cherry (fruit /plant), rabbit, turkey,halib ut(meat /animal), tango, waltz (dance (noun) /dance (verb)).\\nThere arealotofcomplicated issues indeciding whether awordispolysemous orsimply general/v ague. Forinstance,\\nteacherisintuiti velygeneral between male andfemale teachers rather than ambiguous, butgiving good criteria asa\\nbasis ofthisdistinction isdif\\x02cult. Dictionaries arenotmuch help, since their decisions astowhether tosplit asense\\nortoprovide ageneral de\\x02nition areveryoften contingent onexternal factors such asthesizeofthedictionary orthe\\nintended audience, andevenwhen these factors arerelati velyconstant, lexicographers often makedifferent decisions\\nabout whether andhowtosplit upsenses.\\n6.9 Wordsense disambiguation\\nWordsense disambiguation (WSD) isneeded formost NLapplications thatinvolvesemantics (explicitly orimplicitly).\\nInlimited domains, WSD isnottoobigaproblem, butforlargecoverage textprocessing it\\'saserious bottleneck.\\nWSD needs depend ontheapplication \\x97there isnoobjecti venotion ofwordsense (dictionaries differextensi vely)and\\nit\\'sveryhard tocome upwith good criteria tojudge whether ornottodistinguish senses. Butinorder toexperiment\\nwith WSD asastandalone module, there hastobeastandard: most commonly WordNet, because itistheonly\\nextensi vemodern resource forEnglish with noproblematic IPRissues. This iscontro versial, because WordNet hasa\\nvery\\x02negranularity ofsenses \\x97it\\'salsoobvious thatitssenses often overlap. However,theonly current alternati ve\\nisapre-1920 version ofWebster\\' s.Recently WSD `competitions\\' havebeen organised: SENSEV ALandSENSEV AL\\n2.\\nWSD uptotheearly 1990s wasmostly done byhand-constructed rules (still used insome MTsystems). Dahlgren\\ninvestig ated WSD inafairly broad domain inthe1980s. Reasonably broad-co verage WSD generally depends on:\\n52\\x0ffrequenc y\\n\\x0fcollocations\\n\\x0fselectional restrictions/preferences\\nWhat\\' schanged since the1980s isthatvarious statistical ormachine-learning techniques havebeen used toavoid\\nhand-crafting rules.\\n\\x0fsupervised learning. Requires asense-tagged corpus, which isextremely time-consuming toconstruct systemat-\\nically (examples aretheSemcor andSENSEV ALcorpora, butboth arereally toosmall). Most experimentation\\nhasbeen done with asmall setofwords which canbesense-tagged bytheexperimenter (e.g., plant ).Supervised\\nlearning techniques donotcarry overwell from onecorpus toanother .\\n\\x0funsupervised learning (seebelow)\\n\\x0fMachine readable dictionaries (MRDs). Disambiguating dictionary de\\x02nitions according totheinternal data in\\ndictionaries isnecessary tobuildtaxonomies from MRDs. MRDs havealsobeen used asasource ofselectional\\npreference andcollocation information forgeneral WSD (quite successfully).\\nUntil recently ,most ofthestatistical ormachine-learning techniques havebeen evaluated onhomon yms: these are\\nrelati velyeasy todisambiguate. So95% disambiguation ine.g., Yarowsky\\'sexperiments sounds good (see below),\\nbutdoesn\\' ttranslate intohigh precision onallwords when targetisWordNet senses (inSENSEV AL2thebestsystem\\nwasaround 70%).\\nThere havealsobeen some attempts atautomatic sense induction ,where anattempt ismade todetermine theclusters\\nofusages intextsthatcorrespond tosenses. Inprinciple, thisisaverygood idea, since thewhole notion ofaword\\nsense isfuzzy: wordsenses canbeargued tobeartifactsofdictionary publishing. However,sofarsense induction\\nhasnotbeen much explored inmonolingual conte xts,though itcould beconsidered asaninherent partofstatistical\\napproaches toMT.\\n6.10 Collocations\\nInformally ,acollocation isagroup oftwoormore words thatoccur together more often than would beexpected by\\nchance (there areother de\\x02nitions \\x97thisisnotreally aprecise notion). Collocations havealwaysbeen themost useful\\nsource ofinformation forWSD, eveninDahlgren\\' searly experiments. Forinstance:\\n(23) Striped bass arecommon.\\n(24) Bass guitars arecommon.\\nstriped isagood indication thatwe\\'retalking about the\\x02sh(because it\\'saparticular sortofbass), similarly with guitar\\nandmusic. Inboth bass guitar andstriped bass,we\\'vearguably gotamultiw ordexpression (i.e., aconventional phrase\\nthatmight belisted inadictionary), buttheprinciple holds foranysortofcollocation. Thebest collocates forWSD\\ntend tobesyntactically related inthesentence tothewordtobedisambiguated, butmanytechniques simply usea\\nwindo wofwords.\\nJ&M makeauseful (though non-standard) distinction between collocation andco-occurrence: co-occurrence refers to\\ntheappearance ofanother wordinalargerwindo woftextthan acollocation. Forinstance, troutmight co-occur with\\nthe\\x02shsense ofbass.\\n6.11 Yarowsky\\'sunsuper vised learning appr oach toWSD\\nYarowsky(1995) describes atechnique forunsupervised learning using collocates (collocates andco-occurrences in\\nJ&M\\' sterms). Afewseed collocates arechosen foreach sense (manually orviaanMRD), then these areused\\ntoaccurately identify distinct senses. The sentences inwhich thedisambiguated senses occur canthen beused to\\nlearn other discriminating collocates automatically ,producing adecision list. Theprocess canthen beiterated. The\\n53algorithm allowsbadcollocates tobeoverridden. This works because ofthegeneral principle of`one sense per\\ncollocation\\' (experimentally demonstrated byYarowsky\\x97it\\'snotabsolute, butthere areverystrong preferences).\\nInabitmore detail, using Yarowsky\\'sexample ofdisambiguating plant (which ishomon ymous between factory vs\\nvegetation senses):\\n1.Identify allexamples ofthewordtobedisambiguated inthetraining corpus andstore their conte xts.\\nsense training example\\n? compan ysaidthattheplant isstilloperating\\n? although thousands ofplant andanimal species\\n? zonal distrib ution ofplant life\\n? compan ymanuf acturing plant isinOrlando\\netc\\n2.Identify some seeds which reliably disambiguate afewofthese uses. Tagthedisambiguated senses andcount\\ntherestasresidual. Forinstance, choosing `plant life\\' asaseed forthevegetation sense ofplant (sense A)and\\n`manuf acturing plant\\' astheseed forthefactory sense (sense B):\\nsense training example\\n? compan ysaidthattheplant isstilloperating\\n? although thousands ofplant andanimal species\\nA zonal distrib ution ofplant life\\nB compan ymanuf acturing plant isinOrlando\\netc\\nThis disambiguated 2%ofuses inYarowsky\\'scorpus, leaving 98% residual.\\n3.Trainadecision listclassi\\x02er ontheSense A/Sense Bexamples. Adecision listapproach givesalistofcriteria\\nwhich aretried inorder until anapplicable testisfound: thisisthen applied. Thetests areeach associated with\\nareliability metric. Theoriginal seeds arelikelytobeatthetopoftheinitial decision list,followed byother\\ndiscriminating terms. e.g.thedecision listmight include:\\nreliability criterion sense\\n8.10 plant life A\\n7.58 manuf acturing plant B\\n6.27 animal within 10words ofplant A\\netc\\n4.Apply thedecision listclassi\\x02er tothetraining setandaddallexamples which aretagged with greater than a\\nthreshold reliability totheSense AandSense Bsets.\\nsense training example\\n? compan ysaidthattheplant isstilloperating\\nA although thousands ofplant andanimal species\\nA zonal distrib ution ofplant life\\nB compan ymanuf acturing plant isinOrlando\\netc\\n5.Iterate theprevious steps 3and4until convergence\\n6.Apply theclassi\\x02er totheunseen testdata\\nYarowskyalso demonstrated theprinciple of`one sense perdiscourse\\' (again,averystrong, butnotabsolute effect).\\nThis canbeused asanadditional re\\x02nement forthealgorithm above.\\nYarowskyargues thatdecision listsworkbetter than manyother statistical frame works because noattempt ismade to\\ncombine probabilities. This would becomple x,because thecriteria arenotindependent ofeach other .\\nYarowsky\\'sexperiments were nearly allonhomon yms: these principles probably don\\'thold aswell forsense exten-\\nsion.\\n546.12 Evaluation ofWSD\\nThebaseline forWSD isgenerally `pick themost frequent\\' sense: thisishard tobeat! However,inmanyapplications,\\nwedon\\'tknowthefrequenc yofsenses.\\nSENSEV ALandSENSEV AL-2 evaluated WSD inmultiple languages, with various criteria, butgenerally using Word-\\nNetsenses forEnglish. Thehuman ceiling forthistaskvaries considerably between words: probably partly because of\\ninherent differences insemantic distance between groups ofuses andpartly because ofWordNet itself, which some-\\ntimes makesvery\\x02ne-grained distinctions. Aninteresting variant inSENSEV AL-2 wastodooneexperiment onWSD\\nwhere thedisambiguation waswith respect touses requiring different translations intoJapanese. This hastheadvan-\\ntage thatitisuseful andrelati velyobjecti ve,butsometimes thistaskrequires splitting terms which aren\\' tpolysemous\\ninEnglish (e.g., water \\x97hotvscold). Performance ofWSD onthistask seems abitbetter than thegeneral WSD\\ntask.\\n6.13 Further reading\\nJ&M gointoquite alotofdetail about compositional semantics including underspeci\\x02cation.\\nWordNet isfreely downloadable: thewebsite haspointers toseveralpapers which provide agood introduction.\\nForalotmore detail ofWSD than provided byJ&M, seeManning andSch¨utze who haveaverydetailed account of\\nWSD andword-sense induction:\\nManning, Christopher andHinrich Sch¨utze (1999), Foundations ofStatistical Natur alLangua geProcessing ,MIT\\nPress\\nYarowsky\\'spaper iswell-written andshould beunderstandable:\\nYarowsky,David(1995)\\nUnsupervised wordsense disambiguation rivalling supervised methods ,\\nProceedings ofthe33rd Annual Meeting oftheAssociation forComputational Linguistics (ACL-95) MIT, 189\\x96196\\nLikemanyother recent NLP papers, thiscanbedownloaded viawww .citeseer .com\\n557Lectur e7:Discourse\\nUtterances arealwaysunderstood inaparticular conte xt.Conte xt-dependent situations include:\\n1.Referring expressions: pronouns, de\\x02nite expressions etc.\\n2.Universe ofdiscourse: every dogbarked,doesn\\' tmean everydogintheworld butonly everydoginsome\\nexplicit orimplicit conte xtual set.\\n3.Responses toquestions, etc:only makesense inaconte xt:Who came totheparty? NotSandy .\\n4.Implicit relationships between events: Max fell.Johnpushed him\\x97thesecond sentence is(usually) understood\\nasproviding acausal explanation.\\nInthe\\x02rst part ofthislecture, Igiveabrief overvie wofrhetorical relations which canbeseen asstructuring text\\natalevelabovethesentence. I\\'llthen goontotalkabout oneparticular case ofconte xt-dependent interpretation \\x97\\nanaphor resolution. Iwilldescribe analgorithm foranaphor resolution which uses arelati velybroad-co verage shallo w\\nparser andthen discuss avariant ofitthatrelies onPOS-tagging andregular expression matching rather than parsing.\\n7.1 Rhetorical relations andcoher ence\\nConsider thefollowing discourse:\\nMax fell. John pushed him.\\nThis discourse canbeinterpreted inatleast twoways:\\n1.Max fellbecause John pushed him.\\n2.Max fellandthen John pushed him.\\nThere seems tobeanimplicit relationship between thetwooriginal sentences: adiscour serelation orrhetorical\\nrelation .(Iwillusetheterms interchangeably here, though different theories usedifferent terminology ,andrhetorical\\nrelation tends torefer toamore surfacyconcept than discourse relation.) In1thelinkisaform ofexplanation, but2is\\nanexample ofnarration. Theories ofdiscourse/rhetorical relations reify linktypes such asExplanation andNarr ation .\\nTherelationship ismade more explicit in1and2than itwasintheoriginal sentence: because andandthen aresaid\\ntobecuephrases.\\n7.2 Coher ence\\nDiscourses havetohaveconnecti vitytobecoherent:\\nKim gotintohercar.Sandy likesapples.\\nBoth ofthese sentences makeperfect sense inisolation, buttakentogether theyareincoherent. Adding conte xtcan\\nrestore coherence:\\nKim gotintohercar.Sandy likesapples, soKim thought she\\'dgotothefarmshop andseeifshecould\\ngetsome.\\nThesecond sentence canbeinterpreted asanexplanation ofthe\\x02rst. Inmanycases, thiswillalsoworkiftheconte xt\\nisknown,evenifitisn\\'texpressed.\\nStrate gicgeneration requires awayofimplementing coherence. Forexample, consider asystem thatreports share\\nprices. This might generate:\\nIntrading yesterday: Dell wasup4.2%, Safewaywasdown3.2%, Compaq wasup3.1%.\\n56This ismuch lessacceptable than aconnected discourse:\\nComputer manuf acturers gained intrading yesterday: Dell wasup4.2% andCompaq wasup3.1%. But\\nretail stocks suffered: Safewaywasdown3.2%.\\nHere butindicates aContrast. Notmuch actual information hasbeen added (assuming weknowwhat sortofcompan y\\nDell, Compaq andSafewayare), butthediscourse iseasier tofollow.\\nDiscourse coherence assumptions canaffectinterpretation:\\nJohn likesBill. Hegavehimanexpensi veChristmas present.\\nIfweinterpret thisasExplanation, then `he\\' ismost likelyBill. ButifitisJusti\\x02cation (i.e., thespeak erisjustifying\\nthe\\x02rstsentence), then `he\\'isJohn.\\n7.3 Factors in\\x03uencing discourse inter pretation\\n1.Cuephrases. These aresometimes unambiguous, butnotusually .e.g.andisacuephrase when used insentential\\norVPconjunction.\\n2.Punctuation (also prosody) andtextstructure. Forinstance, parenthetical information cannot berelated toa\\nmain clause byNarration, butalistisoften interpreted asNarration:\\nMax fell(John pushed him) andKim laughed.\\nMax fell,John pushed himandKim laughed.\\nSimilarly ,enumerated listscanindicate aform ofnarration.\\n3.Real worldcontent:\\nMax fell. John pushed himashelayontheground.\\n4.Tense andaspect.\\nMax fell. John hadpushed him.\\nMax wasfalling. John pushed him.\\nItshould beclear thatitispotentially veryhard toidentify rhetorical relations. Infact,recent research thatsimply\\nuses cuephrases andpunctuation isproving quite promising. This canbedone byhand-coding aseries of\\x02nite-state\\npatterns, orbyaform ofsupervised learning.\\n7.4 Discourse structur eandsummarization\\nIfweconsider adiscourse relation asarelationship between twophrases, wegetabinary branching treestructure for\\nthediscourse. Inmanyrelationships, such asExplanation, onephrase depends ontheother: e.g., thephrase being\\nexplained isthemain oneandtheother issubsidiary .Infactwecangetridofthesubsidiary phrases andstillhave\\nareasonably coherent discourse. (The main phrase issometimes called thenucleus andthesubsidiary oneisthe\\nsatellite .)This canbeexploited insummarization.\\nForinstance:\\nWegetabinary branching treestructure forthediscourse. Inmanyrelationships onephrase depends on\\ntheother .Infactwecangetridofthesubsidiary phrases andstillhaveareasonably coherent discourse.\\nOther relationships, such asNarration, giveequal weight toboth elements, sodon\\'tgiveanyclues forsummarization.\\nRather than trying to\\x02ndrhetorical relations forarbitrary text,genre-speci\\x02c cues canbeexploited, forinstance for\\nscienti\\x02c texts.This allowsmore detailed summaries tobeconstructed.\\n577.5 Referring expr essions\\nI\\'llnowmoveontotalking about another form ofdiscourse structure, speci\\x02cally thelinkbetween referring expres-\\nsions. Thefollowing example willbeused toillustrate referring expressions andanaphora resolution:\\nNiall Ferguson isproli\\x02c, well-paid andasnapp ydresser .Stephen Moss hated him\\x97atleast until he\\nspent anhour being charmed inthehistorian\\' sOxford study .(quote takenfrom theGuardian)\\nSome terminology:\\nreferentarealworld entity thatsome piece oftext(orspeech) refers to.e.g., thetwopeople who arementioned in\\nthisquote.\\nreferring expr essions bitsoflanguage used toperform reference byaspeak er.In,theparagraph above,Niall Fergu-\\nson,himandthehistorian areallbeing used torefer tothesame person (theycorefer).\\nantecedent thetextevoking areferent. Niall Ferguson istheantecedent ofhimandthehistorian\\nanaphora thephenomenon ofreferring toanantecedent: himandthehistorian areanaphoric because theyrefer toa\\npreviously introduced entity .\\nWhat about asnappy dresser ?Traditionally ,thiswould bedescribed aspredicati ve:thatis,itisapredicate, likean\\nadjecti ve,rather than being areferring expression itself.\\nGenerally ,entities areintroduced inadiscourse (technically ,evoked)byinde\\x02nite noun phrases orproper names.\\nDemonstrati vesandpronouns aregenerally anaphoric. De\\x02nite noun phrases areoften anaphoric (asabove),butoften\\nused tobring amutually knownanduniquely identi\\x02able entity intothecurrent discourse. e.g., thepresident ofthe\\nUS.\\nSometimes, pronouns appear before their referents areintroduced: thisiscataphor a.E.g., atthestart ofadiscourse:\\nAlthough shecouldn\\' tseeanydogs, Kim wassure she\\'dheard barking.\\nboth cases ofsherefer toKim -the\\x02rstisacataphor .\\n7.6 Pronoun agreement\\nPronouns generally havetoagree innumber andgender with their antecedents. Incases where there\\' sachoice of\\npronoun, such ashe/sheoritforananimal (orababy,insome dialects), then thechoice hastobeconsistent.\\n(25) Alittle girlisatthedoor \\x97seewhat shewants, please?\\n(26) Mydoghashurthisfoot\\x97heisinalotofpain.\\n(27) *Mydoghashurthisfoot\\x97itisinalotofpain.\\nComplications include thegender neutral they(some dialects), useoftheywith everybody ,group nouns, conjunctions\\nanddiscontinuous sets:\\n(28) Somebody\\' satthedoor \\x97seewhat theywant,willyou?\\n(29) Idon\\'tknowwho thenewteacher willbe,butI\\'msure they\\'llmakechanges tothecourse.\\n(30) Everybody\\' scoming totheparty ,aren\\' tthey?\\n(31) Theteam played really well, butnowtheyareallverytired.\\n(32) Kim andSandy areasleep: theyareverytired.\\n(33) Kim issnoring andSandy can\\'tkeephereyesopen: theyareboth exhausted.\\n587.7 Re\\x03exi ves\\n(34) Johnicuthimselfishaving. (himself =John, subscript notation used toindicate this)\\n(35) #Johnicuthimjshaving. (i6=j\\x97averyoddsentence)\\nTheinformal andnotfully adequate generalisation isthatre\\x03exivepronouns must beco-referential with apreceding\\nargument ofthesame verb(i.e., something itsubcate gorizes for), while non-re\\x03e xivepronouns cannot be.Inlinguis-\\ntics,thestudy ofinter-sentential anaphora isknownasbinding theory :Iwon\\'tdiscuss thisfurther ,since theconstraints\\nonreference involvedarequite different from those with intra-sentential anaphora.\\n7.8 Pleonastic pronouns\\nPleonastic pronouns aresemantically empty ,anddon\\'trefer:\\n(36) Itissnowing\\n(37) Itisnoteasy tothink ofgood examples.\\n(38) Itisobvious thatKim snores.\\n(39) Itbothers Sandy thatKim snores.\\nNote also:\\n(40) Theyaredigging upthestreet again\\nThis isan(informal) useoftheywhich, though probably nottechnically pleonastic, doesn\\' tapparently refer toa\\ndiscourse referent inthestandard way(they=`theauthorities\\'??).\\n7.9 Salience\\nThere areanumber ofeffects which cause particular pronoun referents tobepreferred, after allthehard constraints\\ndiscussed abovearetakenintoconsideration.\\nRecency More recent referents arepreferred. Only relati velyrecently referred toentities areaccessible.\\n(41) Kim hasafastcar.Sandy hasanevenfaster one. Leelikestodriveit.\\nitpreferentially refers toSandy\\' scar,rather than Kim\\' s.\\nGrammatical roleSubjects>objects>everything else:\\n(42) Fred went totheGrafton Centre with Bill. Hebought aCD.\\nheismore likelytobeinterpreted asFred than asBill.\\nRepeated mention Entities thathavebeen mentioned more frequently arepreferred:\\n(43) Fred wasgetting bored. Hedecided togoshopping. Billwent totheGrafton Centre with Fred. He\\nbought aCD.\\nHe=Fred (maybe) despite thegeneral preference forsubjects.\\nParallelism Entities which share thesame roleasthepronoun inthesame sortofsentence arepreferred:\\n(44) Billwent with Fred totheGrafton Centre. Kim went with himtoLion Yard.\\nHim=Fred, because theparallel interpretation ispreferred.\\n59Coher ence effects Thepronoun resolution may depend ontherhetorical/discourse relation thatisinferred.\\n(45) BilllikesFred. Hehasagreat sense ofhumour .\\nHe=Fred preferentially ,possibly because thesecond sentence isinterpreted asanexplanation ofthe\\x02rst, and\\nhaving asense ofhumour isseen asareason tolikesomeone.\\n7.10 Algorithms forresolving anaphora\\nMost workhasgone intotheproblem ofresolving pronoun referents. Aswell asdiscourse understanding, thisisoften\\nimportant inMT.Forinstance, English ithastoberesolv edtotranslate intoGerman because German hasgrammatical\\ngender (though note, ifthere aretwopossible antecedents, butboth havethesame gender ,weprobably donotneed\\ntoresolv ebetween thetwoforMT). Iwilldescribe oneapproach toanaphora resolution andamodi\\x02cation ofitthat\\nrequires fewerresources.\\n7.11 Lappin andLeass (1994)\\nThe algorithm relies onparsed text(from afairly shallo w,verybroad-co verage parser ,which unfortunately isn\\'t\\ngenerally available). Thetextthesystem wasdeveloped andtested onwasallfrom online computer manuals. The\\nfollowing description isalittle simpli\\x02ed:\\nThediscourse model consists ofasetofreferring NPs arranged intoequivalence classes, each class having aglobal\\nsalience value.\\nForeach sentence:\\n1.Divide bytwotheglobal salience factors foreach existing equivalence class.\\n2.Identify referring NPs (i.e., exclude pleonastic itetc)\\n3.Calculate global salience factors foreach NP(seebelow)\\n4.Update thediscourse model with thereferents andtheir global salience scores.\\n5.Foreach pronoun:\\n(a)Collect potential referents (cutoffisfour sentences back).\\n(b)Filter referents according tobinding theory andagreement constraints.\\n(c)Calculate theperpronoun adjustments foreach referent (seebelow).\\n(d)Select thereferent with thehighest salience value foritsequivalence class plus itsper-pronoun adjustment.\\nIncase ofatie,prefer theclosest referent inthestring.\\n(e)Add thepronoun intotheequivalence class forthatreferent, andincrement thesalience factor bythe\\nnon-duplicate salience factors pertaining tothepronoun.\\nThe salience factors were determined experimentally .Global salience factors mostly takeaccount ofgrammatical\\nfunction \\x97theyencode thehierarch ymentioned previously .Theygivelowest weight toanNPinanadverbial\\nposition, such asinside anadjunct PP.This isachie vedbygiving everynon-adv erbial anextrapositi vescore, because\\nwewantallglobal salience scores tobepositi veintegers. Embedded NPs arealsodownweighted bygiving apositi ve\\nscore tonon-embedded NPs. Recenc yweights mean thatintra-sentential binding ispreferred.\\nGlobal salience factors.\\nrecenc y 100\\nsubject 80\\nobjects ofexistential sentences 70\\ndirect object 50\\nindirect object 40\\noblique complement 40\\nnon-embedded noun 80\\nother non-adv erbial 50\\n`Existential objects\\' refers toNPs which areinsyntactic object position insentences such as:\\n60There isacatinthegarden.\\nHere acatissyntactically anobject, butfunctions more likeasubject, while there,which issyntactically thethe\\nsubject, does notrefer.Anoblique complement isacomplement other than anoun phrase, such asaPP.\\nTheper-pronoun modi\\x02cations havetobecalculated each time acandidate pronoun isbeing evaluated. Themodi\\x02-\\ncations strongly disprefer cataphora andslightly prefer referents which are`parallel\\', where parallel here justmeans\\nhaving thesame syntactic role.\\nPerpronoun salience factors:\\ncataphora -175\\nsame role 35\\nApplying thistothesample discourse:\\nNiall Ferguson isproli\\x02c, well-paid andasnapp ydresser .\\nStephen Moss hated him\\x97atleast until hespent anhour being charmed inthehistorian\\' sOxford study .\\nAssume wehaveprocessed upto`\\x97\\' andareresolving he.Discourse referents:\\nN Niall Ferguson ,him 435\\nS Stephen Moss 310\\nIamassuming thatasnappy dresser isignored, although itmight actually betreated asanother potential referent,\\ndepending ontheparser .\\nNhasscore 155+280((subject +non-embedded +non-adv erbial +recenc y)/2+(direct object +head +non-adv erbial\\n+recenc y))\\nShasscore 310(subject +non-embedded +non-adv erbial +recenc y)+same roleper-pronoun 35\\nSointhiscase, thewrong candidate wins.\\nWenowaddhetothediscourse referent equivalence class. Theadditional weight isonly 80,forsubject, because we\\ndon\\'taddweights foragivenfactor more than once inasentence.\\nN Niall Ferguson ,him,he 515\\nNote thatthewrong result isquite plausible:\\nNiall Ferguson isproli\\x02c, well-paid andasnapp ydresser .\\nStephen Moss hated him\\x97atleast until hespent anafternoon being intervie wed atveryshort notice.\\nTheoverall performance ofthealgorithm reported byLappin andLeass was86% butthiswasoncomputer manuals\\nalone. Their results can\\'tbedirectly replicated, duetotheir useofaproprietary parser ,butother experiments suggest\\nthattheaccurac yonother types oftextcould belower.\\n7.12 Anaphora foreveryone\\nItispotentially important toresolv eanaphoric expressions, evenfor`shallo w\\'NLP tasks, such asWebsearch, where\\nfullparsing isimpractical. Anarticle which mentions thename `Niall Ferguson\\' once, butthen hasmultiple uses of\\n`he\\', `thehistorian\\' etcreferring tothesame person ismore relevanttoasearch for`Niall Ferguson\\' than onewhich\\njustmentions thename once. Itistherefore interesting toseewhether analgorithm canbedeveloped which does not\\nrequire parsed text.Kennedy andBogurae v(1996) describe avariant ofLappin andLeass which wasdeveloped for\\ntextwhich hadjustbeen tagged forpart-of-speech.\\nTheinput textwastagged with theLingsoft tagger (averyhigh precision andrecall tagger thatuses manually developed\\nrules: seehttp://www.lingsoft.fi/demos.html ).Besides POS tags, thisgivessome grammatical function\\ninformation: e.g., itnotates subjects (forEnglish, thisisquite easy todoonthebasis ofPOS-tagged textwith some\\nsimple regular expressions). Thetextwasthen runthrough aseries ofregular expression \\x02lters toidentify NPs and\\nmark expleti veit.Heuristics de\\x02ned asregular expressions arealsoused toidentify theNPs grammatical role. Global\\nsalience factors areasinLappin andLeass, butKennedy andBogurae vaddafactor forconte xt(asdetermined bya\\ntextsegmentation algorithm). Theyalsouseadistinct factor forpossessi veNPs.\\nBecause thisalgorithm doesn\\' thaveaccess toaparser ,theimplementation ofbinding theory hastorelyonheuristics\\nbased ontherolerelationships identi\\x02ed. Otherwise, thealgorithm ismuch thesame asforLappin andLeass.\\n61Overall accurac yisquoted as75%, measured onamixture ofgenres (soitisn\\'tpossible todirectly compare with\\nLappin andLeass, since thatwasonly tested oncomputer manual information). Fewerrors were caused bythelack\\nofdetailed syntactic information. 35% oferrors were caused byfailure toidentify gender correctly ,14% were caused\\nbecause quoted conte xtsweren\\' thandled.\\n7.13 Another note onevaluation\\nThesituation with respect toevaluation ofanaphora resolution islesssatisf actory than POS tagging orWSD. This is\\npartly because oflackofevaluation materials such asindependently mark ed-up corpora. Another factor isthedif\\x02culty\\ninreplication: e.g., Lappin andLeass\\' salgorithm can\\'tbefully replicated because oflack ofavailability oftheparser .\\nThis canbepartially circumv ented byevaluating algorithms ontreebanks, butexisting treebanks arerelati velylimited\\ninthesortoftexttheycontain. Alternati vely,different parsers canbecompared according totheaccurac ywith which\\ntheysupply thenecessary information, butagainthisrequires asuitable testing environment.\\n7.14 Further reading\\nJ&M discuss themost popular approach torhetorical relations, rhetorical structur etheory orRST.Ihaven\\'tdiscussed\\nitindetail here, partly because I\\x02ndthetheory veryunclear: attempts toannotate textusing RST approaches tend\\nnottoyield good interannotator agreement (see comments onevaluation inlecture 3),although tobefair,thisisa\\nproblem with allapproaches torhetorical relations. Thediscussion ofthefactors in\\x03uencing anaphora resolution and\\nthedescription oftheLappin andLeass algorithm thatI\\'vegivenhere arepartly based onJ&M\\' saccount.\\nThereferences belowareforcompleteness rather than suggested reading:\\nLappin, Shalom andHerb Leass (1994)\\nAnalgorithm forpronominal anaphor aresolution ,\\nComputational Linguistics 20(4), 535\\x96561\\nKennedy ,Christopher andBranimir Bogurae v(1996)\\nAnaphor aforeveryone: pronominal anaphor aresolution without aparser,\\nProceedings ofthe16th International Conference onComputational Linguistics (COLING 96), Copenhagen, Den-\\nmark, 113\\x96118\\n628Lectur e8:Applications\\nThis lecture considers three applications ofNLP: machine translation, spok endialogue systems andemail response.\\nThis isn\\'tintended asacomplete overvie wofthese areas, butjustasawayofdescribing howsome ofthetechniques\\nwe\\'veseen intheprevious lectures arebeing used incurrent systems orhowtheymight beused inthefuture.\\nMachine translation\\n8.1 Methodology forMT\\nThere arefour main classical approaches toMT:\\n\\x0fDirect transfer: map between morphologically analysed structures.\\n\\x0fSyntactic transfer: map between syntactically analysed structures.\\n\\x0fSemantic transfer: map between semantics structures.\\n\\x0fInterlingua: construct alanguage-neutral representation from parsing andusethisforgeneration.\\nThe standard illustration ofthedifferent classical approaches toMTistheVauquois triangle. This issupposed to\\nillustrate theamount ofeffortrequired foranalysis andgeneration asopposed totransfer inthedifferent approaches.\\ne.g., direct transfer requires verylittle effortforanalysis orgeneration, since itsimply involvesmorphological analysis,\\nbutitrequires more effortontransfer than syntactic orsemantic transfer do.\\n-direct-syntactic transfer-semantic transfer\\x1e\\nanalysis\\n^generation\\nSource Language TargetLanguageInterlingua\\nTheVauquois triangle ispotentially misleading, because itsuggests asimple trade-of fineffort. Itisatleast asplausi-\\nblethatthecorrect geometry isasbelow(theVauquois inverted funnel with verylong spout):\\n63-direct-syntactic transfer-semantic\\ntransfer\\nunderspeci\\x02ed semanticsresolv edlogical form\\nSource Language TargetLanguageLanguage Neutral Utterance Representation\\nThis diagram isintended toindicate thatthegoal ofproducing alanguage-neutral representation may beextremely\\ndif\\x02cult!\\nStatistical MTinvolveslearning translations from aparallel corpus :i.e.acorpus consisting ofmultiple versions of\\nasingle textindifferent languages. Theclassic workwasdone ontheproceedings oftheCanadian parliament (the\\nCanadian Hansard). Itisnecessary toalign thetexts,sothatsentences which aretranslations ofeach other arepaired:\\nthisisnon-tri vial(the mapping may notbeone-to-one). The original statistical MTapproach canbethought ofas\\ninvolving direct transfer ,with some more recent workbeing closer tosyntactic (orevensemantic) transfer .\\nExample-based MTinvolvesusing adatabase ofexisting translation pairs andtrying to\\x02nd theclosest matching\\nphrase. Itisveryuseful aspartofmachine-aided translation.\\n8.2 MTusing semantic transfer\\nSemantic transfer isanapproach toMTwhich involves:\\n1.Parsing asourcelangua gestring toproduce ameaning representation\\n2.Transforming thatrepresentation tooneappropriate forthetargetlangua ge\\n3.Generating from thetransformed representation\\nConstraint-based grammars arepotentially well suited tosemantic transfer .\\nForinstance:\\nInput: Kim singt\\nSource LF: named (x;\\x93Kim\\x94 );singen (e;x)\\nTargetLF: named (x;\\x93Kim\\x94 );sing(e;x)\\nOutput: Kim sings\\nTransfer rules:\\nsingen (e;x)$sing(e;x)\\n$indicates transfer equivalence ortranslation equivalence :thedouble arrowindicates reversibility:\\nInput: Kim sings\\nSource LF: named (x;\\x93Kim\\x94 );sing(e;x)\\nTargetLF: named (x;\\x93Kim\\x94 );singen (e;x)\\nOutput: Kim singt\\n64`named\\' canberegarded asalanguage-neutral predicate, sonotransfer isnecessary .Wealsoassume wedon\\'tchange\\nstrings like\\x93Kim\\x94.\\nSEMANTIC TRANSFER\\n*\\nj\\nPARSING\\n6\\nMORPHOLOGY\\n6\\nINPUT PROCESSING\\n6\\nsourcelanguage inputTACTICAL GENERA TION\\n?\\nMORPHOLOGY GENERA TION\\n?\\nOUTPUT PROCESSING\\n?\\ntargetlanguage output\\nSemantic transfer rules areaform ofquasi-inference: theymap between meaning representations. Obviously the\\nexample abovewastrivial: more generally some form ofmismatc hislikelytobeinvolved,although theidea of\\nsemantic transfer isthat there isless mismatch atthesemantic levelthan atasyntactic level.Semantic transfer\\ndoes notrequire thatquanti\\x02er scope beresolv ed.Semantic transfer requires detailed bidirectional grammars forthe\\nlanguages involved,which currently makesitmore suitable forhigh-precision, limited domain systems.\\nAnanaphora resolution module ispotentially needed when translating between languages likeEnglish andGerman,\\nsince English itcancorrespond toGerman er,sieores,forinstance. Buttheresolution should bedone onan`as-\\nneeded\\' basis, triggered bytransfer ,since itsome conte xtsthere isnoambiguity .\\nSome deplo yedMTsystems useaform ofsemantic transfer ,butsyntactic transfer ismore common. Inthese systems,\\ngeneration isusually aform oftextreconstruction, rather than `proper\\' tactical generation. Direct transfer isused asa\\nfallback ifsyntactic analysis fails. Systran uses amixture ofdirect transfer andsyntactic transfer: itworks reasonably\\nwell because ithasanenormous lexicon ofphrases. Handling multiw ordexpressions isamajor problem inMT.\\nStatistical MTisthecommonest approach intheresearch community ,followed bysemantic transfer .\\nAllMTsystems require some form ofWSD: potentially bigimpro vements could bemade inthisarea. One dif\\x02culty ,\\nhowever,isthatMTsystems often havetooperate with rather small amounts oftext,which limits theavailability of\\ncues.\\nDialogue systems\\n8.3 Human dialogue basics\\nTurn-taking: generally there arepoints where aspeak erinvites someone else totakeaturn (possibly choosing a\\nspeci\\x02c person), explicitly (e.g., byasking aquestion) orotherwise.\\nPauses: pauses between turns aregenerally veryshort (afewhundred milliseconds, buthighly culture speci\\x02c).\\nLonger pauses areassumed tobemeaningful: example from Levinson (1983: 300)\\nA:Isthere something bothering youornot? (1.0secpause)\\nA:Yesorno?(1.5secpause)\\n65A:Eh?\\nB:No.\\nTurn-taking disruption isverydif\\x02cult toadjust to.This isevident insituations such asdelays onphone lines\\nandpeople using speech prostheses, aswell asslowautomatic systems.\\nOverlap: Utterances canoverlap (theacceptability ofthisisdialect/culture speci\\x02c butunfortunately humans tend to\\ninterrupt automated systems \\x97thisisknownasbargein).\\nBackchannel: Utterances likeUh-huh ,OKcanoccur during other speak er\\'sutterance asasign thatthehearer is\\npaying attention.\\nAttention: The speak erneeds reassurance thatthehearer isunderstanding/paying attention. Often eyecontact is\\nenough, butthisisproblematic with telephone conversations, dark sunglasses, etc. Dialogue systems should\\ngiveexplicit feedback.\\nCooperati vity: Because participants assume theothers arecooperati ve,wegeteffects such asindirect answers to\\nquestions.\\nWhen doyouwanttoleave?\\nMymeeting starts at3pm.\\nAllofthese phenomena mean thattheproblem ofspok endialogue understanding isverycomple x.This together with\\ntheunreliability ofspeech recognition means thatspok endialogue systems arecurrently only usable forverylimited\\ninteractions.\\n8.4 Spok endialogue systems\\n1.Single initiati vesystems (also knownassystem initiati vesystems): system controls what happens when.\\nSystem: Which station doyouwanttoleavefrom?\\nUser: King\\' sCross\\nGenerally verylimited: forinstance, intheexample abovethesystem won\\'taccept anything that\\'snotastation\\nname. Soitwouldn\\' taccept either King\\' sCrossorLiverpool Street,depending onwhen thenexttrainto\\nCambridg eis.Designing such systems tends toinvolveHCI issues (persuading theuser nottocomplicate\\nthings), rather than language related ones.\\n2.Mixedinitiati vedialogue. Both participants cancontrol thedialogue tosome extent.\\nSystem: Which station doyouwanttoleavefrom?\\nUser: Idon\\'tknow,tellmewhich station Ineed forCambridge.\\nThe user hasresponded toaquestion with aquestion oftheir own, thereby taking control ofthedialogue.\\nUnfortunately ,getting systems likethistoworkproperly isverydif\\x02cult andalthough research systems anda\\nsmall number ofpractical systems havebeen built, performance isoften better ifyoudon\\'tallowthissortof\\ninteraction. Theterm `mixed-initiati ve\\'isoften used (some what misleadingly) forsystems which simply allow\\nusers tooptionally specify more than onepiece ofinformation atonce:\\nSystem: Which daydoyouwanttoleave?\\nUser: thetwenty-third\\nOR\\nUser: thetwenty-third ofFebruary\\n3.Dialogue tracking. Explicit dialogue models may impro veperformance inother tasks such asspok enlanguage\\nmachine translation orsummarising ahuman-to-human dialogue. Generally it\\'slesscritical togeteverything\\nright insuch cases, which means broader domains arepotentially realistic.\\n66The useofFSAs incontrolling dialogues wasmentioned inlecture 2.Initial versions ofsimple SDSs cannowbe\\nbuiltinafewweeks using toolkits developed byNuance andother companies: CFGs aregenerally hand-b uiltforeach\\ndialogue state. This istime-consuming, buttesting theSDS with realusers andre\\x02ning ittoimpro veperformance is\\nprobably amore serious bottleneck indeplo ying systems.\\nEmail response using deep grammars\\n8.5 Alargecoverage grammar\\nTheemail response application thatImentioned inlecture 1might beaddressed using domain-speci\\x02c grammars, but\\nunlik eindialogue systems, itismuch more dif\\x02cult tomakethelimitations inthegrammar obvious totheuser (and\\nifthecoverage isverylimited amenu-dri vensystem might well workbetter). Itistooexpensi vetomanually builda\\nnewbroad-co verage grammar foreach newapplication andgrammar induction isgenerally notfeasible because the\\ndata thatisavailable istoolimited. TheLinGO ERG constraint-based grammar mentioned inlecture 5hasbeen used\\nforparsing incommercially-deplo yedemail response systems. The grammar wasslightly tailored forthedifferent\\ndomains, butthismostly involvedadding lexical entries. TheERG hadpreviously been used ontheVerbmobil spok en\\nlanguage MTtask: theexamples belowaretakenfrom this.\\nIndication ofcoverage oftheERG:\\n1. Theweek ofthetwenty second, Ihavetwohour blocks available.\\n2. Ifyougivemeyour name andyour address wewillsend youtheticket.\\n3. Okay ,actually Iforgottosaythatwhat weneed isatwohour meeting.\\n4. Themorning isgood, butnine o\\'clock might bealittle toolate, asI\\nhaveaseminar atteno\\'clock.\\n5. Well,Iamgoing onvacation forthenexttwoweeks, sothe\\x02rstday\\nthatIwould beable tomeet would betheeighteenth\\n6. Didyousaythatyouwere freefrom three to\\x02vep.m. onWednesday ,\\nthethird, because ifsothatwould beaperfect time forme.\\nCoverage wasaround 80% onVerbmobil.\\nEf\\x02cienc y(with thePET system onan850MHz CPU):\\nItem Word Lexical Readings First All Passive\\nLength Entries Reading Readings Edges\\n1 12 33 15 150ms 270ms 1738\\n2 15 41 2 70ms 110ms 632\\n3 15 63 8 70ms 140ms 779\\n4 21 76 240 90ms 910ms 5387\\n5 26 87 300 1460 ms 8990 ms 41873\\n6 27 100 648 1080 ms 1450 ms 7850\\nTheERG andother similar systems havedemonstrated thatitispossible touseageneral purpose grammar inmultiple\\napplications. However,itiscrucial thatthere isafallback strate gywhen aparse fails. Foremail response, thefallback\\nistosend theemail toahuman. Reliability oftheautomated system isextremely important: sending aninappropriate\\nresponse canbeverycostly .\\nAbigdif\\x02culty foremail response isconnecting thesemantics produced bythegeneral purpose grammar tothe\\nunderlying knowledge base ordatabase. This isexpensi veinterms ofmanpo wer,butdoes notrequire much linguistic\\nexpertise. Hence, thissortofapproach ispotentially commercially viable fororganisations thathavetodeal with\\nalotoffairly routine email. Although tailoring thegrammar byadding lexical entries isnottoohard, itismuch\\nmore dif\\x02cult tomanually adjust theweights ongrammar rules andlexical entries sothatthebest parse ispreferred:\\nautomatic methods arede\\x02nitely required here. Much lesstraining data isrequired totune agrammar than toinduce\\none.\\n8.6 Further reading\\nJ&M discuss MTandspok endialogue systems.\\n67Aglossary/index ofsome oftheterms used inthelectur es\\nThis isprimarily intended tocoverconcepts which arementioned inmore than onelecture. Thelecture where theterm\\nisexplained inmost detail isgenerally indicated. Insome cases, Ihavejustgivenapointer tothesection inthelectures\\nwhere theterm isde\\x02ned. Note thatIGE stands forTheInternet Grammar ofEnglish ,http://www .ucl.ac.uk/internet-\\ngrammar/home.htm There areafewcases where thisuses aterm inaslightly different wayfrom these course notes: I\\nhavetried toindicate these.\\nactivechart Seex4.10.\\nadjecti veSeeIGE ornotes forprelecture exercises inlecture 3.\\nadjunct Seeargument andalsoIGE.\\nadverb SeeIGE ornotes forprelecture exercises inlecture 3.\\naf\\x02x Amorpheme which canonly occur inconjunction with other morphemes (lecture 2).\\nAI-complete Ahalf-joking term, applied toproblems thatwould require asolution totheproblem ofrepresenting the\\nworldandacquiring worldknowledge (lecture 1).\\nagreement Therequirement fortwophrases tohavecompatible values forgrammatical features such asnumber and\\ngender .Forinstance, inEnglish, dogsbark isgrammatical butdogbark anddogsbarks arenot. SeeIGE.\\n(lecture 5)\\nambiguity Thesame string (orsequence ofsounds) meaning different things. Contrasted with vagueness .\\nanaphora The phenomenon ofreferring tosomething thatwasmentioned previously inatext.Ananaphor isan\\nexpression which does this, such asapronoun (seex7.5).\\nantonymy Opposite meaning: such asclean anddirty (x6.5).\\nargument Insyntax, thephrases which arelexically required tobepresent byaparticular word(prototypically a\\nverb). This isasopposed toadjunct s,which modify awordorphrase butarenotrequired. Forinstance, in:\\nKim sawSandy onTuesday\\nSandy isanargument butonTuesday isanadjunct. Arguments arespeci\\x02ed bythesubcategorization ofaverb\\netc.Also seetheIGE. (lecture 5)\\naspect Aterm used tocoverdistinctions such aswhether averbsuggests aneventhasbeen completed ornot(as\\nopposed totense, which refers tothetime ofanevent). Forinstance, shewas writing abook vsshewrotea\\nbook .\\nback offUsually used torefer totechniques fordealing with data sparseness inprobabilistic systems: using amore\\ngeneral classi\\x02cation rather than amore speci\\x02c one. Forinstance, using unigram probabilities instead of\\nbigrams; using wordclasses instead ofindividual words (lecture 3).\\nbaseline Inevaluation, theperformance produced byasimple system against which theexperimental technique is\\ncompared (x3.6).\\nbidir ectional Usable forboth analysis andgeneration (lecture 2).\\ncase Distinctions between nominals indicating their syntactic roleinasentence. InEnglish, some pronouns showa\\ndistinction: e.g., sheisused forsubjects, while herisused forobjects. e.g., shelikeshervs*herlikesshe.\\nLanguages such asGerman andLatin mark case much more extensi vely.\\nceiling Inevaluation, theperformance produced bya`perfect\\' system (such ashuman annotation) against which the\\nexperimental technique iscompared (x3.6).\\nchart parsing Seex4.6.\\n68Chomsk yNoam Chomsk y,professor atMIT.Thefounder ofmodern theories ofsyntax inlinguistics.\\nclosed class Refers toparts ofspeech, such asconjunction, forwhich allthemembers could potentially beenumerated\\n(lecture 3).\\ncoher ence Seex7.2\\ncollocation Seex6.10\\ncomplement Forthepurposes ofthiscourse, anargument other than thesubject.\\ncompositionality The idea thatthemeaning ofaphrase isafunction ofthemeaning ofitsparts. compositional\\nsemantics isthestudy ofhowmeaning canbebuiltupbysemantic rules which mirror syntactic structure\\n(lecture 6).\\nconstituent Asequence ofwords which isconsidered asaunitinaparticular grammar (lecture 4).\\nconstraint-based grammar Aformalism which describes alanguage using asetofindependently stated constraints,\\nwithout imposing anyconditions onprocessing orprocessing order (lecture 5).\\ncontext Thesituation inwhich anutterance occurs: includes prior utterances, thephysical environment, background\\nknowledge ofthespeak erandhearer(s), etcetc.\\ncorpus Abody oftextused inexperiments (plural corpor a).Seex3.1.\\ncuephrases Phrases which indicates particular rhetorical relations .\\ndenominal Something derivedfrom anoun: e.g., theverbtango isadenominal verb.\\nderivational morphology Seex2.2\\ndeterminer SeeIGE ornotes forprelecture exercises inlecture 3.\\ndeverbal Something derivedfrom averb:e.g., theadjecti vesurprised .\\ndirectobject SeeIGE. Contrast indir ectobject .\\ndiscourse InNLP,apiece ofconnected text.\\ndiscourse relations Seerhetorical relations .\\ndomain Notaprecise term, butIuseittomean some restricted setofknowledge appropriate foranapplication.\\nerroranalysis Inevaluation, working outwhat sortoferrors arefound foragivenapproach (x3.6).\\nfeatur estructur eSeeLecture 5.\\nfull-f orm lexicon Alexicon where allmorphological variants areexplicitly listed (lecture 2).\\ngeneration Theprocess ofconstructing strings from some input representation. Withbidirectional grammars using\\ncompositional semantics, generation canbesplit intostrategic generation ,which istheprocess ofdeciding on\\nthelogical form (also knownastextplanning ),andtactical generation which istheprocess ofgoing from the\\nlogical form tothestring (also knownasrealization ).x6.2.\\ngenerati vegrammar Thefamily ofapproaches tolinguistics where anatural language istreated asgoverned byrules\\nwhich canproduce allandonly thewell-formed utterances. Lecture 4.\\ngrammar Formally ,inthegenerati vetradition, thesetofrules andthelexicon. Lecture 4.\\nhead Insyntax, themost important element ofaphrase.\\nhear erAnyone ontherecei ving endofanutterance (spok en,written orsigned).x1.3.\\nhomonymy Instances ofpolysemy where thetwosenses areunrelated (x6.8).\\n69hyponymy An`IS-A \\'relationship (x6.4)More general terms arehyper nym s,more speci\\x02c hyponym s.\\nindir ectobject Thebene\\x02ciary inverbphrases likegive apresent toSandy orgive Sandy apresent .Inthiscase the\\nindirect object isSandy andthedirectobject isapresent .\\ninterannotator agreement Thedegree ofagreement between thedecisions oftwoormore humans with respect to\\nsome categorisation (x3.6).\\nlanguage model Aterm generally used inspeech recognition, forastatistical model ofanatural language (lecture 3).\\nlemmatization Finding thestem andaf\\x02xesforwords (lecture 2).\\nlexical ambiguity Ambiguity caused because ofmultiple senses foraword.\\nlexicon ThepartofanNLP system thatcontains information about individual words (lecture 1).\\nlinking Relating syntax andsemantics inlexical entries (x6.1).\\nlocal ambiguity Ambiguity thatarises during analysis etc,butwhich will beresolv edwhen theutterance iscom-\\npletely processed.\\nlogical form Thesemantic representation constructed foranutterance (x6.1).\\nmeaning postulates Inference rules thatcapture some aspects ofthemeaning ofaword.\\nmeronymy The`part-of \\'lexical semantic relation (x6.5).\\nmorpheme Minimal information carrying units within aword(x2.1).\\nmorphology Seex1.2\\nMT Machine translation\\nmultiw ordexpr ession Aconventional phrase thathassomething idiosyncratic about itandtherefore might belisted\\ninadictionary .\\nmumble input Anyunrecognised input inaspok endialogue system (lecture 2).\\nn-gram Asequence ofnwords (x3.2).\\nnamed entity recognition Recognition andcategorisation ofperson names, names ofplaces, dates etc(lecture 4).\\nnoun SeeIGE ornotes forprelecture exercises inlecture 3.\\nnoun phrase (NP) Aphrase which hasanoun assyntactic head .SeeIGE.\\nontology InNLP andAI,aspeci\\x02cation oftheentities inaparticular domain and(sometimes) therelationships\\nbetween them. Often hierarchically structured.\\nopen class Opposite ofclosed class .\\northographic rules spelling rules (x2.3)\\novergenerate Ofagrammar ,toproduce strings which areinvalid, e.g., because theyarenotgrammatical according\\ntohuman judgements.\\npacking Seex4.9\\npassi vechart parsing Seex4.7\\nparse treeSeex4.4\\npart ofspeech Themain syntactic categories: noun, verb,adjecti ve,adverb,preposition, conjunction etc.\\n70part ofspeech tagging Automatic assignment ofsyntactic categories tothewords inatext.The setofcategories\\nused isactually generally more \\x02ne-grained than traditional parts ofspeech.\\npolysemy Thephenomenon ofwords having different senses (x6.8).\\npragmatics Seex1.2\\npredicate Inlogic, something thattakeszero ormore arguments andreturns atruth value. (Used inIGE fortheverb\\nphrase following thesubject inasentence, butIdon\\'tusethatterminology .)\\npre\\x02x Anaf\\x02x thatprecedes thestem .\\nprobabilistic context freegrammars (PCFGs) CFGs with probabilities associated with rules (lecture 4).\\nrealization Another term fortactical generation \\x97seegeneration .\\nreferring expr ession Seex7.5\\nrelativeclause SeeIGE.\\nArestricti verelativeclause isonewhich limits theinterpretation ofanoun toasubset: e.g.thestudents who\\nsleep inlectur esareobviously overworking refers toasubset ofstudents. Contrast non-r estricti ve,which isa\\nform ofparenthetical comment: e.g. thestudents, who sleep inlectur es,areobviously overworking means all\\n(ornearly all)aresleeping.\\nselectional restrictions Constraints onthesemantic classes ofarguments toverbs etc(e.g., thesubject ofthink is\\nrestricted tobeing sentient). Theterm selectional preference isused fornon-absolute restrictions.\\nsemantics Seex1.2\\nsign Asused inlecture 5,thebundle ofproperties representing awordorphrase.\\nsmoothing Redistrib uting observ edprobabilities toallowforsparse data ,especially togiveanon-zero probability\\ntounseen events(lecture 2).\\nsparse data Especially instatistical techniques, data concerning rareeventswhich isn\\'tadequate togivegood proba-\\nbility estimates (lecture 2).\\nspeak erSomeone who makesanutterance (x1.3).\\nspelling rulesx2.3\\nstem Amorpheme which isacentral component ofaword(contrast af\\x02x ).x2.1.\\nstemming Stripping af\\x02x es(seex2.4).\\nstrongequivalence Ofgrammars, accepting/rejecting exactly thesame strings andassigning thesame brack etings\\n(contrast weak equivalence ).Lecture 4.\\nstructural ambiguity Thesituation where thesame string corresponds tomultiple brack etings.\\nsubcategorization Thelexical property thattells ushowmanyargument saverbetccanhave.\\nsuf\\x02x Anaf\\x02x thatfollowsthestem .\\nsummarization Producing ashorter piece oftext(orspeech) thatcaptures theessential information intheoriginal.\\nsynonymy Having thesame meaning (x6.5).\\nsyntax Seex1.2\\ntaxonomy Traditionally ,thescheme ofclassi\\x02cation ofbiological organisms. Extended inNLP tomean ahierarchical\\nclassi\\x02cation ofwordsenses. Theterm ontology issometimes used inarather similar way,butontologies tend\\ntobeclassi\\x02cations ofdomain-kno wledge, without necessarily having adirect linktowords, andmay havea\\nricher structure than ataxonomy .\\n71template Infeature structure grammars, see5.6\\ntense Past,present, future etc.\\ntextplanning Another term forstrategic generation :seegeneration .\\ntraining data Data used totrain anysortofmachine-learning system. Must beseparated from testdata which iskept\\nunseen. Manually-constructed systems should ideally alsousestrictly unseen data forevaluation.\\ntransfer InMT,theprocess ofgoing from arepresentation appropriate totheoriginal (source) language toone\\nappropriate forthetargetlanguage.\\ntreebank acorpus annotated with trees (lecture 4).\\nuni\\x02cation SeeLecture 5,especiallyx5.3.\\nweak equivalence Ofgrammars, accepting/rejecting exactly thesame strings (contrast strongequivalence ).Lecture\\n4.\\nWizard ofOzexperiment Anexperiment where data iscollected, generally foradialogue system, byasking users\\ntointeract with amock-up ofarealsystem, where some orallofthe`processing\\' isactually being done bya\\nhuman rather than automatically .\\nWordNet Seex6.6\\nword-sense disambiguation Seex6.9\\nutterance Apiece ofspeech ortext(sentence orfragment) generated byaspeak erinaparticular conte xt.\\nverb SeeIGE ornotes forprelecture exercises inlecture 3.\\nverbphrase (VP) Aphrase headed byaverb.\\n72Exer cises forNLP course, 2004\\nNotes onexercises\\nThese exercises areorganised bylecture. Theyaredivided intotwoclasses: prelecture andpostlecture. Theprelecture\\nexercises areintended toreviewthebasic concepts thatyou\\'llneed tofully understand thelecture. Depending onyour\\nbackground, youmay \\x02ndthese trivialoryoumay need toread thenotes, butineither case theyshouldn\\' ttakemore\\nthan afewminutes. The\\x02rstoneortwoexamples generally come with answers, other answers areattheend(where\\nappropriate).\\nAnswers tothepostlecture exercises arewith thesupervision notes (where appropriate). These aremostly intended as\\nquick exercises tocheck understanding ofthelecture, though some aremore open-ended.\\nALectur e1\\nA.1 Postlectur eexercises\\nIfyouuseawordprocessor with aspelling andgrammar check er,trylooking atitstreatment ofagreement andsome of\\ntheother phenomena discussed inthelecture. Ifpossible, tryswitching settings between British English andAmerican\\nEnglish.\\nBLectur e2\\nB.1 Prelectur eexercises\\n1.Split thefollowing words into morphological units, labelling each asstem, suf\\x02xorpre\\x02x. Ifthere isany\\nambiguity ,giveallpossible splits.\\n(a)dries\\nanswer: dry(stem), -s(suf\\x02x)\\n(b)cartwheel\\nanswer: cart(stem), wheel (stem)\\n(c)carries\\n(d)running\\n(e)uncaring\\n(f)intruders\\n(g)bookshelv es\\n(h)reattaches\\n(i)anticipated\\n2.Listthesimple pastandpast/passi veparticiple forms ofthefollowing verbs:\\n(a)sing\\nAnswer: simple pastsang ,participle sung\\n(b)carry\\n(c)sleep\\n(d)see\\nNote thatthesimple pastisused byitself (e.g., Kim sang well)while theparticiple form isused with anauxiliary (e.g.,\\nKim hadsung well).Thepassi veparticiple isalwaysthesame asthepast participle inEnglish: (e.g., Kim beganthe\\nlectur eearly ,Kim hadbegunthelectur eearly ,Thelectur ewasbegunearly ).\\n73B.2 Post-lectur eexercises\\n1.Foreach ofthefollowing surfaceforms, givealistofthestates thattheFST giveninthelecture notes for\\ne-insertion passes through, andthecorresponding underlying forms:\\n(a)cats\\n(b)corpus\\n(c)asses\\n(d)assess\\n(e)axes\\n2.Modify theFSA fordates sothatitonly accepts validmonths. Turnyour revised FSA intoaFST which maps\\nbetween thenumerical representation ofmonths andtheir abbre viations (Jan ...Dec).\\nCLectur e3\\nC.1 Pre-lectur e\\nLabel each ofthewords inthefollowing sentences with their partofspeech, distinguishing between nouns, proper\\nnouns, verbs, adjecti ves,adverbs, determiners, prepositions, pronouns andothers. (Traditional classi\\x02cations often\\ndistinguish between alargenumber ofadditional parts ofspeech, butthe\\x02ner distinctions won\\'tbeimportant here.)\\nThere arenotes onpartofspeech distinctions below,ifyouhaveproblems.\\n1.Thebrownfoxcould jump quickly overthedog, Rover.Answer: The/Det brown/Adj fox/Noun could/V erb(modal)\\njump/V erbquickly/Adv erbover/Preposition the/Determiner dog/Noun, Rover/Proper noun.\\n2.Thebigcatchased thesmall dogintothebarn.\\n3.Those barns haveredroofs.\\n4.Dogs often bark loudly .\\n5.Further discussion seems useless.\\n6.Kim didnotlikehim.\\n7.Time\\x03ies.\\nNotes onparts ofspeech. These notes areEnglish-speci\\x02c andarejustintended tohelp with thelectures andtheexer-\\ncises: seealinguistics textbook forde\\x02nitions! Some categories havefuzzy boundaries, butnone ofthecomplicated\\ncases willbeimportant forthiscourse.\\nNoun prototypically ,nouns refer tophysical objects orsubstances: e.g., aardvark ,chainsaw ,rice.Buttheycanalso\\nbeabstract (e.g. truth ,beauty )orrefer toevents, states orprocesses (e.g., decision ).IfyoucansaytheXand\\nhaveasensible phrase, that\\'sagood indication thatXisanoun.\\nPronoun something thatcanstand inforanoun: e.g., him,his\\nProper noun /Proper name aname ofaperson, place etc:e.g., Elizabeth ,Paris\\nVerb Verbs refer toevents, processes orstates butsince nouns andadjecti vescandothisaswell, thedistinction\\nbetween thecategories isbased ondistrib ution, notsemantics. Forinstance, nouns canoccur with determiners\\nlikethe(e.g., thedecision )whereas verbs can\\'t(e.g., *thedecide ).InEnglish, verbs areoften found with\\nauxiliaries (be,have ordo)indicating tense andaspect, andsometime occur with modals, likecan,could etc.\\nAuxiliaries andmodals arethemselv esgenerally treated assubclasses ofverbs.\\n74Adjecti veawordthatmodi\\x02es anoun: e.g., big,loud.Most adjecti vescanalso occur after theverbbeandafew\\nother verbs: e.g., thestudents areunhappy .Numbers aresometimes treated asatype ofadjecti vebylinguists\\nbutgenerally giventheir owncategory intraditional grammars. Pastparticiple forms ofverbs canalsooften be\\nused asadjecti ves(e.g., worried inthevery worried man).Sometimes it\\'simpossible totellwhether something\\nisaparticiple oranadjecti ve(e.g., theman wasworried ).\\nAdverb awordthatmodi\\x02es averb:e.g.quickly,probably .\\nDeterminer these precede nouns e.g., the,every,this.Itisnotalwaysclear whether awordisadeterminer orsome\\ntype ofadjecti ve.\\nPreposition e.g., in,at,with\\nNouns, proper nouns, verbs, adjecti vesandadverbs aretheopen classes :newwords canoccur inanyofthese cate-\\ngories. Determiners, prepositions andpronouns areclosed classes (asareauxiliary andmodal verbs).\\nC.2 Post-lectur e\\nTryoutoneormore ofthefollowing POS tagging sites:\\nhttp://www.coli.uni-sb.de/\\x98thorsten/tnt/\\nhttp://www.comp.lancs.ac.uk/computing/research/ucrel/claws/trial.ht ml\\nhttp://l2r.cs.uiuc.edu/\\x98cogcomp/eoh/posdemo.html\\nOnly the\\x02rst siteuses anapproach comparable tothatdescribed inthelecture. Find twoshort pieces ofnaturally\\noccurring English text,oneofwhich youthink should berelati velyeasy totagcorrectly andonewhich youpredict to\\nbedif\\x02cult. Look atthetagged output andestimate thepercentage ofcorrect tags ineach case, concentrating onthe\\nopen-class words. Youmight liketogetanother student tolook atthesame output andseeifyouagree onwhich tags\\narecorrect.\\nDLectur e4\\nD.1Pre-lectur e\\nPutbrack etsround thenoun phrases andtheverbphrases inthefollowing sentences (ifthere isambiguity ,givetwo\\nbrack etings):\\n1.Thecatwith white furchased thesmall dogintothebarn.\\nAnswer: ((The cat)npwith (white fur)np)npchased (thesmall dog)npinto(thebarn)np\\nThecatwith white fur(chased thesmall dogintothebarn)vp\\n2.Thebigcatwith black furchased thedogwhich barked.\\n3.Three dogs barkedathim.\\n4.Kim sawthebirdw atcher with thebinoculars.\\nNote that noun phrases consist ofthenoun, thedeterminer (ifpresent) andanymodi\\x02ers ofthenoun (adjecti ve,\\nprepositional phrase, relati veclause). This means thatnoun phrases may benested. Verbphrases include theverband\\nanyauxiliaries, plus theobject andindirect object etc(ingeneral, thecomplements oftheverb\\x97discussed inlecture\\n5)andanyadverbial modi\\x02ers. Theverbphrase does notinclude thesubject.\\n75D.2Post-lectur e\\nUsing theCFG giveninthelecture notes (section 4.3):\\n1.showtheedges generated when parsing they\\x02shinriver sinDecember with thesimple chart parser in4.7\\n2.showtheedges generated forthissentence ifpacking isused (asdescribed in4.9)\\n3.showtheedges generated forthey\\x02shinriver sifanactivechart parser isused (asin4.10)\\nELectur e5\\nE.1 Pre-lectur e\\n1.Averysimple form ofsemantic representation corresponds tomaking verbs one-, two-orthree- place logical\\npredicates. Proper names areassumed tocorrespond toconstants. The\\x02rstargument should alwayscorrespond\\ntothesubject oftheactivesentence, thesecond totheobject (ifthere isone) andthethird totheindirect object\\n(i.e., thebene\\x02ciary ,ifthere isone). Giverepresentations forthefollowing examples:\\n(a)Kim likesSandy\\nAnswer: like(Kim, Sandy)\\n(b)Kim sleeps\\n(c)Sandy adores Kim\\n(d)Kim isadored bySandy (note, thisispassi ve:thebyshould notberepresented)\\n(e)Kim gaveRovertoSandy (thetoisnotrepresented)\\n(f)Kim gaveSandy Rover\\n2.Listthree verbs thatareintransiti veonly,three which aresimple transiti veonly,three which canbeintransiti ve\\nortransiti veandthree which areditransiti ves.\\nThedistinction between intransiti ve,transiti veandditransiti veverbs canbeillustrated byexamples such as:\\nsleep \\x97intransiti ve.Noobject is(generally) possible: *Kim slept theevening .\\nadore \\x97transiti ve.Anobject isoblig atory: *Kim adored.\\ngive\\x97-ditransiti ve.These verbs haveanobject andanindirect object. Kim gave Sandy anapple (orKim gave\\nanapple toSandy ).\\nE.2 Post-lectur e\\n1.Givetheuni\\x02cation ofthefollowing feature structures:\\n(a)\\x14\\nCAT\\x02\\x03\\nAGRpl\\x15\\nuni\\x02ed with\\x14\\nCATVP\\nAGR\\x02\\x03\\x15\\n(b)2\\n66666664MOTHER\\x14\\nCATVP\\nAGR1\\x15\\nDTR1\\x14\\nCATV\\nAGR1\\x15\\nDTR2\\x14\\nCATNP\\nAGR\\x02\\x03\\x153\\n77777775uni\\x02ed with\"\\nDTR1\\x14\\nCATV\\nAGRsg\\x15#\\n(c)\\x14\\nF1\\nG 1\\x15\\nuni\\x02ed with2\\n64F\\x02\\nJa\\x03\\nG\\x14\\nJ\\x02\\x03\\nKb\\x153\\n75\\n76(d)\\x14\\nF1a\\nG 1\\x15\\nuni\\x02ed with\\x02\\nGb\\x03\\n(e)\\x14\\nF1\\nG 1\\x15\\nuni\\x02ed with2\\n64F\\x02\\nJa\\x03\\nG\\x14\\nJb\\nKb\\x153\\n75\\n(f)\\x14\\nF\\x02\\nG 1\\x03\\nH 1\\x15\\nuni\\x02ed with\\x14\\nF1\\nH 1\\x15\\n(g)2\\n4F1\\nG 1\\nH 2\\nJ23\\n5uni\\x02ed with\\x14\\nF1\\nJ1\\x15\\n(h)\\x14\\nF\\x02\\nG 1\\x03\\nH 1\\x15\\nuni\\x02ed with\\x14\\nF2\\nH\\x02\\nJ2\\x03\\x15\\n2.Add case totheinitial FSgrammar inorder topreventsentences such astheycantheyfrom parsing.\\n3.Workthough parses ofthefollowing strings forthesecond FSgrammar ,deciding whether theyparse ornot:\\n(a)\\x02sh\\x02sh\\n(b)theycan\\x02sh\\n(c)it\\x02sh\\n(d)theycan\\n(e)they\\x02shit\\n4.Modify thesecond FSgrammar toallowforverbs which taketwocomplements. Also addalexical entry for\\ngive (just dothevariant which takestwonoun phrases).\\nFLectur e6\\nF.1Pre-lectur e\\nWithout looking atadictionary ,write downbrief de\\x02nitions forasmanysenses asyoucanthink offorthefollowing\\nwords:\\n1.plant\\n2.shower\\n3.bass\\nIfpossible, compare your answers with another student\\' sandwith adictionary .\\nF.2Post-lectur e\\n1.Ifyoudidtheexercise associated with theprevious lecture toaddditransiti veverbs tothegrammar ,amend your\\nmodi\\x02ed grammar sothatitproduces semantic representations.\\n2.Givehypern yms and(ifpossible) hypon yms forthenominal senses ofthefollowing words:\\n(a)horse\\n(b)rice\\n(c)curtain\\n3.List some possible seeds forYarowsky\\'salgorithm thatwould distinguish between thesenses ofshower and\\nbass thatyougaveintheprelecture exercise.\\n77GLectur e7\\nG.1 Pre-lectur e\\nNosuggested exercises.\\nG.2 Post-lectur e\\nWorkthrough theLappin andLeass algorithm with ashort piece ofnaturally occurring text.Ifthere arecases where\\nthealgorithm getstheresults wrong, suggest thesorts ofknowledge thatwould beneeded togivethecorrect answer .\\nHLectur e8\\nH.1 Exer cises (pre-orpost- lectur e)\\nIfyouhaveneverused aspok endialogue system, tryoneout.One example isBritish Airw ays\\x03ight arrivals(0870 551\\n1155 \\x97chargedatstandard national rate). Think ofarealistic taskbefore youphone: forinstance, to\\x02ndarrivaltimes\\nforamorning \\x03ight from Edinb urghtoHeathro w.Another example istheTransco Meter Helpline (0870 6081524 \\x97\\nstandard national rate) which tells customers who their gassupplier is.This system uses averysimple dialogue model\\nbutallowsforinterruptions etc.\\nUse Systran (viahttp://world.altavista.com/ )totranslate some textandinvestig atewhether thetextit\\noutputs isgrammatical andwhether itdeals well with issues discussed inthecourse, such aslexical ambiguity and\\npronoun resolution. Ideally youwould getthehelp ofsomeone who speaks alanguage other than English forthisif\\nyou\\'renotfairly \\x03uent inanother language yourself: thelanguage pairs thatSystran deals with arelisted onthesite.\\n78IAnswers tosome ofthepre-lectur eexercises\\nI.1 Lectur e2\\n1.(a)carries\\ncarry (stem) s(suf\\x02x)\\n(b)running\\nrun(stem) ing(suf\\x02x)\\n(c)uncaring\\nun(pre\\x02x) care (stem) ing(suf\\x02x)\\n(d)intruders\\nintrude (stem) er(suf\\x02x)s(suf\\x02x)\\nNote thatin-isnotarealpre\\x02x here\\n(e)bookshelv es\\nbook (stem) shelf (stem) s(suf\\x02x)\\n(f)reattaches\\nre(pre\\x02x) attach (stem) s(suf\\x02x)\\n(g)anticipated\\nanticipate (stem) ed(suf\\x02x)\\n2.(a)carry\\nAnswer: simple pastcarried ,pastparticiple carried\\n(b)sleep\\nAnswer: simple pastslept ,pastparticiple slept\\n(c)see\\nAnswer: simple pastsaw,pastparticiple seen\\nI.2 Lectur e3\\n1.The/Det big/Adj cat/Noun chased/V erbthe/Det small/Adj dog/Noun into/Prep the/Det barn/Noun.\\n2.Those/Det barns/Noun have/Verbred/Adj roofs/Noun.\\n3.Dogs/Noun often/Adv erbbark/V erbloudly/Adv erb.\\n4.Further/Adj discussion/Noun seems/V erbuseless/Adj.\\n5.Kim/Proper noun did/V erb(aux) not/Adv erb(or Other) like/Verbhim/Pronoun.\\n6.Time/Noun \\x03ies/V erb.\\nTime/V erb\\x03ies/Noun. (theimperati ve!)\\nI.3 Lectur e4\\n1.Thebigcatwith black furchased thedogwhich barked.\\n((The bigcat)npwith (black fur)np)npchased (thedogwhich barked)np\\nThebigcatwith black fur(chased thedogwhich barked)vp\\n2.Three dogs barkedathim. (Three dogs)npbarkedat(him)npThree dogs (bark edathim)vp\\n3.Kim sawthebirdw atcher with thebinoculars.\\nAnalysis 1(thebirdw atcher hasthebinoculars) (Kim)npsaw((the birdw atcher)npwith (thebinoculars) np)np\\nKim (sawthebirdw atcher with thebinoculars) vp\\nAnalysis 2(theseeing waswith thebinoculars) (Kim)npsaw(thebirdw atcher)npwith (thebinoculars) np\\nKim (sawthebirdw atcher with thebinoculars) vp\\n79I.4 Lectur e5\\n1.Kim sleeps\\nsleep(Kim)\\n2.Sandy adores Kim\\nadore(Sandy ,Kim)\\n3.Kim isadored bySandy\\nadore(Sandy ,Kim)\\n4.Kim gaveRovertoSandy\\ngive(Kim, Rover,Sandy)\\n5.Kim gaveSandy Rover\\ngive(Kim, Rover,Sandy)\\nSome examples ofdifferent classes ofverb(obviously youhavealmost certainly come upwith different ones!)\\nsleep, snore, sneeze, cough \\x97intransiti veonly\\nadore, comb, rub\\x97simple transiti veonly eat,wash,shave,dust \\x97transiti veorintransiti ve\\ngive,hand, lend \\x97ditransiti ve\\n80'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Initialize the connection to database:**"
      ],
      "metadata": {
        "id": "i4oIaef7WpWi"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "zFBR5HnZSPmK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cdd281a9-1796-4a89-8c74-4fc9bcf1c2a3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:cassandra.cluster:Downgrading core protocol version from 66 to 65 for 038b5603-5e01-46e8-a03f-bde5ad2c288b-us-east-2.db.astra.datastax.com:29042:7d1e19c9-12a6-4994-98b3-32954b433065. To avoid this, it is best practice to explicitly set Cluster(protocol_version) to the version supported by your cluster. http://datastax.github.io/python-driver/api/cassandra/cluster.html#cassandra.cluster.Cluster.protocol_version\n",
            "WARNING:cassandra.cluster:Downgrading core protocol version from 65 to 5 for 038b5603-5e01-46e8-a03f-bde5ad2c288b-us-east-2.db.astra.datastax.com:29042:7d1e19c9-12a6-4994-98b3-32954b433065. To avoid this, it is best practice to explicitly set Cluster(protocol_version) to the version supported by your cluster. http://datastax.github.io/python-driver/api/cassandra/cluster.html#cassandra.cluster.Cluster.protocol_version\n",
            "ERROR:cassandra.connection:Closing connection <AsyncoreConnection(138722702811408) 038b5603-5e01-46e8-a03f-bde5ad2c288b-us-east-2.db.astra.datastax.com:29042:7d1e19c9-12a6-4994-98b3-32954b433065> due to protocol error: Error from server: code=000a [Protocol error] message=\"Beta version of the protocol used (5/v5-beta), but USE_BETA flag is unset\"\n",
            "WARNING:cassandra.cluster:Downgrading core protocol version from 5 to 4 for 038b5603-5e01-46e8-a03f-bde5ad2c288b-us-east-2.db.astra.datastax.com:29042:7d1e19c9-12a6-4994-98b3-32954b433065. To avoid this, it is best practice to explicitly set Cluster(protocol_version) to the version supported by your cluster. http://datastax.github.io/python-driver/api/cassandra/cluster.html#cassandra.cluster.Cluster.protocol_version\n"
          ]
        }
      ],
      "source": [
        "cassio.init(token=ASTRA_DB_APPLICATION_TOKEN, database_id=ASTRA_DB_ID)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ex7NxZYb4Rps"
      },
      "source": [
        "**Create the LangChain embedding and LLM objects for later usage:**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "llm = OpenAI(openai_api_key=OPENAI_API_KEY)\n",
        "embedding = OpenAIEmbeddings(openai_api_key=OPENAI_API_KEY)"
      ],
      "metadata": {
        "id": "WsPJ-j9L_Gbe"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Initializing an instance of the Cassandra class with certain parameters**"
      ],
      "metadata": {
        "id": "X5f-KsctXc3y"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "bg9VAk4USQvU"
      },
      "outputs": [],
      "source": [
        "astra_vs = Cassandra(\n",
        "    embedding=embedding,\n",
        "    table_name=\"astra_langchain_qa\",\n",
        "    session=None, # If None, the Cassandra class may create a new session.\n",
        "    keyspace=None, # defines data replication on nodes. If None, the default keyspace may be used.\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# split the text using Character Text Split to not increase token size\n",
        "def token_length_function(merged_raw_text):\n",
        "    # Use NLTK to tokenize the text into words\n",
        "    tokens = word_tokenize(merged_raw_text)\n",
        "    # Return the count of tokens (words)\n",
        "    return len(tokens)\n",
        "text_splitter = CharacterTextSplitter(\n",
        "    separator = \"\\n\",\n",
        "    chunk_size = 400,\n",
        "    chunk_overlap  = 50, # number of characters that can overlap between adjacent chunks\n",
        "    length_function = token_length_function # we can also use the built in len function\n",
        ")\n",
        "texts = text_splitter.split_text(merged_raw_text)"
      ],
      "metadata": {
        "id": "9FMAhKr77AVO"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "texts[:100]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k8BDHAyT7Gjr",
        "outputId": "3f8f91d2-3862-4fa5-fcda-74c9057e5cb9"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['1Natural Language Processing\\nCS 6320  \\nLecture 1\\nIntroduction to NLP\\nInstructor: Sanda HarabagiuDefinition\\n•NLP is concerned with the computational techniques used for \\nprocessing human language. It creates and implements computer \\nmodels  for the purpose of performing various natural language tasks. \\n•These tasks include :\\n•Mundane applications , e.g. word counting, spell checking, automatic \\nhyphenation\\n•Cutting edge applications , e.g. automated question answering on the Web, \\nbuilding NL interfaces to databases, machine translation,  and others.\\n•What distinguished these applications from other data processing \\napplications is their use of knowledge of language .\\n•NLP is playing an increasing role in curbing the information  explosion \\non Internet and corporate America.  AI vs. NLP\\n•People refer to many AI techniques – like Chat GPT, \\nwhich are in fact novel NLP methods using GPT3 in an \\ninteractive mode.\\n•GPT4  and GPT3  are Large  Language  Models (LLMs)\\n•LLMs have recently revolutionized NLP\\n•BERT  was another  important  NLP milestone\\n•NLP nowadays uses Deep Learning techniques\\nBut, the understanding  of how language  works, and \\nwhat aspects of natural language processing\\nwe need  to be aware  of still requires  the comprehension \\nof classical NLP technique sRelated areas\\n•NLP is a difficult, and largely unsolved problem. One reason for this \\nis its multidisciplinary  nature: \\n•Linguistics  : How  words, phrases, and  sentences are \\nformed.  \\n•Psycholinguistics  : How people understand and \\ncommunicate  using human language. \\n•Cognitive Modeling:  Deals with models and   computational \\naspects of NL ( e.g. algorithms). Related areas\\n•Philosophy : relates to the semantics of language;  notion of  \\nmeaning, how words identify objects. NLP requires  considerable \\nknowledge about the world. \\n•Computer science :  model formulation and implementation  using \\nmodern methods.  \\n•Artificial intelligence : issues related to knowledge representation \\nand reasoning.\\n•Statistics:  many NLP problems are modeled using probabilistic \\nmodels.\\n•Machine learning:  automatic learning of rules and procedures \\nbased on lexical, syntactic and semantic features.\\n•NL Engineering : implementation of large, realistic systems.  \\nModern software development methods play an important role.Applications of NLP\\n•Text - based applications : \\n•Finding documents on certain topics (document classification)  \\n•Information extraction: extract information related events, relations, \\nconcepts \\n•Complete understanding of texts:  requires a deep structure analysis,',\n",
              " \"Modern software development methods play an important role.Applications of NLP\\n•Text - based applications : \\n•Finding documents on certain topics (document classification)  \\n•Information extraction: extract information related events, relations, \\nconcepts \\n•Complete understanding of texts:  requires a deep structure analysis, \\n•Reading comprehension\\n•Translation from a language to another,  \\n•Summarization,  \\n•Knowledge acquisition,\\n•Question -Answering     \\n•Dialogue - based applications  (involve human - machine \\ncommunication): \\n•Conversational Agents\\n•Tutoring systems \\n•Problem solving.   \\n•Speech processingBasic levels of language processing 1/2\\n1.Phonetic   - how words are related to the sounds that realize \\nthem. Essential for speech processing.  \\n2.Morphological Knowledge  - how words are constructed : e.g  \\nfriend, friendly, unfriendly, friendliness. \\n3.Syntactic Knowledge  - how words can be put together to form  \\ncorrect sentences, and the role each word plays in the sentence. \\ne.g.:\\n          John ate the cake.  \\n1.Semantic Knowledge  - Words and sentence meaning: \\n          They saw a log.\\n          They saw a log yesterday.\\n          He saws a log.Basic levels of language processing 2/2\\n5.Pragmatic  Knowledge - how sentences are used in different \\nsituations(or contexts).  \\n          Mary grabbed her umbrella.   \\n     a)  It is a cloudy day.  \\n     b)  She was afraid of dogs.\\n5.Discourse Knowledge  - how the meaning of words and \\nsentences is effected by the   proceeding sentences; pronoun  \\nresolution.         \\n          John gave his bike to Bill. \\n  He didn't care much for it anyway.\\n5.World Knowledge   - the vast amount of  knowledge necessary \\nto  understand texts. Used to identify beliefs, goals. \\n6.Language generation  - have the machine generate coherent \\ntext  or speech.  Examples of NLP difficulties \\n1.Syntactic ambiguity - when a word has more than one \\npart of  speech:    \\n  Example: Rice flies like sand . \\n Note that these syntactic ambiguities lead to different parse \\nstructures. Sometimes it is possible to use grammar rules \\n(like subject verb  agreement) to disambiguate :   \\n        Flying planes are dangerous.      \\n        Flying planes is dangerous.     \\n2. Semantic ambiguity - when a word has more than one \\npossible  meaning  (or sense):      \\n  John killed  the wolf.       \\n  John killed the project.       \\n  John killed that bottle of wine.\",\n",
              " 'Flying planes are dangerous.      \\n        Flying planes is dangerous.     \\n2. Semantic ambiguity - when a word has more than one \\npossible  meaning  (or sense):      \\n  John killed  the wolf.       \\n  John killed the project.       \\n  John killed that bottle of wine.        \\n  John killed Jane.  (at tennis , or murdered her)More Examples of NLP difficulties \\n3. Structural ambiguity - when a sentence has more than one \\npossible parse structure; e.g. prepositional attachment .\\n•Example:\\n  John saw the boy in the park with a telescope .  \\nAnother syntactic parse\\nAdditional NLP difficulties \\nAmbiguities of a sentence : \\n \\n Example:      \\n I made her duck.     \\n \\n Possible interpretations:   \\n \\n1.I cooked waterfowl for her.    \\n2.I cooked waterfowl belonging to her.    \\n3.I created the (plaster ?) duck she owns.    \\n4.I caused her to quickly lower her head or body\\n5.I wave my magic wand and turned her into \\nundifferentiated waterfowl.State of the art in NLP Research 1/2\\n•NLP Publications  : \\n•Association of Computational Linguistics (ACL):\\n•Conferences: ACL, HLT -NAACL, EACL, EMNLP  \\n•Journals: Computational Linguistics, TACL\\n• AAAI - every year proceedings.\\n•IJCAI - every year proceedings.\\n•The Web Conference\\n•On the WWWeb: http://aclweb.org\\n•Natural Language Engineering  (journal).•Machine Readable Dictionaries  (MRD)  WordNet, \\nLDOCE.\\n•Large corpora :  \\n•Penn Treebank —contains   2 -3 months of  Wall Street \\nJournal articles (~ .5 million words of English, POS  \\ntagged and parsed),  \\n•Brown corpus,\\n•SemCor.\\n•Google GiGaword\\n•Neural Language ProcessingState of the art in NLP Research 2/2Neural Language Learning\\n•Nowadays, it is the “de facto” way of doing NLP\\n➢BERT: Pre -training of Deep Bidirectional Transformers for \\nLanguage Understanding\\n❑https://www.aclweb.org/anthology/N19 -1423.pdf\\noBERT is designed to pretrain deep bidirectional \\nrepresentations from unlabeled text by jointly conditioning \\non both left and right context in all layers. As a result, the \\npre-trained BERT model can be finetuned with just one \\nadditional output layer to create state -of-the-art models for \\na wide range of tasks, such as question answering and \\nlanguage inference, without substantial task -specific \\narchitecture modifications.Evaluation of NLP systems\\n➢The General Language Understanding Evaluation \\n(GLUE) benchmark (Wang et al., 2018a) is a',\n",
              " 'additional output layer to create state -of-the-art models for \\na wide range of tasks, such as question answering and \\nlanguage inference, without substantial task -specific \\narchitecture modifications.Evaluation of NLP systems\\n➢The General Language Understanding Evaluation \\n(GLUE) benchmark (Wang et al., 2018a) is a \\ncollection of diverse natural language understanding \\ntasks.\\nohttps://www.aclweb.org/anthology/W18 -5446.pdf\\noTAKE -HOME LESSON: In order to build neural \\nlanguage processing systems, we rely on vast \\nannotated dataset.\\n➢It is IMPOSSIBLE to build NLP systems without \\nlooking and deeply understanding the texts.\\n➢ANNOTATION experience is KEY!!!\\n ========== SPACE BETWEEN FILES ==========  ========== SPACE BETWEEN FILES ========== \\nNatural Language Processing\\n2004, 8Lectures\\nAnn Copestak e(aac@cl.cam.ac.uk )\\nhttp://www .cl.cam.ac.uk/users/aac/\\nCopyright c\\rAnn Copestak e,2003\\x962004\\nLectur eSynopsis\\nAims\\nThis course aims tointroduce thefundamental techniques ofnatural language processing andtodevelop anunder -\\nstanding ofthelimits ofthose techniques. Itaims tointroduce some current research issues, andtoevaluate some\\ncurrent andpotential applications.\\n\\x0fIntroduction. Brief history ofNLP research, current applications, generic NLP system architecture, knowledge-\\nbased versusprobabilistic approaches.\\n\\x0fFinite-state techniques. In\\x03ectional andderivational morphology ,\\x02nite-state automata inNLP,\\x02nite-state\\ntransducers.\\n\\x0fPrediction andpart-of-speech tagging .Corpora, simple N-grams, wordprediction, stochastic tagging, evalu-\\nating system performance.\\n\\x0fParsing andgeneration. Generati vegrammar ,conte xt-free grammars, parsing andgeneration with conte xt-free\\ngrammars, weights andprobabilities.\\n\\x0fParsing with constraint-based grammars. Constraint-based grammar ,uni\\x02cation.\\n\\x0fCompositional andlexical semantics. Simple compositional semantics inconstraint-based grammar .Semantic\\nrelations, WordNet, wordsenses, wordsense disambiguation.\\n\\x0fDiscourse anddialogue. Anaphora resolution, discourse relations.\\n\\x0fApplications. Machine translation, email response, spok endialogue systems.\\nObjecti ves\\nAttheendofthecourse students should\\n\\x0fbeable todescribe thearchitecture ofandbasic design forageneric NLP system \\x93shell\\x94\\n\\x0fbeable todiscuss thecurrent andlikelyfuture performance ofseveral NLP applications, such asmachine\\ntranslation andemail response\\n\\x0fbeable todescribe brie\\x03y afundamental technique forprocessing language forseveralsubtasks, such asmor-\\nphological analysis, parsing, wordsense disambiguation etc.\\n\\x0funderstand howthese techniques drawonandrelate toother areas of(theoretical) computer science, such as\\nformal language theory ,formal semantics ofprogramming languages, ortheorem proving\\n1Overview\\nNLP isalargeandmultidisciplinary \\x02eld, sothiscourse canonly provide averygeneral introduction. The \\x02rst\\nlecture isdesigned togiveanovervie wofthemain subareas andaverybrief idea ofthemain applications and',\n",
              " \"\\x0funderstand howthese techniques drawonandrelate toother areas of(theoretical) computer science, such as\\nformal language theory ,formal semantics ofprogramming languages, ortheorem proving\\n1Overview\\nNLP isalargeandmultidisciplinary \\x02eld, sothiscourse canonly provide averygeneral introduction. The \\x02rst\\nlecture isdesigned togiveanovervie wofthemain subareas andaverybrief idea ofthemain applications and\\nthemethodologies which havebeen emplo yed. Thehistory ofNLP isbrie\\x03y discussed asawayofputting thisinto\\nperspecti ve.Thenextsixlectures describe some ofthemain subareas inmore detail. Theorganisation isroughly based\\nonincreased `depth' ofprocessing, starting with relati velysurface-oriented techniques andprogressing toconsidering\\nmeaning ofsentences andmeaning ofutterances inconte xt.Most lectures willstart offbyconsidering thesubarea as\\nawhole andthen goontodescribe oneormore sample algorithms which tackle particular problems. Thealgorithms\\nhavebeen chosen because theyarerelati velystraightforw ardtodescribe andbecause theyillustrate aspeci\\x02c technique\\nwhich hasbeen showntobeuseful, buttheideaistoexemplify anapproach, nottogiveadetailed survey(which would\\nbeimpossible inthetime available). (Lecture 5isabitdifferent inthatitconcentrates onadata structure instead of\\nanalgorithm.) The\\x02nal lecture brings thepreceding material together inorder todescribe thestate oftheartinthree\\nsample applications.\\nThere arevarious themes running throughout thelectures. One theme istheconnection tolinguistics andthetension\\nthatsometimes exists between thepredominant viewintheoretical linguistics andtheapproaches adopted within NLP.\\nAsome what related theme isthedistinction between knowledge-based andprobabilistic approaches. Evaluation will\\nbediscussed intheconte xtofthedifferent algorithms.\\nBecause NLP issuch alargearea, there aremanytopics that aren' ttouched onatallinthese lectures. Speech\\nrecognition andspeech synthesis isalmost totally ignored. Information retrie valandinformation extraction arethe\\ntopic ofaseparate course givenbySimone Teufel, forwhich thiscourse isaprerequisite.\\nFeedback onthehandout, listsoftypos etc,would begreatly appreciated.\\nRecommended Reading\\nRecommended Book:\\nJurafsk y,Daniel andJames Martin, Speec handLangua geProcessing ,Prentice-Hall, 2000 (referenced asJ&M through-\\noutthishandout).\\nBackground:\\nThese books areabout linguistics rather thatNLP/computational linguistics. Theyarenotnecessary tounderstand the\\ncourse, butshould givereaders anidea about some oftheproperties ofhuman languages thatmakeNLP interesting\\nandchallenging, without being technical.\\nPinker,S.,TheLangua geInstinct ,Penguin, 1994.\\nThis isathought-pro voking andsometimes contro versial `popular' introduction tolinguistics.\\nMatthe ws,Peter ,Linguistics: avery short introduction ,OUP ,2003.\\nThetitleisaccurate ...\\nBackground/reference:\\nTheInternet Grammar ofEnglish ,http://www .ucl.ac.uk/internet-grammar/home.htm\\nSyntactic concepts andterminology .\\nStudy andSuper vision Guide\\nThe handouts andlectures should contain enough information toenable students toadequately answer theexam\\nquestions, butthehandout isnotintended tosubstitute foratextbook. Inmost cases, J&M gointo aconsiderable\",\n",
              " \"Thetitleisaccurate ...\\nBackground/reference:\\nTheInternet Grammar ofEnglish ,http://www .ucl.ac.uk/internet-grammar/home.htm\\nSyntactic concepts andterminology .\\nStudy andSuper vision Guide\\nThe handouts andlectures should contain enough information toenable students toadequately answer theexam\\nquestions, butthehandout isnotintended tosubstitute foratextbook. Inmost cases, J&M gointo aconsiderable\\namount offurther detail: rather than putlotsofsuggestions forfurther reading inthehandout, ingeneral Ihave\\nassumed thatstudents willlook atJ&M, andthen followupthereferences inthere iftheyareinterested. Thenotes at\\ntheendofeach lecture givedetails ofthesections ofJ&M thatarerelevantanddetails ofanydiscrepancies with these\\nnotes.\\nSupervisors ought tofamiliarise themselv eswith therelevantparts ofJurafsk yandMartin (seenotes attheendofeach\\nlecture). However,good students should \\x02nd itquite easy tocome upwith questions thatthesupervisors (and the\\n2lecturer) can'tanswer! Language islikethat...\\nGenerally I'mtaking arather informal/e xample-based approach toconcepts such as\\x02nite-state automata, conte xt-free\\ngrammars etc. PartIIstudents should havealready gottheformal background thatenables them tounderstand the\\napplication toNLP.Diploma andPartII(General) students may nothavecovered allthese concepts before, butthe\\nexpectation isthattheexamples arestraightforw ardenough sothatthiswon'tmatter toomuch.\\nThis course inevitably assumes some verybasic linguistic knowledge, such asthedistinction between themajor parts\\nofspeech. Itintroduces some linguistic concepts thatwon'tbefamiliar toallstudents: since I'llhavetogothrough\\nthese quickly ,reading the\\x02rstfewchapters ofanintroductory linguistics textbook may help students understand the\\nmaterial. The idea istointroduce justenough linguistics tomotivatetheapproaches used within NLP rather than\\ntoteach thelinguistics foritsownsake.Attheendofthishandout, there aresome mini-e xercises tohelp students\\nunderstand theconcepts: itwould beveryuseful ifthese were attempted before thelectures asindicated. There are\\nalsosome suggested post-lecture exercises.\\nExam questions won'trely onstudents remembering thedetails ofanyspeci\\x02c linguistic phenomenon. Asfaras\\npossible, exam questions will besuitable forpeople who speak English asasecond language. Forinstance, ifa\\nquestion relied onknowledge oftheambiguity ofaparticular English word, agloss oftherelevantsenses would be\\ngiven.\\nOfcourse, I'llbehapp ytotryandanswer questions about thecourse ormore general NLP questions, preferably by\\nemail.\\n31Lectur e1:Introduction toNLP\\nThe aimofthislecture istogivestudents some idea oftheobjecti vesofNLP.The main subareas ofNLP will be\\nintroduced, especially those which willbediscussed inmore detail intherestofthecourse. There willbeapreliminary\\ndiscussion ofthemain problems involvedinlanguage processing bymeans ofexamples takenfrom NLP applications.\\nThis lecture also introduces some methodological distinctions andputs theapplications andmethodology intosome\\nhistorical conte xt.\\n1.1 What isNLP?\\nNatural language processing (NLP) canbede\\x02ned astheautomatic (orsemi-automatic) processing ofhuman language.\\nTheterm `NLP' issometimes used rather more narro wlythan that, often excluding information retrie valandsometimes\",\n",
              " \"historical conte xt.\\n1.1 What isNLP?\\nNatural language processing (NLP) canbede\\x02ned astheautomatic (orsemi-automatic) processing ofhuman language.\\nTheterm `NLP' issometimes used rather more narro wlythan that, often excluding information retrie valandsometimes\\nevenexcluding machine translation. NLP issometimes contrasted with `computational linguistics', with NLP being\\nthought ofasmore applied. Nowadays, alternati veterms areoften preferred, like`Language Technology' or`Language\\nEngineering'. Language isoften used incontrast with speech (e.g., Speech andLanguage Technology). ButI'mgoing\\ntosimply refer toNLP andusetheterm broadly .\\nNLP isessentially multidisciplinary: itisclosely related tolinguistics (although theextent towhich NLP overtly draws\\nonlinguistic theory varies considerably). Italsohaslinks toresearch incogniti vescience, psychology ,philosoph yand\\nmaths (especially logic). Within CS,itrelates toformal language theory ,compiler techniques, theorem proving, ma-\\nchine learning andhuman-computer interaction. Ofcourse itisalsorelated toAI,though nowadays it'snotgenerally\\nthought ofaspartofAI.\\n1.2 Some linguistic terminology\\nThecourse isorganised sothatthere aresixlectures corresponding todifferent NLP subareas, moving from relati vely\\n`shallo w'processing toareas which involvemeaning andconnections with therealworld. These subareas loosely\\ncorrespond tosome ofthestandard subdi visions oflinguistics:\\n1.Morphology: thestructure ofwords. Forinstance, unusually canbethought ofascomposed ofapre\\x02x un-,a\\nstem usual ,andanaf\\x02x-ly.composed iscompose plus thein\\x03ectional af\\x02x-ed:aspelling rulemeans weend\\nupwith composed rather than composeed .Morphology willbediscussed inlecture 2.\\n2.Syntax: thewaywords areused toform phrases. e.g., itispart ofEnglish syntax thatadeterminer such as\\nthewillcome before anoun, andalso thatdeterminers areoblig atory with certain singular nouns. Formal and\\ncomputational aspects ofsyntax willbediscussed inlectures 3,4and5.\\n3.Semantics. Compositional semantics istheconstruction ofmeaning (generally expressed aslogic) based on\\nsyntax. This iscontrasted tolexical semantics, i.e.,themeaning ofindividual words. Compositional andlexical\\nsemantics isdiscussed inlecture 6.\\n4.Pragmatics: meaning inconte xt.This will come into lecture 7,although linguistics andNLP generally have\\nverydifferent perspecti veshere.\\n1.3 Whyislanguage processing dif\\x02cult?\\nConsider trying tobuildasystem thatwould answer email sentbycustomers toaretailer selling laptops andaccessories\\nviatheInternet. This might beexpected tohandle queries such asthefollowing:\\n\\x0fHasmyorder number 4291 been shipped yet?\\n\\x0fIsFD5 compatible with a505G?\\n\\x0fWhat isthespeed ofthe505G?\\n4Assume thequery istobeevaluated against adatabase containing product andorder information, with relations such\\nasthefollowing:\\nORDER\\nOrder number Date ordered Date shipped\\n4290 2/2/02 2/2/02\\n4291 2/2/02 2/2/02\\n4292 2/2/02\",\n",
              " \"\\x0fHasmyorder number 4291 been shipped yet?\\n\\x0fIsFD5 compatible with a505G?\\n\\x0fWhat isthespeed ofthe505G?\\n4Assume thequery istobeevaluated against adatabase containing product andorder information, with relations such\\nasthefollowing:\\nORDER\\nOrder number Date ordered Date shipped\\n4290 2/2/02 2/2/02\\n4291 2/2/02 2/2/02\\n4292 2/2/02\\nUSER: Hasmyorder number 4291 been shipped yet?\\nDBQUER Y:order(number=4291,date shipped=?)\\nRESPONSE TOUSER: Order number 4291 wasshipped on2/2/02\\nItmight look quite easy towrite patterns forthese queries, butverysimilar strings canmean verydifferent things,\\nwhile verydifferent strings canmean much thesame thing. 1and2belowlook verysimilar butmean something\\ncompletely different, while 2and3look verydifferent butmean much thesame thing.\\n1.Howfastisthe505G?\\n2.Howfastwillmy505G arrive?\\n3.Please tellmewhen Icanexpect the505G Iordered.\\nWhile some tasks inNLP canbedone adequately without having anysortofaccount ofmeaning, others require that\\nwecanconstruct detailed representations which willre\\x03ect theunderlying meaning rather than thesuper\\x02cial string.\\nInfact,innatural languages (asopposed toprogramming languages), ambiguity isubiquitous, soexactly thesame\\nstring might mean different things. Forinstance inthequery:\\nDoyousellSonylaptops anddisk drives?\\ntheuser may ormay notbeasking about Sonydisk drives.This particular ambiguity may berepresented bydifferent\\nbrack etings:\\nDoyousell(Sonylaptops) and(disk drives)?\\nDoyousell(Sony(laptops anddisk drives))?\\nWe'llseelotsofexamples ofdifferent types ofambiguity inthese lectures.\\nOften humans haveknowledge oftheworld which resolv esapossible ambiguity ,probably without thespeak eror\\nhearer evenbeing awarethatthere isapotential ambiguity .1Buthand-coding such knowledge inNLP applications\\nhasturned outtobeimpossibly hard todoformore than verylimited domains: theterm AI-complete issometimes\\nused (byanalogy toNP-complete), meaning thatwe'dhavetosolvetheentire problem ofrepresenting theworld\\nandacquiring world knowledge.2Theterm AI-complete isintended jokingly ,butconveyswhat' sprobably themost\\nimportant guiding principle incurrent NLP: we'relooking forapplications which don'trequire AI-complete solutions:\\ni.e.,ones where wecanworkwith verylimited domains orapproximate fullworld knowledge byrelati velysimple\\ntechniques.\\n1.4 Some NLP applications\\nThefollowing listisnotcomplete, butuseful systems havebeen builtfor:\\n1I'llusehearergenerally tomean theperson who isontherecei ving end, regardless ofthemodality ofthelanguage transmission: i.e.,regardless\\nofwhether it'sspok en,signed orwritten. Similarly ,I'llusespeak erfortheperson generating thespeech, textetcandutterance tomean thespeech\\nortextitself. This isthestandard linguistic terminology ,which recognises thatspok enlanguage isprimary andtextisalater development.\\n2Inthiscourse, Iwillusedomain tomean some circumscribed body ofknowledge: forinstance, information about laptop orders constitutes a\\nlimited domain.\\n5\\x0fspelling andgrammar checking\\n\\x0foptical character recognition (OCR)\\n\\x0fscreen readers forblind andpartially sighted users\",\n",
              " \"ortextitself. This isthestandard linguistic terminology ,which recognises thatspok enlanguage isprimary andtextisalater development.\\n2Inthiscourse, Iwillusedomain tomean some circumscribed body ofknowledge: forinstance, information about laptop orders constitutes a\\nlimited domain.\\n5\\x0fspelling andgrammar checking\\n\\x0foptical character recognition (OCR)\\n\\x0fscreen readers forblind andpartially sighted users\\n\\x0faugmentati veandalternati vecommunication (i.e., systems toaidpeople who havedif\\x02culty communicating\\nbecause ofdisability)\\n\\x0fmachine aided translation (i.e., systems which help ahuman translator ,e.g., bystoring translations ofphrases\\nandproviding online dictionaries integrated with wordprocessors, etc)\\n\\x0flexicographers' tools\\n\\x0finformation retrie val\\n\\x0fdocument classi\\x02cation (\\x02ltering, routing)\\n\\x0fdocument clustering\\n\\x0finformation extraction\\n\\x0fquestion answering\\n\\x0fsummarization\\n\\x0ftextsegmentation\\n\\x0fexam marking\\n\\x0freport generation (possibly multilingual)\\n\\x0fmachine translation\\n\\x0fnatural language interf aces todatabases\\n\\x0femail understanding\\n\\x0fdialogue systems\\nSeveralofthese applications arediscussed brie\\x03y below.Roughly speaking, theyareordered according tothecom-\\nplexityofthelanguage technology required. Theapplications towards thetopofthelistcanbeseen simply asaidsto\\nhuman users, while those atthebottom arepercei vedasagents intheir ownright. Perfect performance onanyofthese\\napplications would beAI-complete, butperfection isn'tnecessary forutility: inmanycases, useful versions ofthese\\napplications hadbeen builtbythelate70s. Commercial success hasoften been harder toachie ve,however.\\n1.5 Spelling andgrammar checking\\nAllspelling check erscan\\x03agwords which aren' tinadictionary .\\n(1) *Theneccesary steps areobvious.\\n(2) Thenecessary steps areobvious.\\nIftheuser canexpand thedictionary ,orifthelanguage hascomple xproducti vemorphology (seex2.1), then asimple\\nlistofwords isn'tenough todothisandsome morphological processing isneeded.3\\nMore subtle cases involvewords which arecorrect inisolation, butnotinconte xt.Syntax could sortsome ofthese\\ncases out.Forinstance, possessi veitsgenerally hastobeimmediately followed byanoun orbyoneormore adjecti ves\\nwhich areimmediately infront ofanoun:\\n3Note theuseof*(`star') above:thisnotation isused inlinguistics toindicate asentence which isjudged (bytheauthor ,atleast) tobeincorrect.\\n?isgenerally used forasentence which isquestionable, oratleast doesn' thavetheintended interpretation. #isused forapragmatically anomalous\\nsentence.\\n6(3) *Itsafairexchange.\\n(4) It'safairexchange.\\n(5) *Thedogcame intotheroom, it'stailwagging.\\n(6) Thedogcame intotheroom, itstailwagging.\\nBut, itsometimes isn'tlocally clear what theconte xtis:e.g.fairisambiguous between anoun andanadjecti ve.\\n(7) *`Itsfair', wasallKim said.\\n(8) `It'sfair', wasallKim said.\\n(9) *Everyvillage hasanannual fair,except Kimbolton: it'sfairisheld twice ayear.\",\n",
              " \"(7) *`Itsfair', wasallKim said.\\n(8) `It'sfair', wasallKim said.\\n(9) *Everyvillage hasanannual fair,except Kimbolton: it'sfairisheld twice ayear.\\n(10) Everyvillage hasanannual fair,except Kimbolton: itsfairisheld twice ayear.\\nThemost elaborate spelling/grammar check erscangetsome ofthese cases right, butnone areanywhere near perfect.\\nSpelling correction canrequire aform ofwordsense disambiguation :\\n(11) #Thetree'sbowswere heavywith snow.\\n(12) Thetree'sboughs were heavywith snow.\\nGetting thisright requires anassociation between treeandbough .Inthepast, attempts might havebeen made to\\nhand-code thisinterms ofgeneral knowledge oftrees andtheir parts. Howeverthissortofhand-coding isnotsuitable\\nforapplications thatworkonunbounded domains. These days machine learning techniques aregenerally used to\\nderivewordassociations from corpora:4thiscanbeseen asasubstitute forthefully detailed world knowledge, but\\nmay actually beamore realistic model ofhowhumans dowordsense disambiguation. However,commercial systems\\ndon't(yet) dothissystematically .\\nSimple subject verbagreement canbecheck edautomatically:5\\n(13) Myfriends likepizza.\\n(14) Myfriend likespizza.\\n(15) *Myfriends likespizza.\\n(16) *Myfriend likepizza.\\n(17) Myfriends were unhapp y.\\n(18) *Myfriend were unhapp y.\\nButthisisn'tasstraightforw ardasitmay seem:\\n(19) Anumber ofmyfriends were unhapp y.\\n(20) Thenumber ofmyfriends who were unhapp ywasamazing.\\n(21) Myfamily were unhapp y.\\nWhether thelastexample isgrammatical ornotdepends onyour dialect ofEnglish: itisgrammatical formost British\\nEnglish speak ers,butnotformanyAmericans.\\nChecking punctuation canbehard (evenAI-complete):\\nBBC NewsOnline, 3October ,2001\\n4Acorpus isabody oftextthathasbeen collected forsome purpose, seex3.1.\\n5InEnglish, thesubject ofasentence isgenerally anoun phrase which comes before theverb,incontrast totheobject, which followstheverb.\\n7Students atCambridge University ,who come from lessaf\\x03uent backgrounds, arebeing offered upto\\n1,000 ayear under abursary scheme.\\nThis sentence contains anon-r estrictive relative clause :who come fromlessaf\\x03uent backgrounds .This isaform\\nofparenthetical comment. The sentence implies thatmost/all students atCambridge come from lessaf\\x03uent back-\\ngrounds. What thereporter probably meant wasarestricti verelati ve,which should nothavecommas round it:\\nStudents atCambridge University who come from lessaf\\x03uent backgrounds arebeing offered upto1,000\\nayear under abursary scheme.\\nArestricti verelati veisatype ofmodi\\x02er :thatis,itfurther speci\\x02es theentities under discussion. Thus itrefers toa\\nsubset ofthestudents.\\n1.6 Information retrie val,information extraction andquestion answering\",\n",
              " \"Students atCambridge University who come from lessaf\\x03uent backgrounds arebeing offered upto1,000\\nayear under abursary scheme.\\nArestricti verelati veisatype ofmodi\\x02er :thatis,itfurther speci\\x02es theentities under discussion. Thus itrefers toa\\nsubset ofthestudents.\\n1.6 Information retrie val,information extraction andquestion answering\\nInformation retrie valinvolvesreturning asetofdocuments inresponse toauser query: Internet search engines area\\nform ofIR.However,onechange from classical IRisthatInternet search nowuses techniques thatrank documents\\naccording tohowmanylinks there aretothem (e.g., Google' sPageRank) aswell asthepresence ofsearch terms.\\nInformation extraction involvestrying todisco verspeci\\x02c information from asetofdocuments. The information\\nrequired canbedescribed asatemplate. Forinstance, forcompan yjoint ventures, thetemplate might haveslots for\\nthecompanies, thedates, theproducts, theamount ofmone yinvolved.Theslot\\x02llers aregenerally strings.\\nQuestion answering attempts to\\x02ndaspeci\\x02c answer toaspeci\\x02c question from asetofdocuments, oratleast ashort\\npiece oftextthatcontains theanswer .\\n(22) What isthecapital ofFrance?\\nParishasbeen theFrench capital formanycenturies.\\nThere aresome question-answering systems ontheWeb,butmost useverybasic techniques. Forinstance, AskJeeves\\nrelies onafairly largestaffofpeople who search theweb to\\x02ndpages which areanswers topotential questions. The\\nsystem performs verylimited manipulation ontheinput tomap toaknownquestion. Thesame basic technique isused\\ninmanyonline help systems.\\n1.7 Machine translation\\nMTworkstarted intheUSintheearly \\x02fties, concentrating onRussian toEnglish. Aprototype system waspublicly\\ndemonstrated in1954 (remember thatthe\\x02rstelectronic computer hadonly been builtafewyears before that). MT\\nfunding gotdrastically cutintheUSinthemid-60s andceased tobeacademically respectable insome places, but\\nSystran wasproviding useful translations bythelate60s. Systran isstillgoing (updating itovertheyears isanamazing\\nfeatofsoftw areengineering): Systran nowpowers AltaV ista'sBabelFish\\nhttp://world.altavista.com/\\nandmanyother translation services ontheweb.\\nUntil the80s, theutility ofgeneral purpose MTsystems wasseverely limited bythefactthattextwasnotavailable in\\nelectronic form: Systran used teams ofskilled typists toinput Russian documents.\\nSystran andsimilar systems arenotasubstitute forhuman translation: theyareuseful because theyallowpeople to\\ngetanidea ofwhat adocument isabout, andmaybe decide whether itisinteresting enough togettranslated properly .\\nThis ismuch more relevantnowthatdocuments etcareavailable ontheWeb.Badtranslation isalso, apparently ,good\\nenough forchatrooms.\\nSpok enlanguage translation isviable forlimited domains: research systems include Verbmobil, SLTandCSTAR.\\n81.8 Natural language interfaces anddialogue systems\\nNatural language interf aces were the`classic' NLP problem inthe70sand80s. LUN ARistheclassic example of\\nanatural language interf acetoadatabase (NLID): itsdatabase concerned lunar rock samples brought back from the\\nApollo missions. LUN ARisdescribed byWoods (1978) (butnote most oftheworkwasdone severalyears earlier): it\\nwascapable oftranslating elaborate natural language expressions intodatabase queries.\",\n",
              " \"anatural language interf acetoadatabase (NLID): itsdatabase concerned lunar rock samples brought back from the\\nApollo missions. LUN ARisdescribed byWoods (1978) (butnote most oftheworkwasdone severalyears earlier): it\\nwascapable oftranslating elaborate natural language expressions intodatabase queries.\\nSHRDLU (Winograd, 1973) wasasystem capable ofparticipating inadialogue about amicro world(theblocks world)\\nandmanipulating thisworldaccording tocommands issued inEnglish bytheuser.SHRDLU hadabigimpact onthe\\nperception ofNLP atthetime since itseemed toshowthat computers could actually `understand' language: the\\nimpossibility ofscaling upfrom themicro worldwasnotrealised.\\nLUN ARandSHRDLU both exploited thelimitations ofoneparticular domain tomakethenatural language under -\\nstanding problem tractable, particularly with respect toambiguity .Totakeatrivialexample, ifyouknowyour database\\nisabout lunar rock, youdon'tneed toconsider themusic ormovement senses ofrockwhen you'reanalysing aquery .\\nThere havebeen manyadvances inNLP since these systems were built: systems havebecome much easier tobuild,\\nandsome what easier touse,buttheystillhaven'tbecome ubiquitous. Natural Language interf aces todatabases were\\ncommercially available inthelate1970s, butlargely died outbythe1990s: porting tonewdatabases andespecially to\\nnewdomains requires veryspecialist skills andisessentially tooexpensi ve(automatic porting wasattempted butnever\\nsuccessfully developed). Users generally preferred graphical interf aces when these became available. Speech input\\nwould makenatural language interf aces much more useful: unfortunately ,speak er-independent speech recognition\\nstill isn'tgood enough foreven1970s scale NLP toworkwell. Techniques fordealing with misrecognised data\\nhaveprovedhard todevelop. Inmanyways,current commercially-deplo yedspok endialogue systems areusing pre-\\nSHRDLU technology .\\n1.9 Some morehistory\\nBefore the1970s, most NLP researchers were concentrating onMTasanapplication (see above).NLP wasavery\\nearly application ofCSandstarted about thesame time asChomsk ywaspublishing his\\x02rstmajor works informal\\nlinguistics (Chomsk yanlinguistics quickly became dominant, especially intheUS). Inthe1950s andearly 1960s,\\nideas about formal grammar were being workedoutinlinguistics andalgorithms forparsing natural language were\\nbeing developed atthesame time asalgorithms forparsing programming languages. However,most linguists were\\nuninterested inNLP andtheapproach thatChomsk ydeveloped turned outtobeonly some what indirectly useful for\\nNLP.\\nNLP inthe1970s and\\x02rsthalfofthe1980s waspredominantly based onaparadigm where extensi velinguistic and\\nreal-w orld knowledge washand-coded. There wascontro versy about howmuch linguistic knowledge wasnecessary\\nforprocessing, with some researchers downplaying syntax, inparticular ,infavourofworld knowledge. NLP re-\\nsearchers were verymuch partoftheAIcommunity (especially intheUSandtheUK), andthedebate thatwent onin\\nAIabout theuseoflogic vsother meaning representations (`neat' vs`scruf fy')alsoaffected NLP.Bythe1980s, several\",\n",
              " \"forprocessing, with some researchers downplaying syntax, inparticular ,infavourofworld knowledge. NLP re-\\nsearchers were verymuch partoftheAIcommunity (especially intheUSandtheUK), andthedebate thatwent onin\\nAIabout theuseoflogic vsother meaning representations (`neat' vs`scruf fy')alsoaffected NLP.Bythe1980s, several\\nlinguistic formalisms hadappeared which were fully formally grounded andreasonably computationally tractable, and\\nthelinguistic/logical paradigm inNLP was\\x02rmly established. Unfortunately ,thisdidn' tlead tomanyuseful systems,\\npartly because manyofthedif\\x02cult problems (disambiguation etc)were seen assomebody else'sjob(and mainstream\\nAIwasnotdeveloping adequate knowledge representation techniques) andpartly because most researchers were con-\\ncentrating onthe`agent-lik e'applications andneglecting theuser aids. Although thesymbolic, linguistically-based\\nsystems sometimes workedquite well asNLIDs, theyprovedtobeoflittle usewhen itcame toprocessing lessre-\\nstricted text,forapplications such asIE.Italso became apparent thatlexical acquisition wasaserious bottleneck for\\nserious development ofsuch systems.\\nStatistical NLP became themost common paradigm inthe1990s, atleast intheresearch community .Speech recog-\\nnition haddemonstrated thatsimple statistical techniques worked,givenenough training data. NLP systems were\\nbuiltwhich required verylimited hand-coded knowledge, apart from initial training material. Most applications were\\nmuch shallo werthan theearlier NLIDs, buttheswitch tostatistical NLP coincided with achange inUSfunding,\\nwhich started toemphasise speech-based interf aces andIE.There wasalso ageneral realization oftheimportance\\nofserious evaluation andofreporting results inawaythatcould bereproduced byother researchers. USfunding\\nemphasised competitions with speci\\x02c tasks andsupplied testmaterial, which encouraged this, although there wasa\\ndownside inthatsome ofthetechniques developed were verytask-speci\\x02c. Itshould beemphasised thatthere had\\n9been computational workoncorpora formanyyears (much ofitbylinguists): itbecame much easier todocorpus\\nworkbythelate1980s asdisk space became cheap andmachine-readable textbecame ubiquitous. Despite theshift\\ninresearch emphasis tostatistical approaches, most commercial systems remained primarily based onhand-coded\\nlinguistic information.\\nMore recently thesymbolic/statistical split hasbecome lesspronounced, since most researchers areinterested inboth.6\\nThere isconsiderable emphasis onmachine learning ingeneral, including machine learning forsymbolic processing.\\nLinguistically-based NLP hasmade something ofacomeback, with increasing availability ofopen source resources,\\nandtherealisation thatatleast some oftheclassic statistical techniques seem tobereaching limits onperformance,\\nespecially because ofdif\\x02culties inadapting tonewtypes oftext.However,themodern linguistically-based approaches\\naremaking useofmachine learning andstatistical processing. Thedotcom boom andbusthasconsiderably affected\\nNLP,butit'stooearly tosaywhat thelong-term implications are.Theubiquity oftheInternet hascertainly changed\\nthespace ofinteresting NLP applications, andthevastamount oftextavailable canpotentially beexploited, especially\\nforstatistical techniques.\\n1.10 Generic `deep' NLP application architectur e\\nManyNLP applications canbeadequately implemented with relati velyshallo wprocessing. Forinstance, spelling\",\n",
              " \"NLP,butit'stooearly tosaywhat thelong-term implications are.Theubiquity oftheInternet hascertainly changed\\nthespace ofinteresting NLP applications, andthevastamount oftextavailable canpotentially beexploited, especially\\nforstatistical techniques.\\n1.10 Generic `deep' NLP application architectur e\\nManyNLP applications canbeadequately implemented with relati velyshallo wprocessing. Forinstance, spelling\\nchecking only requires awordlistandsimple morphology tobeuseful. I'llusetheterm `deep' NLP forsystems that\\nbuildameaning representation (oranelaborate syntactic representation), which isgenerally agreed toberequired for\\napplications such asNLIDs, email question answering andgood MT.\\nThemost important principle inbuilding asuccessful NLP system ismodularity .NLP systems areoften bigsoftw are\\nengineering projects \\x97success requires thatsystems canbeimpro vedincrementally .\\nThe input toanNLP system could bespeech ortext.Itcould also begesture (multimodal input orperhaps aSign\\nLanguage). Theoutput might benon-linguistic, butmost systems need togivesome sortoffeedback totheuser,even\\niftheyaresimply performing some action (issuing aticket,paying abill, etc). However,often thefeedback canbe\\nveryformulaic.\\nThere' sgeneral agreement thatthefollowing system components canbedescribed semi-independently ,although as-\\nsumptions about thedetailed nature oftheinterf aces between them differ.Notallsystems haveallofthese components:\\n\\x0finput preprocessing: speech recogniser ortextpreprocessor (non-tri vialinlanguages likeChinese orforhighly\\nstructured textforanylanguage) orgesture recogniser .Such system might themselv esbeverycomple x,butI\\nwon'tdiscuss them inthiscourse \\x97we'llassume thattheinput tothemain NLP component issegmented text.\\n\\x0fmorphological analysis: thisisrelati velywell-understood forthemost common languages thatNLP hasconsid-\\nered, butiscomplicated formanylanguages (e.g., Turkish, Basque).\\n\\x0fpartofspeech tagging: notanessential partofmost deep processing systems, butsometimes used asawayof\\ncutting downparser search space.\\n\\x0fparsing: thisincludes syntax andcompositional semantics, which aresometimes treated asseparate components\\n\\x0fdisambiguation: thiscanbedone aspartofparsing, or(partially) lefttoalater phase\\n\\x0fconte xtmodule: thismaintains information about theconte xt,foranaphora resolution, forinstance.\\n\\x0ftextplanning: thepartoflanguage generation that'sconcerned with deciding what meaning toconvey(Iwon't\\ndiscuss thisinthiscourse)\\n\\x0ftactical generation: convertsmeaning representations tostrings. This may usethesame grammar andlexicon7\\nastheparser .\\n\\x0fmorphological generation: aswith morphological analysis, thisisrelati velystraightforw ardforEnglish.\\n6Atleast, there areonly afewresearchers who avoidstatistical techniques asamatter ofprinciple andallstatistical systems haveasymbolic\\ncomponent!\\n7Theterm lexicon isgenerally used forthepartoftheNLP system thatcontains dictionary-lik einformation \\x97i.e.information about individual\\nwords.\\n10\\x0foutput processing: text-to-speech, textformatter ,etc. Aswith input processing, thismay becomple x,butfor\\nnowwe'llassume thatwe'reoutputting simple text.\\nApplication speci\\x02c components, forinstance:\",\n",
              " \"component!\\n7Theterm lexicon isgenerally used forthepartoftheNLP system thatcontains dictionary-lik einformation \\x97i.e.information about individual\\nwords.\\n10\\x0foutput processing: text-to-speech, textformatter ,etc. Aswith input processing, thismay becomple x,butfor\\nnowwe'llassume thatwe'reoutputting simple text.\\nApplication speci\\x02c components, forinstance:\\n1.ForNLinterf aces, email answering andsoon,weneed aninterf acebetween semantic representation (expressed\\nassome form oflogic, forinstance) andtheunderlying knowledge base.\\n2.ForMTbased ontransfer ,weneed acomponent thatmaps between semantic representations.\\nItisalso veryimportant todistinguish between theknowledge sources andtheprograms thatusethem. Forinstance,\\namorphological analyser hasaccess toalexicon andasetofmorphological rules: themorphological generator might\\nshare these knowledge sources. Thelexicon forthemorphology system may bethesame asthelexicon fortheparser\\nandgenerator .\\nOther things might berequired inorder toconstruct thestandard components andknowledge sources:\\n\\x0flexicon acquisition\\n\\x0fgrammar acquisition\\n\\x0facquisition ofstatistical information\\nForacomponent tobeatruemodule, itobviously needs awell-de\\x02ned setofinterf aces. What' slessobvious isthatit\\nneeds itsownevaluation strate gyandtestsuites: developers need tobeable toworksome what independently .\\nInprinciple, atleast, components arereusable invarious ways: forinstance, aparser could beused with multiple\\ngrammars, thesame grammar canbeprocessed bydifferent parsers andgenerators, aparser/grammar combination\\ncould beused inMTorinanatural language interf ace. However,foravariety ofreasons, itisnoteasy toreuse\\ncomponents likethis, andgenerally alotofworkisrequired foreach newapplication, evenifit'sbased onanexisting\\ngrammar orthegrammar isautomatically acquired.\\nWecandrawschematic diagrams forapplications showing howthemodules \\x02ttogether .\\n1.11 Natural language interface toaknowledge base\\nKB\\n*\\nKBINTERF ACE/CONTEXT\\n6\\nPARSING\\n6\\nMORPHOLOGY\\n6\\nINPUT PROCESSING\\n6\\nuser inputj\\nKBOUTPUT/TEXT PLANNING\\n?\\nTACTICAL GENERA TION\\n?\\nMORPHOLOGY GENERA TION\\n?\\nOUTPUT PROCESSING\\n?\\noutput\\n11Insuch systems, theconte xtmodule generally getsincluded aspartoftheKBinterf acebecause thediscourse state is\\nquite simple, andconte xtual resolution isdomain speci\\x02c. Similarly ,there' soften noelaborate textplanning require-\\nment, though thisdepends verymuch ontheKBandtype ofqueries involved.\\nInlectures 2\\x967, various algorithms will bediscussed which could beparts ofmodules inthisgeneric architecture,\\nalthough most arealsouseful inlesselaborate conte xts.Lecture 8willdiscuss thearchitecture andrequirements ofa\\nfewapplications inabitmore detail.\\n1.12 General comments\\n\\x0fEven`simple' NLP applications, such asspelling check ers,need comple xknowledge sources forsome prob-\\nlems.\\n\\x0fApplications cannot be100% perfect, because fullrealworldknowledge isnotpossible.\\n\\x0fApplications thatarelessthan 100% perfect canbeuseful (humans aren' t100% perfect anyway).\\n\\x0fApplications thataidhumans aremuch easier toconstruct than applications which replace humans. Itisdif\\x02cult\\ntomakethelimitations ofsystems which accept speech orlanguage obvious tonaivehuman users.\",\n",
              " \"\\x0fApplications cannot be100% perfect, because fullrealworldknowledge isnotpossible.\\n\\x0fApplications thatarelessthan 100% perfect canbeuseful (humans aren' t100% perfect anyway).\\n\\x0fApplications thataidhumans aremuch easier toconstruct than applications which replace humans. Itisdif\\x02cult\\ntomakethelimitations ofsystems which accept speech orlanguage obvious tonaivehuman users.\\n\\x0fNLP interf aces arenearly alwayscompeting with anon-language based approach.\\n\\x0fCurrently nearly allapplications either dorelati velyshallo wprocessing onarbitrary input ordeep processing on\\nnarro wdomains. MTcanbedomain-speci\\x02c tovarying extents: MTonarbitrary textisn'tverygood, buthas\\nsome applications.\\n\\x0fLimited domain systems require extensi veandexpensi veexpertise toport. Research thatrelies onextensi ve\\nhand-coding ofknowledge forsmall domains isnowgenerally regarded asadead-end, though reusable hand-\\ncoding isadifferent matter .\\n\\x0fThedevelopment ofNLP hasmainly been drivenbyhardw areandsoftw areadvances, andsocietal andinfras-\\ntructure changes, notbygreat newideas. Impro vements inNLP techniques aregenerally incremental rather\\nthan revolutionary .\\n122Lectur e2:Mor phology and\\x02nite-state techniques\\nThis lecture starts with abrief discussion ofmorphology ,concentrating mainly onEnglish morphology .Theconcept\\nofalexicon inanNLP system isdiscussed with respect tomorphological processing. Spelling rules areintroduced\\nandtheuseof\\x02nite state transducers toimplement spelling rules isexplained. The lecture concludes with abrief\\novervie wofsome other uses of\\x02nite state techniques inNLP.\\n2.1 Averybrief andsimpli\\x02ed introduction tomorphology\\nMorphology concerns thestructure ofwords. Words areassumed tobemade upofmorpheme s,which aretheminimal\\ninformation carrying unit. Morphemes which canonly occur inconjunction with other morphemes areaf\\x02xes:words\\naremade upofastem (more than oneinthecase ofcompounds) andzero ormore af\\x02xes.Forinstance, dogisastem\\nwhich may occur with theplural suf\\x02x+si.e.,dogs.English only hassuf\\x02xes(af\\x02xeswhich come after astem) and\\npre\\x02x es(which come before thestem \\x97inEnglish these arelimited toderivational morphology), butother languages\\nhavein\\x02xes (af\\x02xeswhich occur inside thestem) andcircum\\x02x es(af\\x02xeswhich goaround astem). Forinstance,\\nArabic hasstems (root forms) such asktb,which arecombined with in\\x02xestoform words (e.g., kataba ,hewrote;\\nkotob,books). Some English irregular verbs showarelic ofin\\x03ection byin\\x02xation (e.g. sing,sang ,sung )butthis\\nprocess isnolonger productive (i.e., itwon'tapply toanynewwords, such asping).8\\n2.2 In\\x03ectional vsderivational morphology\\nIn\\x03ectional andderivational morphology canbedistinguished, although thedividing lineisn'talwayssharp. The\\ndistinction isofsome importance inNLP,since itmeans different representation techniques may beappropriate.\\nIn\\x03ectional morphology canbethought ofassetting values ofslots insome paradigm .In\\x03ectional morphology\",\n",
              " \"process isnolonger productive (i.e., itwon'tapply toanynewwords, such asping).8\\n2.2 In\\x03ectional vsderivational morphology\\nIn\\x03ectional andderivational morphology canbedistinguished, although thedividing lineisn'talwayssharp. The\\ndistinction isofsome importance inNLP,since itmeans different representation techniques may beappropriate.\\nIn\\x03ectional morphology canbethought ofassetting values ofslots insome paradigm .In\\x03ectional morphology\\nconcerns properties such astense, aspect, number ,person, gender ,andcase, although notalllanguages code allof\\nthese: English, forinstance, hasverylittle morphological marking ofcase andgender .Derivational af\\x02xes,such as\\nun-,re-,anti- etc,haveabroader range ofsemantic possibilities anddon't\\x02tintoneat paradigms. In\\x03ectional af\\x02xes\\nmay becombined (though notinEnglish). However,there arealwaysobvious limits tothis, since once allthepossible\\nslotvalues are`set', nothing elsecanhappen. Incontrast, there arenoobvious limitations onthenumber ofderivational\\naf\\x02xes(antidisestablishmentarianism ,antidisestablishmentarianismization )andtheymay evenbeapplied recursi vely\\n(antiantimissile ).Insome languages, such asInuit, derivational morphology isoften used where English would use\\nadjecti valmodi\\x02cation orother syntactic means. This leads toverylong `words' occurring naturally andispresumably\\nresponsible fortheclaim that`Eskimo' hashundreds ofwords forsnow.\\nIn\\x03ectional morphology isgenerally close tofully producti ve,inthesense that awordofaparticular class will\\ngenerally showallthepossible in\\x03ections although theactual af\\x02xused may vary.Forinstance, anEnglish verbwill\\nhaveapresent tense form, a3rdperson singular present tense form, apastparticiple andapassi veparticiple (thelatter\\ntwobeing thesame forregular verbs). This willalso apply toanynewwords which enter thelanguage: e.g., textas\\naverb\\x97texts,texted.Derivational morphology islessproducti veandtheclasses ofwords towhich anaf\\x02xapplies\\nislessclearcut. Forinstance, thesuf\\x02x-eeisrelati velyproducti ve(textee sounds plausible, meaning therecipient\\nofatextmessage, forinstance), butdoesn' tapply toallverbs (?snoree,?jogee,?dropee ).Derivational af\\x02xesmay\\nchange thepartofspeech ofaword(e.g., -ise/-izeconvertsnouns intoverbs: plural,pluralise ).However,there are\\nalso examples ofwhat issometimes called zeroderivation ,where asimilar effectisobserv edwithout anaf\\x02x:e.g.\\ntango ,waltz etcarewords which arebasically nouns butcanbeused asverbs.\\nStems andaf\\x02xescanbeindividually ambiguous. There isalsopotential forambiguity inhowawordform issplit into\\nmorphemes. Forinstance, unionised could beunion -ise-edor(inchemistry) un-ion-ise-ed.This sortofstructural\\nambiguity isn'tnearly ascommon inEnglish morphology asinsyntax, however.Note thatun-ionisnotapossible\\nform (because un-can'tattach toanoun). Furthermore, although there isapre\\x02x un-thatcanattach toverbs, itnearly\\nalwaysdenotes areversal ofaprocess (e.g., untie ),whereas theun-thatattaches toadjecti vesmeans `not', which is\",\n",
              " 'ambiguity isn\\'tnearly ascommon inEnglish morphology asinsyntax, however.Note thatun-ionisnotapossible\\nform (because un-can\\'tattach toanoun). Furthermore, although there isapre\\x02x un-thatcanattach toverbs, itnearly\\nalwaysdenotes areversal ofaprocess (e.g., untie ),whereas theun-thatattaches toadjecti vesmeans `not\\', which is\\nthemeaning inthecase ofun-ion-ise-ed.Hence theinternal structure ofun-ion-ise-edhastobe(un- ((ion -ise)\\n-ed)) .\\n8Arguably ,though, spok enEnglish hasoneproducti vein\\x02xation process, exempli\\x02ed byabsobloodylutely .\\n132.3 Spelling rules\\nEnglish morphology isessentially concatenati ve:i.e.,wecanthink ofwords asasequence ofpre\\x02x es,stems and\\nsuf\\x02xes.Some words haveirregular morphology andtheir in\\x03ectional forms simply havetobelisted. However,in\\nother cases, there areregular phonological orspelling changes associated with af\\x02xation. Forinstance, thesuf\\x02x-sis\\npronounced differently when itisadded toastem which ends ins,xorzandthespelling re\\x03ects thiswith theaddition\\nofane(boxes etc). Forthepurposes ofthiscourse, we\\'lljusttalkabout spelling effects rather than phonological\\neffects: these effects canbecaptured byspelling rules (also knownasortho graphic rules ).\\nEnglish spelling rules canbedescribed independently oftheparticular stems andaf\\x02xesinvolved,simply interms of\\ntheaf\\x02xboundary .The`e-insertion\\' rulecanbedescribed asfollows:\\n\"!e=8\\n<\\n:s\\nx\\nz9\\n=\\n;^s\\nInsuch rules, themapping isalwaysgivenfrom the`underlying\\' form tothesurfaceform, themapping isshownto\\ntheleftoftheslash andtheconte xttotheright, with the indicating theposition inquestion.\"isused fortheempty\\nstring and^fortheaf\\x02xboundary .This particular ruleisread assaying thattheempty string maps to`e\\'intheconte xt\\nwhere itispreceded byans,x,orzandanaf\\x02xboundary andfollowed byans.Forinstance, thismaps box^stoboxes .\\nThis rulemight look asthough itiswritten inaconte xtsensiti vegrammar formalism, butactually we\\'llseeinx2.7\\nthatitcorresponds toa\\x02nite state transducer .Because theruleisindependent oftheparticular af\\x02x,itapplies equally\\ntotheplural form ofnouns andthe3rdperson singular present form ofverbs. Other spelling rules inEnglish include\\nconsonant doubling (e.g., rat,ratted ,though note, not*auditted )andy/ieconversion (party ,parties ).\\n2.4 Applications ofmorphological processing\\nItispossible touseafull-form lexicon forEnglish NLP: i.e.,tolistallthein\\x03ected forms andtotreat derivational\\nmorphology asnon-producti ve.However,when anewwordhastobetreated (generally because theapplication is\\nexpanded butinprinciple because anewwordhasentered thelanguage) itisredundant tohavetospecify (orlearn)\\nthein\\x03ected forms aswell asthestem, since thevastmajority ofwords inEnglish haveregular morphology .Soa\\nfull-form lexicon isbestregarded asaform ofcompilation. Manyother languages havemanymore in\\x03ectional forms,\\nwhich increases theneed todomorphological analysis rather than full-form listing.\\nIRsystems usestemming rather than fullmorphological analysis. ForIR,what isrequired istorelate forms, notto',\n",
              " \"thein\\x03ected forms aswell asthestem, since thevastmajority ofwords inEnglish haveregular morphology .Soa\\nfull-form lexicon isbestregarded asaform ofcompilation. Manyother languages havemanymore in\\x03ectional forms,\\nwhich increases theneed todomorphological analysis rather than full-form listing.\\nIRsystems usestemming rather than fullmorphological analysis. ForIR,what isrequired istorelate forms, notto\\nanalyse them compositionally ,andthiscanmost easily beachie vedbyreducing allmorphologically comple xforms\\ntoacanonical form. Although thisisreferred toasstemming, thecanonical form may notbethelinguistic stem.\\nThemost commonly used algorithm isthePorter stemmer ,which uses aseries ofsimple rules tostrip endings (see\\nJ&M, section 3.4) without theneed foralexicon. However,stemming does notnecessarily help IR.Search engines\\nsometimes doin\\x03ectional morphology ,butthiscanbedangerous. Forinstance, onesearch engine searches forcorpus\\naswell ascorpor awhen giventhelatter asinput, resulting inalargenumber ofspurious results involving Corpus\\nChristi andsimilar terms.\\nInmost NLP applications, however,morphological analysis isaprecursor tosome form ofparsing. Inthiscase, the\\nrequirement istoanalyse theform into astem andaf\\x02xessothatthenecessary syntactic (and possibly semantic)\\ninformation canbeassociated with it.Morphological analysis isoften called lemmatization .Forinstance, forthepart\\nofspeech tagging application which wewilldiscuss inthenextlecture, muggedwould beassigned apartofspeech\\ntagwhich indicates itisaverb, though mug isambiguous between verbandnoun. Forfullparsing, asdiscussed\\ninlectures 4and5,we'llneed more detailed syntactic andsemantic information. Morphological generation takesa\\nstem andsome syntactic information andreturns thecorrect form. Forsome applications, there isarequirement that\\nmorphological processing isbidir ectional :thatis,canbeused foranalysis andgeneration. The\\x02nite state transducers\\nwewilllook atbelowhavethisproperty .\\n2.5 Lexical requir ements formorphological processing\\nThere arethree sorts oflexical information thatareneeded forfull, high precision morphological processing:\\n14\\x0faf\\x02xes,plus theassociated information conveyedbytheaf\\x02x\\n\\x0firregular forms, with associated information similar tothatforaf\\x02xes\\n\\x0fstems with syntactic categories (plus more detailed information ifderivational morphology istobetreated as\\nproducti ve)\\nOne approach toanaf\\x02xlexicon isforittoconsist ofapairing ofaf\\x02xandsome encoding ofthesyntactic/semantic\\neffectoftheaf\\x02x.9Forinstance, consider thefollowing fragment ofasuf\\x02xlexicon (wecanassume there isaseparate\\nlexicon forpre\\x02x es):\\nedPAST_VERB\\nedPSP_VERB\\nsPLURAL_NOUN\\nHerePAST_VERB ,PSP_VERB andPLURAL_NOUN areabbre viations forsome bundle ofsyntactic/semantic infor -\\nmation: we'lldiscuss thisbrie\\x03y inx5.7.\\nAlexicon ofirregular forms isalso needed. One approach isforthistojustbeatriple consisting ofin\\x03ected form,\\n`af\\x02xinformation' andstem, where `af\\x02xinformation' corresponds towhate verencoding isused fortheregular af\\x02x.\\nForinstance:\\nbeganPAST_VERB begin\\nbegunPSP_VERB begin\\nNote thatthisinformation canbeused forgeneration aswell asanalysis, ascantheaf\\x02xlexicon.\\nInmost cases, English irregular forms arethesame forallsenses ofaword. Forinstance, ranisthepast ofrun\",\n",
              " \"`af\\x02xinformation' andstem, where `af\\x02xinformation' corresponds towhate verencoding isused fortheregular af\\x02x.\\nForinstance:\\nbeganPAST_VERB begin\\nbegunPSP_VERB begin\\nNote thatthisinformation canbeused forgeneration aswell asanalysis, ascantheaf\\x02xlexicon.\\nInmost cases, English irregular forms arethesame forallsenses ofaword. Forinstance, ranisthepast ofrun\\nwhether wearetalking about athletes, politicians ornoses. This argues forassociating irregularity with particular\\nwordforms rather than particular senses, especially since compounds also tend tofollowtheirregular spelling, even\\nnon-producti velyformed ones (e.g., theplural ofdormouse isdormice ).However,there areexceptions: e.g., The\\nwashing washung/*hang edouttodryvsthemurdererwashang ed.\\nMorphological analysers also generally haveaccess toalexicon ofregular stems. This isneeded forhigh precision:\\ne.g.toavoidanalysing corpus ascorpu -sweneed toknowthatthere isn'tawordcorpu .There arealso cases where\\nhistorically awordwasderived,butwhere thebase form isnolonger found inthelanguage: wecanavoidanalysing\\nunkempt asun-kempt ,forinstance, simply bynothaving kempt inthestem lexicon. Ideally thislexicon should have\\nsyntactic information: forinstance, feed could befee-ed,butsince feeisanoun rather than averb,thisisn'tapossible\\nanalysis. However,intheapproach we'llassume, themorphological analyser issplit into twostages. The \\x02rst of\\nthese only concerns morpheme forms andreturns both fee-edandfeed giventheinput feed.Asecond stage which is\\nclosely coupled tothesyntactic analysis then rules outfee-edbecause theaf\\x02xandstem syntactic information arenot\\ncompatible (seex5.7foroneapproach tothis).\\nIfmorphology waspurely concatenati ve,itwould beverysimple towrite analgorithm tosplit offaf\\x02xes.Spelling\\nrules complicate thissome what: infact,it'sstillpossible todoareasonable jobforEnglish with adhoccode, buta\\ncleaner andmore general approach istouse\\x02nite state techniques.\\n2.6 Finite state automata forrecognition\\nThe approach tospelling rules thatwe'lldescribe involvestheuseof\\x02nite state transducers (FSTs). Rather than\\njumping straight intothis, we'llbrie\\x03y consider thesimpler \\x02nite state automata andhowtheycanbeused inasimple\\nrecogniser .Suppose wewanttorecognise dates (just dayandmonth pairs) written intheformat day/month. Theday\\nandthemonth may beexpressed asoneortwodigits (e.g. 11/2, 1/12 etc). This format corresponds tothefollowing\\nsimple FSA, where each character corresponds toonetransition:\\n9J&M describe analternati veapproach which istomakethesyntactic information correspond toalevelina\\x02nite state transducer .However,at\\nleast forEnglish, thisconsiderably complicates thetransducers.\\n150,1,2,3 digit / 0,1 0,1,2\\ndigit digit1 2 3 4 5 6\\nAccept states areshownwith adouble circle. This isanon-deterministic FSA: forinstance, aninput starting with the\\ndigit 3will movetheFSA toboth state 2andstate 3.This corresponds toalocal ambiguity :i.e.,onethatwill be\\nresolv edbysubsequent conte xt.Byconvention, there must beno`left over'characters when thesystem isinthe\\x02nal\\nstate.\",\n",
              " 'Accept states areshownwith adouble circle. This isanon-deterministic FSA: forinstance, aninput starting with the\\ndigit 3will movetheFSA toboth state 2andstate 3.This corresponds toalocal ambiguity :i.e.,onethatwill be\\nresolv edbysubsequent conte xt.Byconvention, there must beno`left over\\'characters when thesystem isinthe\\x02nal\\nstate.\\nTomakethisabitmore interesting, suppose wewanttorecognise acomma-separated listofsuch dates. TheFSA,\\nshownbelow,nowhasacycle andcanaccept asequence ofinde\\x02nite length (note thatthisisiteration andnotfull\\nrecursion, however).\\n0,1,2,3 digit / 0,1 0,1,2\\ndigit digit\\n,1 2 3 4 5 6\\nBoth these FSAs willaccept sequences which arenotvaliddates, such as37/00. Conversely ,ifweusethem togenerate\\n(random) dates, wewillgetsome invalidoutput. Ingeneral, asystem which generates output which isinvalidissaid\\ntoovergenerate.Infact,inmanylanguage applications, some amount ofovergeneration canbetolerated, especially if\\nweareonly concerned with analysis.\\n2.7 Finite state transducers\\nFSAs canbeused torecognise particular patterns, butdon\\'t,bythemselv es,allowforanyanalysis ofwordforms.\\nHence formorphology ,weuse\\x02nite state transducers (FSTs) which allowthesurfacestructure tobemapped intothe\\nlistofmorphemes. FSTs areuseful forboth analysis andgeneration, since themapping isbidirectional. This approach\\nisknownastwo-le velmorpholo gy.\\nToillustrate two-levelmorphology ,consider thefollowing FST,which recognises theaf\\x02x-sallowing forenviron-\\nments corresponding tothee-insertion spelling ruleshowninx2.3.10\\n10Actually ,I\\'vesimpli\\x02ed thisslightly sothecorrespondence tothespelling ruleisnotexact: J&M giveamore comple xtransducer which isan\\naccurate re\\x03ection ofthespelling rule.\\n161other :other\\n\":^\\n2s:s\\n3\\n4other :others:sx:xz:z e:^\\ns:sx:xz:z\\nTransducers map between tworepresentations, soeach transition corresponds toapair ofcharacters. Aswith the\\nspelling rule, weusethespecial character `\"\\'tocorrespond totheempty character and`^\\'tocorrespond toanaf\\x02x\\nboundary .Theabbre viation `other :other\\' means thatanycharacter notmentioned speci\\x02cally intheFST maps to\\nitself. Aswith theFSA example, weassume thattheFST only accepts aninput iftheendoftheinput corresponds to\\nanaccept state (i.e., no`left-o ver\\'characters areallowed).\\nForinstance, with thisFST,`dogs\\'maps to`dog\\x88s\\',`foxes\\'maps to`fox\\x88s\\'and`buzzes\\'maps to`bu\\nzz\\x88s\\'.When thetransducer isruninanalysis mode, thismeans thesystem candetect anaf\\x02xboundary (and hence\\nlook upthestem andtheaf\\x02xintheappropriate lexicons). Ingeneration mode, itcanconstruct thecorrect string. This\\nFST isnon-deterministic.\\nSimilar FSTs canbewritten fortheother spelling rules forEnglish (although todoconsonant doubling correctly ,in-\\nformation about stress andsyllable boundaries isrequired andthere arealsodifferences between British andAmerican',\n",
              " \"look upthestem andtheaf\\x02xintheappropriate lexicons). Ingeneration mode, itcanconstruct thecorrect string. This\\nFST isnon-deterministic.\\nSimilar FSTs canbewritten fortheother spelling rules forEnglish (although todoconsonant doubling correctly ,in-\\nformation about stress andsyllable boundaries isrequired andthere arealsodifferences between British andAmerican\\nspelling conventions which complicate matters). Morphology systems areusually implemented sothatthere isone\\nFST perspelling ruleandthese operate inparallel.\\nOne issue with thisuseofFSTs isthattheydonotallowforanyinternal structure ofthewordform. Forinstance, we\\ncanproduce asetofFSTs which willresult inunionised being mapped intoun^ion^ise^ed,butaswe'veseen, the\\naf\\x02xesactually havetobeapplied intheright order andthisisn'tmodelled bytheFSTs.\\n2.8 Some other uses of\\x02nite state techniques inNLP\\n\\x0fGrammars forsimple spok endialogue systems. Finite state techniques arenotadequate tomodel grammars\\nofnatural languages: we'lldiscuss thisalittle inx4.12. However,forverysimple spok endialogue systems,\\na\\x02nite-state grammar may beadequate. More comple xgrammars canbewritten asCFGs andcompiled into\\n\\x02nite state approximations.\\n\\x0fPartial grammars fornamed entity recognition (brie\\x03y discussed inx4.12).\\n\\x0fDialogue models forspok endialogue systems (SDS). SDS usedialogue models foravariety ofpurposes: in-\\ncluding controlling thewaythattheinformation acquired from theuser isinstantiated (e.g., theslots thatare\\n\\x02lled inanunderlying database) andlimiting thevocabulary toachie vehigher recognition rates. FSAs canbe\\nused torecord possible transitions between states inasimple dialogue. Forinstance, consider theproblem of\\n17obtaining adate expressed asadayandamonth from auser.There arefour possible states, corresponding to\\ntheuser input recognised sofar:\\n1.Noinformation. System prompts formonth andday.\\n2.Month only isknown.System prompts forday.\\n3.Day only isknown.System prompts formonth.\\n4.Month anddayknown.\\nTheFSA isshownbelow.Theloops thatstayinasingle state correspond touserresponses thataren' trecognised\\nascontaining therequired information (mumble istheterm generally used foranunrecognised input).\\n1mumble\\nmonth day\\nday&month2mumble\\nday3mumble\\nmonth\\n4\\n2.9 Probabilistic FSAs\\nInmanycases, itisuseful toaugment theFSA with information about transition probabilities. Forinstance, inthe\\nSDS system described above,itismore likelythatauser willspecify amonth alone than adayalone. Aprobabilistic\\nFSA fortheSDS isshownbelow.Note thattheprobabilities ontheoutgoing arcsfrom each state must sum to1.\\n10.1\\n0.5 0.1\\n0.3 20.1\\n0.930.2\\n0.8\\n4\\n2.10 Further reading\\nChapters 2and3ofJ&M. Much ofChapter 2should befamiliar from other courses intheCST (atleast toPartII\\nstudents). Chapter 3uses more elaborate transducers than I'vediscussed.\\n183Lectur e3:Prediction andpart-of-speech tagging\\nThis lecture introduces some simple statistical techniques andillustrates their useinNLP forprediction ofwords and\\npart-of-speech categories. Itstarts with adiscussion ofcorpora, then introduces wordprediction. Wordprediction can\",\n",
              " \"students). Chapter 3uses more elaborate transducers than I'vediscussed.\\n183Lectur e3:Prediction andpart-of-speech tagging\\nThis lecture introduces some simple statistical techniques andillustrates their useinNLP forprediction ofwords and\\npart-of-speech categories. Itstarts with adiscussion ofcorpora, then introduces wordprediction. Wordprediction can\\nbeseen asawayof(crudely) modelling some syntactic information (i.e., wordorder). Similar statistical techniques\\ncanalsobeused todisco verparts ofspeech foruses ofwords inacorpus. Thelecture concludes with some discussion\\nofevaluation.\\n3.1 Corpora\\nAcorpus (corpora istheplural) issimply abody oftextthat hasbeen collected forsome purpose. Abalanced\\ncorpus contains textswhich represent different genres (newspapers, \\x02ction, textbooks, parliamentary reports, cooking\\nrecipes, scienti\\x02c papers etcetc): early examples were theBrowncorpus (USEnglish) andtheLancaster -Oslo-Ber gen\\n(LOB) corpus (British English) which areeach about 1million words: themore recent British National Corpus (BNC)\\ncontains approx 100million words andincludes 20million words ofspok enEnglish. Corpora areimportant formany\\ntypes oflinguistic research, although mainstream linguists havetended todismiss their useinfavourofreliance on\\nintuiti vejudgements. Corpora areessential formuch modern NLP research, though NLP researchers haveoften used\\nnewspaper text(particularly theWallStreet Journal) rather than balanced corpora.\\nDistrib uted corpora areoften annotated insome way:themost important type ofannotation forNLP ispart-of-speech\\ntagging (POS tagging), which we'lldiscuss further below.\\nCorpora may also becollected foraspeci\\x02c task. Forinstance, when implementing anemail answering application,\\nitisessential tocollect samples ofrepresentati veemails. Forinterf aceapplications inparticular ,collecting acorpus\\nrequires asimulation oftheactual application: generally thisisdone byaWizardofOzexperiment, where ahuman\\npretends tobeacomputer .\\nCorpora areneeded inNLP fortworeasons. Firstly ,wehavetoevaluate algorithms onreallanguage: corpora are\\nrequired forthispurpose foranystyle ofNLP.Secondly ,corpora provide thedata source formanymachine-learning\\napproaches.\\n3.2 Prediction\\nTheessential idea ofprediction isthat, givenasequence ofwords, wewanttodetermine what' smost likelytocome\\nnext.There areanumber ofreasons towanttodothis: themost important isasaform oflangua gemodelling for\\nautomatic speech recognition. Speech recognisers cannot accurately determine awordfrom thesound signal forthat\\nwordalone, andtheycannot reliably tellwhere each wordstarts and\\x02nishes.11Sothemost probable wordischosen\\nonthebasis ofthelanguage model, which predicts themost likelyword,giventheprior conte xt.Thelanguage models\\nwhich arecurrently most effectiveworkonthebasis ofN-grams(atype ofMark ovchain),where thesequence ofthe\\npriorn\\x001words isused topredict thenext.Trigram models usethepreceding 2words, bigram models thepreceding\\nwordandunigram models usenoconte xtatall,butsimply workonthebasis ofindividual wordprobabilities. Bigrams\",\n",
              " \"onthebasis ofthelanguage model, which predicts themost likelyword,giventheprior conte xt.Thelanguage models\\nwhich arecurrently most effectiveworkonthebasis ofN-grams(atype ofMark ovchain),where thesequence ofthe\\npriorn\\x001words isused topredict thenext.Trigram models usethepreceding 2words, bigram models thepreceding\\nwordandunigram models usenoconte xtatall,butsimply workonthebasis ofindividual wordprobabilities. Bigrams\\narediscussed below,though Iwon'tgointodetails ofexactly howtheyareused inspeech recognition.\\nWordprediction isalsouseful incommunication aids: i.e.,systems forpeople who can'tspeak because ofsome form\\nofdisability .People who usetext-to-speech systems totalkbecause ofanon-linguistic disability usually havesome\\nform ofgeneral motor impairment which also restricts their ability totype atnormal rates (strok e,ALS, cerebral\\npalsy etc). Often theyusealternati veinput devices, such asadapted keyboards, pufferswitches, mouth sticks or\\neyetrack ers. Generally such users canonly construct textatafewwords aminute, which istooslowforanything\\nlikenormal communication tobepossible (normal speech isaround 150words perminute). Asapartial aid,aword\\nprediction system issometimes helpful: thisgivesalistofcandidate words thatchanges astheinitial letters areentered\\nbytheuser.Theuser chooses thedesired wordfrom amenu when itappears. Themain dif\\x02culty with using statistical\\nprediction models insuch applications isin\\x02nding enough data: tobeuseful, themodel really hastobetrained onan\\nindividual speak er'soutput, butofcourse verylittle ofthisislikelytobeavailable.\\n11Infact,although humans arebetter atdoing thisthan speech recognisers, wealso need conte xttorecognise words, especially words likethe\\nanda.\\n19Prediction isimportant inestimation ofentrop y,including estimations oftheentrop yofEnglish. Thenotion ofentrop y\\nisimportant inlanguage modelling because itgivesametric forthedif\\x02culty oftheprediction problem. Forinstance,\\nspeech recognition ismuch easier insituations where thespeak erisonly saying twoeasily distinguishable words than\\nwhen thevocabulary isunlimited: measurements ofentrop ycanquantify this, butwon'tbediscussed further inthis\\ncourse.\\nOther applications forprediction include optical character recognition (OCR), spelling correction andtextsegmen-\\ntation forlanguages such asChinese, which areconventionally written without explicit wordboundaries. Some ap-\\nproaches towordsense disambiguation, tobediscussed inlecture 6,canalsobetreated asaform ofprediction.\\n3.3 bigrams\\nAbigram model assigns aprobability toawordbased ontheprevious word: i.e.P(wnjwn\\x001)wherewnisthenth\\nwordinsome string. Theprobability ofsome string ofwordsP(Wn\\n1)isthus approximated bytheproduct ofthese\\nconditional probabilities:\\nP(Wn\\n1)\\x19nY\\nk=1P(wkjwk\\x001)\\nForexample, suppose wehavethefollowing tinycorpus ofutterances:\\ngood morning\\ngood afternoon\\ngood afternoon\\nitisverygood\\nitisgood\\nWe'llusethesymbolhsitoindicate thestart ofanutterance, sothecorpus really looks like:\\nhsigood morninghsigood afternoonhsigood afternoonhsiitisverygoodhsiitisgoodhsi\\nThebigram probabilities aregivenas\\nC(wn\\x001wn)P\\nwC(wn\\x001w)\",\n",
              " \"P(Wn\\n1)\\x19nY\\nk=1P(wkjwk\\x001)\\nForexample, suppose wehavethefollowing tinycorpus ofutterances:\\ngood morning\\ngood afternoon\\ngood afternoon\\nitisverygood\\nitisgood\\nWe'llusethesymbolhsitoindicate thestart ofanutterance, sothecorpus really looks like:\\nhsigood morninghsigood afternoonhsigood afternoonhsiitisverygoodhsiitisgoodhsi\\nThebigram probabilities aregivenas\\nC(wn\\x001wn)P\\nwC(wn\\x001w)\\ni.e.thecount ofaparticular bigram, normalised bydividing bythetotal number ofbigrams starting with thesame\\nword(which isequivalent tothetotal number ofoccurrences ofthatword, except inthecase ofthelasttoken,a\\ncomplication which canbeignored forareasonable sizeofcorpus).\\nsequence count bigramprobability\\n<s> 5\\n<s>good 3 .6\\n<s>it 2 .4\\ngood 5\\ngoodmorning 1 .2\\ngoodafternoon 2 .4\\ngood<s> 2 .4\\nmorning 1\\nmorning <s> 1 1\\nafternoon 2\\nafternoon <s>2 1\\nitis 2 1\\nisvery 1 .5\\nisgood 1 .5\\nverygood 1 1\\n20This yields aprobability of0.24 forthestring `hsigoodhsi'which isthehighest probability utterance thatwecan\\nconstruct onthebasis ofthebigrams from thiscorpus, ifweimpose theconstraint thatanutterance must beginwith\\nhsiandendwithhsi.\\nForapplication tocommunication aids, wearesimply concerned with predicting thenextword: once theuser has\\nmade their choice, thewordcan'tbechanged. Forspeech recognition, theN-gram approach isapplied tomaximise\\nthelikelihood ofasequence ofwords, hence we'relooking to\\x02ndthemost likelysequence overall. Notice thatwe\\ncanregardbigrams ascomprising asimple deterministic weighted FSA. TheViterbi algorithm ,anef\\x02cient method of\\napplying N-grams inspeech recognition andother applications, isusually described interms ofanFSA.\\nTheprobability of`hsiverygood' based onthiscorpus is0,since theconditional probability of`very'given`hsi'is0\\nsince wehaven'tfound anyexamples ofthisinthetraining data. Ingeneral, thisisproblematic because wewillnever\\nhaveenough data toensure thatwewillseeallpossible eventsandsowedon'twanttoruleoutunseen eventsentirely .\\nToallowforsparsedata wehavetousesmoothing ,which simply means thatwemakesome assumption about the\\n`real' probability ofunseen orveryinfrequently seen events anddistrib utethatprobability appropriately .Acommon\\napproach issimply toaddonetoallcounts: thisisadd-one smoothing which isnotsound theoretically ,butissimple\\ntoimplement. Abetter approach inthecase ofbigrams istobackofftotheunigram probabilities: i.e.,todistrib ute\\ntheunseen probability mass sothatitisproportional totheunigram probabilities. This sortofestimation isextremely\\nimportant togetgood results from N-gram techniques, butwewon'tdiscuss thedetails inthiscourse.\\n3.4 Partofspeech tagging\\nPrediction techniques canbeused forwordclasses, rather than justindividual words. One important application isto\\npart-of-speech tagging (POS tagging), where thewords inacorpus areassociated with atagindicating some syntactic\\ninformation thatapplies tothatparticular useoftheword. Forinstance, consider theexample sentence below:\\nTheycan\\x02sh.\\nThis hastworeadings: one(themost likely)about ability to\\x02shandother about putting \\x02shincans. \\x02shisambiguous\",\n",
              " \"part-of-speech tagging (POS tagging), where thewords inacorpus areassociated with atagindicating some syntactic\\ninformation thatapplies tothatparticular useoftheword. Forinstance, consider theexample sentence below:\\nTheycan\\x02sh.\\nThis hastworeadings: one(themost likely)about ability to\\x02shandother about putting \\x02shincans. \\x02shisambiguous\\nbetween asingular noun, plural noun andaverb,while canisambiguous between singular noun, verb(the `put in\\ncans' use) andmodal verb.However,theyisunambiguously apronoun. (Iamignoring some lesslikelypossibilities,\\nsuch asproper names.) These distinctions canbeindicated byPOS tags:\\ntheyPNP\\ncanVM0VVBVVINN1\\nfishNN1NN2VVBVVI\\nThere areseveralstandard tagsets used incorpora andinPOS tagging experiments. TheoneI'musing fortheexamples\\ninthislecture isCLA WS5(C5) which isgiveninfullinappendix CinJ&M. Themeaning ofthetagsaboveis:\\nNN1singular noun\\nNN2pluralnoun\\nPNPpersonal pronoun\\nVM0modalauxiliary verb\\nVVBbaseformofverb(except infinitive)\\nVVIinfinitive formofverb(i.e.occurswith`to')\\nAPOS tagger resolv esthelexical ambiguities togivethemost likelysetoftagsforthesentence. Inthiscase, theright\\ntagging islikelytobe:\\nTheyPNP canVM0 \\x02shVVB .PUN\\nNote thetagforthefullstop: punctuation istreated asunambiguous. POS tagging canberegarded asaform ofvery\\nbasic wordsense disambiguation.\\nTheother syntactically possible reading is:\\nTheyPNP canVVB \\x02shNN2 .PUN\\n21However,POS taggers (unlik efullparsers) don'tattempt toproduce globally coherent analyses. Thus aPOS tagger\\nmight return:\\nTheyPNP canVM0 \\x02shNN2 .PUN\\ndespite thefactthatthisdoesn' tcorrespond toapossible reading ofthesentence.\\nPOS tagging isuseful asawayofannotating acorpus because itmakesiteasier toextract some types ofinformation\\n(forlinguistic research orNLP experiments). Italso actsasabasis formore comple xforms ofannotation. Named\\nentity recognisers (discussed inlecture 4)aregenerally runonPOS-tagged data. POS taggers aresometimes runas\\npreprocessors tofullparsing, since thiscancutdownthesearch space tobeconsidered bytheparser .\\n3.5 Stochastic POS tagging\\nOne form ofPOS tagging applies theN-gram technique thatwesawabove,butinthiscase itapplies tothePOS\\ntags rather than theindividual words. Themost common approaches depend onasmall amount ofmanually tagged\\ntraining data from which POS N-grams canbeextracted.12I'llillustrate thiswith respect toanother trivialcorpus:\\nTheyused tocan\\x02shinthose towns. Butnowfewpeople \\x02shthere.\\nThis might betagged asfollows:\\nThey_PNP used_VVD to_TO0can_VVI fish_NN2 in_PRPthose_DT0 towns_NN2 ._PUN\\nBut_CJC now_AV0 few_DT0 people_NN2 fish_VVB in_PRPthese_DT0 areas_NN2 ._PUN\\nThis yields thefollowing counts andprobabilities:\\nsequence count bigramprobability\\nAV0 1\\nAV0DT0 1 1\\nCJC 1\\nCJCAV0 1 1\\nDT0 3\\nDT0NN2 3 1\\nNN2 4\\nNN2PRP 1 0.25\\nNN2PUN 2 0.5\\nNN2VVB 1 0.25\\nPNP 1\\nPNPVVD 1 1\\nPRP 1\\nPRPDT0 2 1\\nPUN 1\\nPUNCJC 1 1\\nTO0 1\\n12Itispossible tobuildPOS taggers thatworkwithout ahand-tagged corpus, buttheydon'tperform aswell asasystem trained onevena1,000\",\n",
              " \"CJC 1\\nCJCAV0 1 1\\nDT0 3\\nDT0NN2 3 1\\nNN2 4\\nNN2PRP 1 0.25\\nNN2PUN 2 0.5\\nNN2VVB 1 0.25\\nPNP 1\\nPNPVVD 1 1\\nPRP 1\\nPRPDT0 2 1\\nPUN 1\\nPUNCJC 1 1\\nTO0 1\\n12Itispossible tobuildPOS taggers thatworkwithout ahand-tagged corpus, buttheydon'tperform aswell asasystem trained onevena1,000\\nwordcorpus which canbetagged inafewhours. Furthermore, these algorithms stillrequire alexicon which associates possible tagswith words.\\n22TO0VVI 1 1\\nVVB 1\\nVVBPRP 1 1\\nVVD 1\\nVVDTO0 1 1\\nVVI 1\\nVVINN2 1 1\\nWecanalsoobtain alexicon from thetagged data:\\nwordtagcount\\ntheyPNP1\\nusedVVD1\\ntoTO01\\ncanVVI1\\nfishNN21\\nVVB1\\ninPRP2\\nthoseDT01\\ntownsNN21\\n.PUN1\\nbutCJC1\\nnowAV01\\nfewDT01\\npeopleNN21\\ntheseDT01\\nareasNN21\\nTheidea ofstochastic POS tagging isthatthetagcanbeassigned based onconsideration ofthelexical probability\\n(howlikelyitisthatthewordhasthattag), plus thesequence ofprior tags. Forabigram model, weonly look at\\nasingle previous tag. This isslightly more complicated than thewordprediction case because wehavetotakeinto\\naccount both words andtags.\\nWearetrying toestimate theprobability ofasequence oftags givenasequence ofwords:P(TjW).ByBayes\\ntheorem:\\nP(TjW)=P(T)P(WjT)\\nP(W)\\nSince we'relooking atassigning tags toaparticular sequence ofwords,P(W)isconstant, soforarelati vemeasure\\nofprobability wecanuse:\\nP(TjW)=P(T)P(WjT)\\nWenowhavetoestimateP(T)andP(WjT).Ifwemakethebigram assumption, P(T)isapproximated byP(tijti\\x001)\\n\\x97i.e.,theprobability ofsome taggiventheimmediately preceding tag.Weapproximate P(WjT)asP(wijti).These\\nvalues canbeestimated from thecorpus frequencies.\\nNote thatweendupmultiplying P(tijti\\x001)withP(wijti)(the probability ofthewordgiventhetag) rather than\\nP(tijwi)(the probability ofthetaggiventheword). Forinstance, ifwe'retrying tochoose between thetags NN2\\nandVVB for\\x02shinthesentence they\\x02sh,wecalculateP(NN2ijPNPi\\x001),P(\\x02shijNN2i),P(VVBijPNPi\\x001)and\\nP(\\x02shijVVBi).\\nInfact,POS taggers generally usetrigrams rather than bigrams \\x97therelevantequations aregiveninJ&M, page 306.\\nAswith wordprediction, backoffandsmoothing arecrucial forreasonable performance.\\nWhen aPOS tagger sees awordwhich wasnotinitstraining data, weneed some wayofassigning possible tagstothe\\nword. One approach issimply touseallpossible open class tags, with probabilities based ontheunigram probabilities\\n23ofthose tags. Open class words areones forwhich wecannevergiveacomplete listforaliving language, since words\\narealwaysbeing added: i.e.,verbs, nouns, adjecti vesandadverbs. The restareconsidered closed class. Abetter\",\n",
              " \"word. One approach issimply touseallpossible open class tags, with probabilities based ontheunigram probabilities\\n23ofthose tags. Open class words areones forwhich wecannevergiveacomplete listforaliving language, since words\\narealwaysbeing added: i.e.,verbs, nouns, adjecti vesandadverbs. The restareconsidered closed class. Abetter\\napproach istouseamorphological analyser torestrict thisset:e.g., words ending in-edarelikelytobeVVD (simple\\npast) orVVN (past participle), butcan'tbeVVG(-ing form).\\n3.6 Evaluation ofPOS tagging\\nPOS tagging algorithms areevaluated interms ofpercentage ofcorrect tags. Thestandard assumption isthatevery\\nwordshould betagged with exactly onetag,which isscored ascorrect orincorrect: there arenomarks fornear\\nmisses. Generally there aresome words which canbetagged inonly oneway,soareautomatically counted ascorrect.\\nPunctuation isgenerally givenanunambiguous tag. Therefore thesuccess rates ofover95% which aregenerally\\nquoted forPOS tagging arealittle misleading: thebaseline ofchoosing themost common tagbased onthetraining\\nsetoften gives90% accurac y.Some POS taggers returns multiple tagsincases where more than onetaghasasimilar\\nprobability .\\nItisworth noting thatincreasing thesize ofthetagset does notnecessarily result indecreased performance: this\\ndepends onwhether thetagsthatareadded cangenerally beassigned unambiguously ornot.Potentially ,adding more\\n\\x02ne-grained tags could increase performance. Forinstance, suppose wewanted todistinguish between present tense\\nverbs according towhether theywere 1st,2ndor3rdperson. WiththeC5tagset, andthestochastic tagger described,\\nthiswould beimpossible todowith high accurac y,because allpronouns aretagged PRP,hence theyprovide no\\ndiscriminating power.Ontheother hand, ifwetagged IandweasPRP1, youasPRP2 andsoon,theN-gram approach\\nwould allowsome discrimination. Ingeneral, predicting onthebasis ofclasses means wehavelessofasparse data\\nproblem than when predicting onthebasis ofwords, butwealso lose discriminating power.There isalso something\\nofatradeof fbetween theutility ofasetoftagsandtheir usefulness inPOS tagging. Forinstance, C5assigns separate\\ntags forthedifferent forms ofbe,which isredundant formanypurposes, buthelps makedistinctions between other\\ntagsintagging models where theconte xtisgivenbyatagsequence alone (i.e., rather than considering words prior to\\nthecurrent one).\\nPOS tagging exempli\\x02es some general issues inNLP evaluation:\\nTraining data andtestdata The assumption inNLP isalwaysthatasystem should workonnoveldata, therefore\\ntestdata must bekeptunseen.\\nFormachine learning approaches, such asstochastic POS tagging, theusual technique istospilt adata setinto\\n90% training and10% testdata. Care needs tobetakenthatthetestdata isrepresentati ve.\\nForanapproach thatrelies onsigni\\x02cant hand-coding, thetestdata should beliterally unseen bytheresearchers.\\nDevelopment cycles involvelooking atsome initial data, developing thealgorithm, testing onunseen data,\\nrevising thealgorithm andtesting onanewbatch ofdata. Theseen data iskeptforregression testing.\",\n",
              " \"90% training and10% testdata. Care needs tobetakenthatthetestdata isrepresentati ve.\\nForanapproach thatrelies onsigni\\x02cant hand-coding, thetestdata should beliterally unseen bytheresearchers.\\nDevelopment cycles involvelooking atsome initial data, developing thealgorithm, testing onunseen data,\\nrevising thealgorithm andtesting onanewbatch ofdata. Theseen data iskeptforregression testing.\\nBaselines Evaluation should bereported with respect toabaseline, which isnormally what could beachie vedwith a\\nverybasic approach, giventhesame training data. Forinstance, thebaseline forPOS tagging with training data\\nistochoose themost common tagforaparticular wordonthebasis ofthetraining data (and tosimply choose\\nthemost frequent tagofallforunseen words).\\nCeiling Itisoften useful totryandcompute some sortofceiling fortheperformance ofanapplication. This isusually\\ntakentobehuman performance onthattask, where theceiling isthepercentage agreement found between two\\nannotators (interannotator agreement ).ForPOS tagging, thishasbeen reported as96% (which makesexisting\\nPOS taggers look impressi ve).Howeverthisraises lotsofquestions: relati velyuntrained human annotators\\nworking independently often havequite lowagreement, buttrained annotators discussing results canachie ve\\nmuch higher performance (approaching 100% forPOS tagging). Human performance varies considerably be-\\ntween individuals. Inanycase, human performance may notbearealistic ceiling onrelati velyunnatural tasks,\\nsuch asPOS tagging.\\nErroranalysis Theerror rateonaparticular problem willbedistrib uted veryunevenly.Forinstance, aPOS tagger\\nwillneverconfuse thetagPUN with thetagVVN (past participle), butmight confuse VVN with AJ0(adjecti ve)\\nbecause there' sasystematic ambiguity formanyforms (e.g., given ).Foraparticular application, some errors\\n24may bemore important than others. Forinstance, ifoneislooking forrelati velylowfrequenc ycases ofde-\\nnominal verbs (that isverbs derivedfrom nouns \\x97e.g., canoe ,tango ,forkused asverbs), then POS tagging is\\nnotdirectly useful ingeneral, because averbal usewithout acharacteristic af\\x02xislikelytobemistagged. This\\nmakesPOS-tagging lessuseful forlexicographers, who areoften speci\\x02cally interested in\\x02nding examples of\\nunusual worduses. Similarly ,intextcategorisation, some errors aremore important than others: e.g. treating\\nanincoming order foranexpensi veproduct asjunk email isamuch worse error than theconverse.\\nRepr oducibility Ifatallpossible, evaluation should bedone onagenerally available corpus sothatother researchers\\ncanreplicate theexperiments.\\n3.7 Further reading\\nThis lecture hasskimmed overmaterial thatiscovered inseveralchapters ofJ&M. See5.9fortheViterbi algorithm,\\nChapter 6forN-grams (especially 6.3,6.4and6.7), 7.1-7.3 forspeech recognition andChapter 8onPOS tagging.\\n254Lectur e4:Parsing andgeneration\\nInthislecture, we'lldiscuss syntax inawaywhich ismuch closer tothestandard notions informal linguistics than\\nPOS-tagging is.Tostart with, we'llbrie\\x03y motivatetheidea ofagenerati vegrammar inlinguistics, reviewthenotion\",\n",
              " \"Chapter 6forN-grams (especially 6.3,6.4and6.7), 7.1-7.3 forspeech recognition andChapter 8onPOS tagging.\\n254Lectur e4:Parsing andgeneration\\nInthislecture, we'lldiscuss syntax inawaywhich ismuch closer tothestandard notions informal linguistics than\\nPOS-tagging is.Tostart with, we'llbrie\\x03y motivatetheidea ofagenerati vegrammar inlinguistics, reviewthenotion\\nofaconte xt-free grammar andthen showaconte xt-free grammar foratinyfragment ofEnglish. We'llthen show\\nhowconte xtfreegrammars canbeused toimplement generators andparsers, anddiscuss chart parsing, which allows\\nef\\x02cient processing ofstrings containing ahigh degree ofambiguity .Finally we'llbrie\\x03y touch onprobabilistic\\nconte xt-free approaches.\\n4.1 Generati vegrammar\\nSince Chomsk y'sworkinthe1950s, much workinformal linguistics hasbeen concerned with thenotion ofagenera-\\ntivegrammar \\x97i.e.,aformally speci\\x02ed grammar thatcangenerate allandonly theacceptable sentences ofanatural\\nlanguage. It'simportant torealise thatnobody hasactually written such agrammar foranynatural language oreven\\ncome close todoing so:what most linguists arereally interested inistheprinciples thatunderly such grammars, espe-\\ncially totheextent thattheyapply toallnatural languages. NLP researchers, ontheother hand, areatleast sometimes\\ninterested inactually building andusing large-scale detailed grammars.\\nTheformalisms which areofinterest tousformodelling syntax assign internal structure tothestrings ofalanguage,\\nwhich canberepresented bybrack eting. Wealready sawsome evidence ofthisinderivational morphology (the\\nunionised example), buthere weareconcerned with thestructure ofphrases. Forinstance, thesentence:\\nthedogslept\\ncanbebrack eted\\n((the (bigdog)) slept)\\nThephrase, bigdog,isanexample ofaconstituent (i.e. something thatisenclosed inapairofbrack ets): thebigdog\\nisalso aconstituent, butthebigisnot. Constituent structure isgenerally justi\\x02ed byarguments about substitution\\nwhich Iwon'tgointohere: J&M discuss thisbrie\\x03y ,butseeanintroductory syntax book forafulldiscussion. Inthis\\ncourse, Iwillsimply givebrack eted structures andhope thattheconstituents makesense intuiti vely,rather than trying\\ntojustify them.\\nTwogrammars aresaid tobeweakly-equivalent iftheygenerate thesame strings. Twogrammars arestrongly-\\nequivalent iftheyassign thesame brack etings toallstrings theygenerate.\\nInmost, butnotall,approaches, theinternal structures aregivenlabels. Forinstance, thebigdogisanoun phrase\\n(abbre viated NP), slept ,slept inthepark andlickedSandy areverb phrases(VPs). Thelabels such asNPandVPcor-\\nrespond tonon-terminal symbols inagrammar .Inthislecture, we'lldiscuss theuseofsimple conte xt-free grammars\\nforlanguage description, moving onto amore expressi veformalism inlecture 5.\\n4.2 Context freegrammars\\nTheidea ofaconte xt-free grammar (CFG) should befamiliar from formal language theory .ACFG hasfour compo-\\nnents, described here astheyapply togrammars ofnatural languages:\\n1.asetofnon-terminal symbols (e.g., S,VP), conventionally written inuppercase;\\n2.asetofterminal symbols (i.e., thewords), conventionally written inlowercase;\",\n",
              " 'nents, described here astheyapply togrammars ofnatural languages:\\n1.asetofnon-terminal symbols (e.g., S,VP), conventionally written inuppercase;\\n2.asetofterminal symbols (i.e., thewords), conventionally written inlowercase;\\n3.asetofrules (productions), where thelefthand side (themother) isasingle non-terminal andtheright hand\\nsideisasequence ofoneormore non-terminal orterminal symbols (thedaughters);\\n4.astart symbol, conventionally S,which isamember ofthesetofnon-terminal symbols.\\n26The formal description ofaCFG generally allowsproductions with anempty righthandside (e.g., Det!\").Itis\\nconvenient toexclude these however,since theycomplicate parsing algorithms, andaweakly-equi valent grammar can\\nalwaysbeconstructed thatdisallo wssuch empty productions .\\nAgrammar inwhich allnonterminal daughters aretheleftmost daughter inarule(i.e., where allrules areoftheform\\nX!Ya\\x03),issaid tobeleft-associative .Agrammar where allthenonterminals arerightmost isright-associative .\\nSuch grammars areweakly-equi valent toregular grammars (i.e., grammars thatcanbeimplemented byFSAs), but\\nnatural languages seem torequire more expressi vepowerthan this(seex4.12).\\n4.3 Asimple CFG forafragment ofEnglish\\nThefollowing tinyfragment isintended toillustrate some oftheproperties ofCFGs sothatwecandiscuss parsing\\nandgeneration. Ithassome serious de\\x02ciencies asarepresentation ofeventhisfragment, which we\\'llignore fornow,\\nthough we\\'lldiscuss some ofthem inlecture 5.\\nS->NPVP\\nVP->VPPP\\nVP->V\\nVP->VNP\\nVP->VVP\\nNP->NPPP\\nPP->PNP\\n;;;lexicon\\nV->can\\nV->fish\\nNP->fish\\nNP->rivers\\nNP->pools\\nNP->December\\nNP->Scotland\\nNP->it\\nNP->they\\nP->in\\nTherules with terminal symbols ontheRHS correspond tothelexicon. Here andbelow,comments arepreceded by\\n;;;\\nHere aresome strings which thisgrammar generates, along with their brack etings:\\nthey\\x02sh\\n(S(NPthey)(VP(V\\x02sh)))\\ntheycan\\x02sh\\n(S(NPthey)(VP(Vcan) (VP(V\\x02sh))))\\n;;;themodal verb`areable to\\'reading\\n(S(NPthey)(VP(Vcan) (NP\\x02sh)))\\n;;;thelessplausible, put\\x02shincans, reading\\nthey\\x02shinrivers\\n(S(NPthey)(VP(VP(V\\x02sh)) (PP(Pin)(NPrivers))))\\nthey\\x02shinriversinDecember',\n",
              " \"(S(NPthey)(VP(Vcan) (NP\\x02sh)))\\n;;;thelessplausible, put\\x02shincans, reading\\nthey\\x02shinrivers\\n(S(NPthey)(VP(VP(V\\x02sh)) (PP(Pin)(NPrivers))))\\nthey\\x02shinriversinDecember\\n(S(NPthey)(VP(VP(V\\x02sh)) (PP(Pin)(NP(NPrivers)(PP(Pin)(NPDecember))))))\\n;;;i.e.theimplausible reading where theriversareinDecember\\n;;;(cfriversinScotland)\\n(S(NPthey)(VP(VP(VP(V\\x02sh)) (PP(Pin)(NP(NPrivers)))) (PP(Pin)(NPDecember))))\\n;;;i.e.the\\x02shing isdone inDecember\\n27One important thing tonotice about these examples isthatthere' slotsofpotential forambiguity .Inthetheycan\\x02sh\\nexample, thisisduetolexical ambiguity (itarises from thedual lexical entries ofcanand\\x02sh),butthelastexample\\ndemonstrates purely structur alambiguity .Inthiscase, theambiguity arises from thetwopossible attac hments ofthe\\nprepositional phrase (PP) inDecember :itcanattach totheNP(river s)ortotheVP.These attachments correspond\\ntodifferent semantics, asindicated bytheglosses. PPattachment ambiguities areamajor headache inparsing, since\\nsequences offour ormore PPsarecommon inrealtextsandthenumber ofreadings increases astheCatalan series,\\nwhich isexponential. Other phenomena havesimilar properties: forinstance, compound nouns (e.g. long-stay car\\npark shuttle bus).\\nNotice that\\x02shcould havebeen entered inthelexicon directly asaVP,butthatthiswould cause problems ifwewere\\ndoing derivational morphology ,because wewanttosaythatsuf\\x02xeslike-edapply toVs.Making river setcNPs rather\\nthan nouns isasimpli\\x02cation I'veadopted here tokeepthegrammar smaller .\\n4.4 Parse trees\\nParsetrees areequivalent tobrack eted structures, butareeasier toread forcomple xcases. Aparse treeandbrack eted\\nstructure foronereading oftheycan\\x02shinDecember isshownbelow.Thecorrespondence should beobvious.\\nS\\nNP VP\\nthey V VP\\ncan VP PP\\nV\\n\\x02shP NP\\nin December\\n(S(NPthey)(VP(Vcan) (VP(VP(V\\x02sh)) (PP(Pin)(NPDecember)))))\\n4.5 Using agrammar asarandom generator\\nThefollowing simple algorithm illustrates howagrammar canbeused togenerate sentences.\\nExpand catcategory sentence-r ecord:\\nLetpossibilities beasetcontaining alllexical items which match category andallrules with left-hand sidecategory\\nIfpossibilities isempty ,\\nthen fail\\nelse\\nRandomly select apossibility chosen from possibilities\\nIfchosen islexical,\\nthen append ittosentence-r ecord\",\n",
              " \"4.5 Using agrammar asarandom generator\\nThefollowing simple algorithm illustrates howagrammar canbeused togenerate sentences.\\nExpand catcategory sentence-r ecord:\\nLetpossibilities beasetcontaining alllexical items which match category andallrules with left-hand sidecategory\\nIfpossibilities isempty ,\\nthen fail\\nelse\\nRandomly select apossibility chosen from possibilities\\nIfchosen islexical,\\nthen append ittosentence-r ecord\\nelseexpand catoneach rhscategory inchosen (left toright) with theupdated sentence-r ecord\\nreturn sentence-r ecord\\nForinstance:\\nExpand catS()\\n28possibilities =S->NPVP\\nchosen =S->NPVP\\nExpand catNP()\\npossibilities =it,they,\\x02sh\\nchosen =\\x02sh\\nsentence-record =(\\x02sh)\\nExpand catVP(\\x02sh)\\npossibilities =VP->V,VP->VVP,VP->VNP\\nchosen =VP->V\\nExpand catV(\\x02sh)\\npossibilities =\\x02sh, can\\nchosen =\\x02sh\\nsentence-record =(\\x02sh \\x02sh)\\nObviously ,thestrings generated could bearbitrarily long. Ifinthisnaivegeneration algorithm, weexplored allthe\\nsearch space rather than randomly selecting apossible expansion, thealgorithm wouldn' tterminate.\\nReal generation operates from semantic representations, which aren' tencoded inthisgrammar ,soinwhat follows\\nwe'llconcentrate ondescribing parsing algorithms instead. However,it'simportant torealise thatCFGs are,inprin-\\nciple, bidirectional.\\n4.6 Chart parsing\\nInorder toparse with reasonable ef\\x02cienc y,weneed tokeeparecord oftherules thatwehaveapplied sothatwedon't\\nhavetobacktrack andredo workthatwe'vedone before. This works forparsing with CFGs because therules are\\nindependent oftheir conte xt:aVPcanalwaysexpand asaVandanNPregardless ofwhether ornotitwaspreceded\\nbyanNPoraV,forinstance. (Insome cases wemay beable toapply techniques thatlook attheconte xttocutdown\\nthesearch space, because wecantellthataparticular ruleapplication isnevergoing tobepartofasentence, butthisis\\nstrictly a\\x02lter: we'renevergoing togetincorrect results byreusing partial structures.) This record keeping strate gyis\\nanapplication ofdynamic programming which isused inprocessing formal languages too. InNLP thedata structure\\nused forrecording partial results isgenerally knownasachart andalgorithms forparsing using such structures are\\nreferred toaschart parsers.\\nAchart isalistofedges.Inthesimplest version ofchart parsing, each edge records aruleapplication andhasthe\\nfollowing structure:\\n[id,leftverte x,right verte x,mother category,daughter s]\\nAvertexisanintegerrepresenting apoint intheinput string, asillustrated below:\\n.they.can.fish.\\n0123\\nmother category refers totherulethathasbeen applied tocreate theedge. daughter sisalistoftheedges thatacted\\nasthedaughters forthisparticular ruleapplication: itisthere purely forrecord keeping sothattheoutput ofparsing\\ncanbealabelled brack eting.\\nForinstance, thefollowing edges would beamong those found onthechart after acomplete parse oftheycan\\x02sh\\naccording tothegrammar givenabove(idnumbering isarbitrary):\\nidleftrightmother daughters\\n31 2V (can)\\n42 3NP (fish)\",\n",
              " \"asthedaughters forthisparticular ruleapplication: itisthere purely forrecord keeping sothattheoutput ofparsing\\ncanbealabelled brack eting.\\nForinstance, thefollowing edges would beamong those found onthechart after acomplete parse oftheycan\\x02sh\\naccording tothegrammar givenabove(idnumbering isarbitrary):\\nidleftrightmother daughters\\n31 2V (can)\\n42 3NP (fish)\\n52 3V (fish)\\n62 3VP (5)\\n71 3VP (35)\\n81 3VP (34)\\n29Thedaughters fortheterminal ruleapplications aresimply theinput wordstrings. Note thatlocal ambiguities cor-\\nrespond tosituations where aparticular span hasmore than oneassociated edge. We'llseebelowthatwecanpack\\nstructures sothatweneverhavetwoedges with thesame category andthesame span, butwe'llignore thisforthe\\nmoment (seex4.9). Also, inthischart we'reonly recording complete ruleapplications: thisispassive chart parsing.\\nThemore ef\\x02cient active chart isdiscussed below,inx4.10.\\n4.7 Abottom-up passi vechart parser\\nThe following pseudo-code sketch isforaverysimple chart parser .The main function isAdd new edge which is\\ncalled foreach wordintheinput going lefttoright. Add new edge recursi velyscans backw ards looking forother\\ndaughters.\\nParse:\\nInitialise thechart (i.e., clear previous results)\\nForeach wordwordintheinput sentence, letfrombetheleftvertex,tobetheright vertexanddaughter sbe(word)\\nForeach category category thatislexically associated with word\\nAdd new edge from,to,category,daughter s\\nOutput results forallspanning edges\\n(i.e., ones thatcovertheentire input andwhich haveamother corresponding totherootcategory)\\nAdd new edge from,to,category,daughter s:\\nPutedge inchart: [id,from,to,category,daughter s]\\nForeach ruleinthegrammar ofform lhs->cat1...catn\\x001,category\\nFind setoflistsofcontiguous edges [id1,from1,to1,cat1,daughter s1]...[idn\\x001,fromn\\x001,from,catn\\x001,daughter sn\\x001]\\n(such thatto1=from2etc)\\nForeach listofedges, Add new edge from1,to,lhs,(id1...id)\\nNotice thatthismeans thatthegrammar rules areindexedbytheir rightmost category,andthattheedges inthechart\\nmust beindexedbytheir tovertex(because wescan backw ardfrom therightmost category). Consider:\\n.they.can.fish.\\n0123\\nThefollowing diagram showsthechart edges astheyareconstructed inorder (when there isachoice, taking rules in\\napriority order according totheorder theyappear inthegrammar):\\nidleftrightmother daughters\\n10 1NP (they)\\n21 2V (can)\\n31 2VP (2)\\n40 2S (13)\\n52 3V (fish)\\n62 3VP (5)\\n71 3VP (26)\\n80 3S (17)\\n92 3NP (fish)\",\n",
              " 'idleftrightmother daughters\\n10 1NP (they)\\n21 2V (can)\\n31 2VP (2)\\n40 2S (13)\\n52 3V (fish)\\n62 3VP (5)\\n71 3VP (26)\\n80 3S (17)\\n92 3NP (fish)\\n101 3VP (29)\\n110 3S (110)\\nThespanning edges are11and8:theoutput routine togivebrack eted parses simply outputs aleftbrack et,outputs\\nthecategory,recurses through each ofthedaughters andthen outputs aright brack et.So,forinstance, theoutput from\\nedge 11is:\\n(S(NPthey)(VP(Vcan)(NPfish)))\\n304.8 Adetailed trace ofthesimple chart parser\\nParse\\nword=they\\ncategories =NP\\nAdd new edge 0,1,NP,(they)\\nthey can \\x02sh1\\nMatching grammar rules are:\\nVP->VNP\\nPP->PNP\\nNomatching edges corresponding toVorP\\nword=can\\ncategories =V\\nAdd new edge 1,2,V,(can)\\nthey can \\x02sh1 2\\nMatching grammar rules are:\\nVP->V\\nsetofedge lists=f(2)g\\nAdd new edge 1,2,VP,(2)\\nthey can \\x02sh1 23\\nMatching grammar rules are:\\nS->NPVP\\nVP->VVP\\nsetofedge listscorresponding toNPVP=f(1;3)g\\nAdd new edge 0,2,S,(1,3)\\nthey can \\x02sh1 234\\nNomatching grammar rules forS\\nNoedges matching VVP\\n31word=\\x02sh\\ncategories =V,NP\\nAdd new edge 2,3,V,(\\x02sh)\\nthey can \\x02sh1 234\\n5\\nMatching grammar rules are:\\nVP->V\\nsetofedge lists=f(5)g\\nAdd new edge 2,3,VP,(5)\\nthey can \\x02sh1 234\\n56\\nMatching grammar rules are:\\nS->NPVP\\nVP->VVP\\nNoedges match NP\\nsetofedge listsforVVP=f(2;6)g\\nAdd new edge 1,3,VP,(2,6)\\nthey can \\x02sh1 234\\n567\\nMatching grammar rules are:\\nS->NPVP\\nVP->VVP\\nsetofedge listsforNPVP=f(1;7)g\\nAdd new edge 0,3,S,(1,7)\\nthey can \\x02sh1 234\\n5678\\nNomatching grammar rules forS\\nNoedges matching V\\n32Add new edge 2,3,NP,(\\x02sh)\\nthey can \\x02sh1 234\\n5678 9\\nMatching grammar rules are:\\nVP->VNP\\nPP->PNP\\nsetofedge listscorresponding toVNP=f(2;9)g',\n",
              " \"they can \\x02sh1 234\\n5678\\nNomatching grammar rules forS\\nNoedges matching V\\n32Add new edge 2,3,NP,(\\x02sh)\\nthey can \\x02sh1 234\\n5678 9\\nMatching grammar rules are:\\nVP->VNP\\nPP->PNP\\nsetofedge listscorresponding toVNP=f(2;9)g\\nAdd new edge 1,3,VP,(2,9)\\nthey can \\x02sh1 234\\n5678 910\\nMatching grammar rules are:\\nS->NPVP\\nVP->VVP\\nsetofedge listscorresponding toNPVP=f(1;10)g\\nAdd new edge 0,3,S,(1,10)\\nthey can \\x02sh1 234\\n5678 91011\\nNomatching grammar rules forS\\nNoedges corresponding toVVP\\nNoedges corresponding toPNP\\nNofurther words ininput\\nSpanning edges are8and11:Output results for8\\n33(S(NPthey)(VP(Vcan)(VP(Vfish))))\\nOutput results for11\\n(S(NPthey)(VP(Vcan)(NPfish)))\\n4.9 Packing\\nThe algorithm givenaboveisexponential inthecase where there areanexponential number ofparses. The body\\nofthealgorithm canbemodi\\x02ed sothatitruns incubic time, though producing theoutput isstillexponential. The\\nmodi\\x02cation issimply tochange thedaughters value onanedge tobeasetoflistsofdaughters andtomakeanequality\\ncheck before adding anedge sowedon'taddonethat'sequivalent toanexisting one. That is,ifweareabout toadd\\nanedge:\\n[id,leftverte x,right verte x,mother category,daughter s]\\nandthere isanexisting edge:\\n[id-old ,leftverte x,right verte x,mother category,daughter s-old ]\\nwesimply modify theoldedge torecord thenewdaughters:\\n[id-old ,leftverte x,right verte x,mother category,daughter s-oldtdaughter s]\\nThere isnoneed torecurse with thisedge, because wecouldn' tgetanynewresults.\\nFortheexample above,everything proceeds asbefore uptoedge 9:\\nidleftrightmother daughters\\n10 1NP {(they)}\\n21 2V {(can)}\\n31 2VP {(2)}\\n40 2S {(13)}\\n52 3V {(fish)}\\n62 3VP {(5)}\\n71 3VP {(26)}\\n80 3S {(17)}\\n92 3NP {(fish)}\\nHowever,rather than addedge 10,which would be:\\n101 3VP (29)\\nwematch thiswith edge 7,andsimply addthenewdaughters tothat.\\n71 3VP {(26),(29)}\",\n",
              " \"80 3S {(17)}\\n92 3NP {(fish)}\\nHowever,rather than addedge 10,which would be:\\n101 3VP (29)\\nwematch thiswith edge 7,andsimply addthenewdaughters tothat.\\n71 3VP {(26),(29)}\\nThe algorithm then terminates. Weonly haveonespanning edge (edge 8)butthedisplay routine ismore comple x\\nbecause wehavetoconsider thealternati vesetsofdaughters foredge 7.(Youshould gothrough thistoconvince\\nyourself thatthesame results areobtained asbefore.) Although inthiscase, theamount ofprocessing savedissmall,\\ntheeffects aremuch more important with longer sentences (consider hebelie vestheycan\\x02sh,forinstance).\\n4.10 Activechart parsing\\nAmore minor ef\\x02cienc yimpro vement isobtained bystoring theresults ofpartial rule applications. This isactive\\nchart parsing, socalled because thepartial edges areconsidered tobeactive:i.e.they`want'more input tomakethem\\ncomplete. Anactiveedge records theinput itexpects aswell asthedaughters ithasalready seen. Forinstance, with\\nanactivechart parser ,wemight havethefollowing edges after seeing they:\\n34idleftrightmother expected daughters\\n10 1NP {(they)}\\n20 1S VP {(1?)}\\nThedaughter mark edas?willbeinstantiated bytheedge corresponding totheVPwhen itisfound.\\n4.11 Ordering thesearchspace\\nInthepseudo-code above,theorder ofaddition ofedges tothechart wasdetermined bytherecursion. Ingeneral,\\nchart parsers makeuseofanagenda ofedges, sothatthenextedges tobeoperated onaretheones thatare\\x02rstonthe\\nagenda. Different parsing algorithms canbeimplemented bymaking thisagenda astack oraqueue, forinstance.\\nSofar,we'veconsidered bottom upparsing: analternati veistopdown parsing, where theinitial edges aregivenby\\ntherules whose mother corresponds tothestart symbol.\\nSome ef\\x02cienc yimpro vements canbeobtained byordering thesearch space appropriately ,though which version is\\nmost ef\\x02cient depends onproperties oftheindividual grammar .However,themost important reason touseanexplicit\\nagenda iswhen wearereturning parses insome sortofpriority order ,corresponding toweights ondifferent grammar\\nrules orlexical entries.\\nWeights canbemanually assigned torules andlexical entries inamanually constructed grammar .However,in\\nthelastdecade, alotofworkhasbeen done onautomatically acquiring probabilities from acorpus annotated with\\ntrees (atreebank ),either aspartofageneral process ofautomatic grammar acquisition, orasautomatically acquired\\nadditions toamanually constructed grammar .Probabilistic CFGs (PCFGs) canbede\\x02ned quite straightforw ardly ,if\\ntheassumption ismade thattheprobabilities ofrules andlexical entries areindependent ofoneanother (ofcourse\\nthisassumption isnotcorrect, buttheorderings givenseem toworkquite well inpractice). Theimportance ofthisis\\nthatwerarely wanttoreturn allparses inarealapplication, butinstead wewanttoreturn those which aretop-rank ed:\\ni.e.,themost likelyparses. This isespecially truewhen weconsider thatrealistic grammars caneasily return many\",\n",
              " \"theassumption ismade thattheprobabilities ofrules andlexical entries areindependent ofoneanother (ofcourse\\nthisassumption isnotcorrect, buttheorderings givenseem toworkquite well inpractice). Theimportance ofthisis\\nthatwerarely wanttoreturn allparses inarealapplication, butinstead wewanttoreturn those which aretop-rank ed:\\ni.e.,themost likelyparses. This isespecially truewhen weconsider thatrealistic grammars caneasily return many\\nthousands ofparses forsentences ofquite moderate length (20words orso).Ifedges areprioritised byprobability ,very\\nlowpriority edges canbecompletely excluded from consideration ifthere isacut-of fsuch thatwecanbereasonably\\ncertain thatnoedges with alowerpriority than thecut-of fwillcontrib utetothehighest-rank edparse. Limiting the\\nnumber ofanalyses under consideration isknownasbeam search(theanalogy isthatwe'relooking within abeam of\\nlight, corresponding tothehighest probability edges). Beam search islinear rather than exponential orcubic. Justas\\nimportantly ,agood priority ordering from aparser reduces theamount ofworkthathastobedone to\\x02lter theresults\\nbywhate versystem isprocessing theparser' soutput.\\n4.12 Whycan't weuseFSAs tomodel thesyntax ofnatural languages?\\nInthislecture, westarted using CFGs. This raises thequestion ofwhyweneed thismore expressi ve(and hence\\ncomputationally expensi ve)formalism, rather than modelling syntax with FSAs. One reason isthatthesyntax of\\nnatural languages cannot bedescribed byanFSA, eveninprinciple, duetothepresence ofcentr e-embedding ,i.e.\\nstructures which map to:\\nA!\\x0bA\\x0c\\nandwhich generate grammars oftheformanbn.Forinstance:\\nthestudents thepolice arrested complained\\nhasacentre-embedded structure. However,humans havedif\\x02culty processing more than twolevelsofembedding:\\n?thestudents thepolice thejournalists criticised arrested complained\\nIftherecursion is\\x02nite (nomatter howdeep), then thestrings ofthelanguage canbegenerated byanFSA. Soit'snot\\nentirely clear whether formally anFSA might notsuf\\x02ce.\\nThere' safairly extensi vediscussion ofthese issues inJ&M ,butthere aretwoessential points forourpurposes:\\n351.Grammars written using \\x02nite state techniques alone areveryhighly redundant, which makesthem verydif\\x02cult\\ntobuildandmaintain.\\n2.Without internal structure, wecan'tbuildupgood semantic representations.\\nHence theuseofmore powerful formalisms: inthenextlecture, we'lldiscuss theinadequacies ofsimple CFGs from\\nasimilar perspecti ve.\\nHowever,FSAs areveryuseful forpartial grammars which don'trequire fullrecursion. Inparticular ,forinformation\\nextraction, weneed torecognise named entities :e.g. Professor Smith, IBM, 101Dalmatians, theWhite House, the\\nAlps andsoon.Although NPs areingeneral recursi ve(theman who likesthedogwhichbites postmen ),relati ve\\nclauses arenotgenerally partofnamed entities. Also theinternal structure ofthenames isunimportant forIE.Hence\\nFSAs canbeused, with sequences such as`title surname', `DT0 PNP' etc\\nCFGs canbeautomatically compiled intoapproximately equivalent FSAs byputting bounds ontherecursion. This is\\nparticularly important inspeech recognition engines.\\n4.13 Further reading\",\n",
              " \"clauses arenotgenerally partofnamed entities. Also theinternal structure ofthenames isunimportant forIE.Hence\\nFSAs canbeused, with sequences such as`title surname', `DT0 PNP' etc\\nCFGs canbeautomatically compiled intoapproximately equivalent FSAs byputting bounds ontherecursion. This is\\nparticularly important inspeech recognition engines.\\n4.13 Further reading\\nThis lecture hascovered material which J&M discuss inchapters 9and10,though wealsotouched onPCFGs (covered\\nintheir chapter 12)andissues oflanguage comple xitywhich theydiscuss inchapter 13.J&M' sdiscussion coversthe\\nEarle yalgorithm, which canbethought ofasaform ofactivetop-do wnchart parsing. Ichose toconcentrate on\\nbottom-up parsing inthislecture, mainly because I\\x02nditeasier todescribe, butalsobecause itiseasier toseehowto\\nextend thistoPCFGs. Bottom-up parsing also seems tohavebetter practical performance with thesortofgrammars\\nwe'lllook atinlecture 5.\\nThere arealargenumber ofintroductory linguistics textbooks which coverelementary syntax anddiscuss concepts\\nsuch asconstituenc y.Forinstance, students could usefully look atthe\\x02rst\\x02vechapters ofTallerman (1998):\\nTallerman, Maggie, Under standing Syntax ,Arnold, London, 1998\\nAnalternati vewould bethe\\x02rsttwochapters ofSagandWasow(1999) \\x97copies should beintheComputer Lab-\\noratory library .This hasanarro werfocus than most other syntax books, butcoversamuch more detailed grammar\\nfragment. Thelater chapters (particularly 3and4)arerelevantforlecture 5.\\nSag, IvanA.andThomas Wasow,Syntactic Theory \\x97aformal introduction ,CSLI Publications, Stanford, CA, USA,\\n1999\\n365Lectur e5:Parsing with constraint-based grammars\\nTheCFG approach which we'velookedatsofarhassome serious de\\x02ciencies asamodel ofnatural language. Inthis\\nlecture, I'lldiscuss some ofthese andgiveanintroduction toamore expressi veformalism which iswidely used in\\nNLP,againwith thehelp ofasample grammar .Inthe\\x02rstpartofthenextlecture, Iwillalso sketch howwecanuse\\nthisapproach todocompositional semantics.\\n5.1 De\\x02ciencies inatomic category CFGs\\nIfweconsider thegrammar wesawinthelastlecture, severalproblems areapparent. One isthatthere isnoaccount\\nofagreement, so,forinstance, *it\\x02shisallowed bythegrammar aswell asthey\\x02sh.13\\nWecould, ofcourse, allowforagreement byincreasing thenumber ofatomic symbols intheCFG, introducing NP-sg,\\nNP-pl, VP-sg andVP-pl, forinstance. Butthisapproach would soon become verytedious:\\nS->NP-sgVP-sg\\nS->NP-plVP-pl\\nVP-sg->V-sgNP-sg\\nVP-sg->V-sgNP-pl\\nVP-pl->V-plNP-sg\\nVP-pl->V-plNP-pl\\nNote thatwehavetoexpand outthesymbols evenwhen there' snoconstraint onagreement, since wehavenowayof\\nsaying thatwedon'tcare about thevalue ofnumber foracategory.\\nAnother linguistic phenomenon thatwearefailing todeal with issubcate gorization .This isthelexical property that\\ntells ushowmanyargument saverbcanhave(among other things). Averbsuch asadore,forinstance, istransiti ve:a\\nsentence such as*Kim adoredisstrange, while Kim adoredSandy isusual. Averbsuch asgive isditransitive :Kim\",\n",
              " \"Another linguistic phenomenon thatwearefailing todeal with issubcate gorization .This isthelexical property that\\ntells ushowmanyargument saverbcanhave(among other things). Averbsuch asadore,forinstance, istransiti ve:a\\nsentence such as*Kim adoredisstrange, while Kim adoredSandy isusual. Averbsuch asgive isditransitive :Kim\\ngave Sandy anapple (orKim gave anapple toSandy ).Without going intodetails ofexactly howsubcate gorization is\\nde\\x02ned, orwhat anargument is,itshould beintuiti velyobvious thatwe'renotencoding thisproperty with ourCFG.\\nThegrammar allowsthefollowing, forinstance:\\nthey\\x02sh\\x02shit\\n(S(NPthey)(VP(V\\x02sh) (VP(V\\x02sh) (NPit))))\\nAgainthiscould bedealt with bymultiplying outsymbols (V-intrans, V-ditrans etc), butthegrammar becomes ex-\\ntremely cumbersome.\\nFinally ,consider thephenomenon oflong-distance dependencies ,exempli\\x02ed, forinstance, by:\\nwhich problem didyousayyoudon'tunderstand?\\nwho doyouthink Kim askedSandy tohit?\\nwhich kids didyousaywere making allthatnoise?\\nTraditionally ,these sentences aresaid tocontain `gap's,corresponding totheplace where thenoun phrase would\\nnormally appear: thegapsaremark edbyunderscores below:\\nwhich problem didyousayyoudon'tunderstand ?\\nwho doyouthink Kim askedSandy tohit?\\nwhich kids didyousay were making allthatnoise?\\nNotice that, inthethird example, theverbwereshowsplural agreement.\\nDoing thisinstandard CFGs ispossible, butextremely verbose, potentially leading tomillions ofrules. Instead of\\nhaving simple atomic categories intheCFG, wewanttoallowforfeatures onthecategories, which canhavevalues\\nindicating things likeplurality .Asthelong-distance dependenc yexamples should indicate, thefeatures need tobe\\ncomple x-valued. Forinstance,\\n13There wasalso noaccount ofcase:thisisonly re\\x03ected inafewplaces inmodern English, but*theycantheyisclearly ungrammatical (as\\nopposed totheycanthem ,which isgrammatical with thetransiti veverbuseofcan).\\n37*what kiddidyousay were making allthatnoise?\\nisnotgrammatical. The analysis needs tobeable torepresent theinformation thatthegapcorresponds toaplural\\nnoun phrase.\\nInwhat follows,Iwillillustrate asimple constr aint-based grammar formalism, using featur estructur es.Aconstraint-\\nbased grammar describes alanguage using asetofindependently stated constraints, without imposing anyconditions\\nonprocessing orprocessing order .ACFG canbetakenasanexample ofaconstraint-based grammar ,butusually the\\nterm isreserv edforricher formalisms. Thesimplest waytothink offeature structures (FSs) isthatwe'rereplacing the\\natomic categories ofaCFG with more comple xdata structures. I'll\\x02rstillustrate thisidea intuiti vely,using agrammar\\nfragment liketheoneinlecture 4butenforcing agreement. I'llthen gothrough thefeature structure formalism inmore\\ndetail. This isfollowed byanexample ofamore comple xgrammar ,which allowsforsubcate gorization (Iwon'tshow\\nhowcase andlong-distance dependencies aredealt with).\\n5.2 Averysimple FSgrammar encoding agreement\",\n",
              " 'fragment liketheoneinlecture 4butenforcing agreement. I\\'llthen gothrough thefeature structure formalism inmore\\ndetail. This isfollowed byanexample ofamore comple xgrammar ,which allowsforsubcate gorization (Iwon\\'tshow\\nhowcase andlong-distance dependencies aredealt with).\\n5.2 Averysimple FSgrammar encoding agreement\\nInaFSgrammar ,rules aredescribed asrelating FSs: i.e.,lexical entries andphrases areFSs. Inthese formalisms,\\ntheterm sign isoften used torefer tolexical entries andphrases collecti vely.Infact,rules themselv escanbetreated\\nasFSs. Feature structures aresingly-rooted directed acyclic graphs, with arcslabelled byfeatures andterminal nodes\\nassociated with values. Aparticular feature inastructure may beatomic-valued ,meaning itpoints toaterminal node\\ninthegraph, orcomple x-valued ,meaning itpoints toanon-terminal node. Asequence offeatures isknownasapath.\\nForinstance, inthestructure below,there aretwoarcs, labelled with CATand AGR,andthree nodes, with thetwo\\nterminal nodes having values noun andsg.Each ofthefeatures isthus atomic-v alued.\\nCAT-noun\\nAGR\\njsg\\nInthegraph below,thefeature HEADiscomple x-valued, andthevalue ofAGR(i.e., thevalue ofthepath HEADAGR)\\nisunspeci\\x02ed:\\nHEAD-CAT-noun\\nAGR\\nj\\nFSsareusually drawnasattrib ute-value matrices orAVMs. TheAVMs corresponding tothetwoFSsaboveareas\\nfollows:\\n\\x14\\nCATnoun\\nAGRsg\\x15\\n\"\\nHEAD\\x14\\nCATnoun\\nAGR\\x02\\x03\\x15#\\nSince FSsaregraphs, rather than trees, aparticular node may beaccessed from therootbymore than onepath: thisis\\nknownasreentr ancy .InAVMs, reentranc yisconventionally indicated byboxedintegers, with node identity indicated\\nbyintegeridentity .Theactual integers used arearbitrary .This isillustrated with anabstract example using features F\\nand Gbelow:\\n38Graph AVM\\nNon-reentrantaF:\\nG- a\\x14\\nFa\\nGa\\x15\\nReentrantF\\nzG- a\\x14\\nF0a\\nG 0\\x15\\nWhen using FSsingrammars, structures arecombined byuni\\x02cation .This means thatalltheinformation inthetwo\\nstructures iscombined. Theempty square brack ets(\\x02\\x03)inanAVM indicate thatavalue isunspeci\\x02ed: i.e.thisisa\\nnode which canbeuni\\x02ed with aterminal node (i.e., anatomic value) oracomple xvalue. More details ofuni\\x02cation\\naregivenbelow.\\nWhen FSsareused inaparticular grammar ,allsigns will haveasimilar setoffeatures (although sometimes there\\naredifferences between lexical andphrasal signs). Feature structure grammars canbeused toimplement avariety of\\nlinguistic frame works. Forthe\\x02rstexample ofaFSgrammar ,we\\'lljustconsider howagreement could beencoded.\\nSuppose wearetrying tomodel agrammar which isweakly equivalent totheCFG fragment below:\\nS->NP-sgVP-sg\\nS->NP-plVP-pl\\nVP-sg->V-sgNP-sg\\nVP-sg->V-sgNP-pl\\nVP-pl->V-plNP-sg\\nVP-pl->V-plNP-pl\\nV-pl->like\\nV-sg->likes\\nNP-sg->it\\nNP-pl->they\\nNP-sg->fish\\nNP-pl->fish\\nTheFSequivalent shownbelowsplits upthecategories sothatthemain category andtheagreement values aredistinct.\\nInthegrammar below,Ihaveused thearrownotation forrules asanabbre viation: Iwilldescribe theactual FSencoding',\n",
              " 'VP-sg->V-sgNP-pl\\nVP-pl->V-plNP-sg\\nVP-pl->V-plNP-pl\\nV-pl->like\\nV-sg->likes\\nNP-sg->it\\nNP-pl->they\\nNP-sg->fish\\nNP-pl->fish\\nTheFSequivalent shownbelowsplits upthecategories sothatthemain category andtheagreement values aredistinct.\\nInthegrammar below,Ihaveused thearrownotation forrules asanabbre viation: Iwilldescribe theactual FSencoding\\nofrules shortly .TheFSgrammar justneeds tworules. There isasingle rulecorresponding totheS->NPVPrule,\\nwhich enforces identity ofagreement values between theNPandtheVPbymeans ofreentranc y(indicated bythetag\\n1).Therulecorresponding toVP->VNPsimply makestheagreement values oftheVandtheVPthesame but\\nignores theagreement value ontheNP.14Thelexicon speci\\x02es agreement values forit,they,likeandlikes,butleaves\\ntheagreement value for\\x02shuninstantiated (i.e., underspeci\\x02ed). Note thatthegrammar alsohasarootFS:astructure\\nonly counts asavalidparse ifitisuni\\x02able with theroot.\\nFSgrammar fragment encoding agreement\\nGrammar rules\\nRule1\\x14\\nCATS\\nAGR1\\x15\\n!\\x14\\nCATNP\\nAGR1\\x15\\n,\\x14\\nCATVP\\nAGR1\\x15\\nRule2\\x14\\nCATVP\\nAGR1\\x15\\n!\\x14\\nCATV\\nAGR1\\x15\\n,\\x14\\nCATNP\\nAGR\\x02\\x03\\x15\\nLexicon:\\n;;;noun phrases\\nthey\\x14\\nCATnoun\\nAGRpl\\x15\\n14Note thatthereentranc yindicators arelocal toeach rule: the 1inrule1isnotthesame structure asthe 1inrule2.\\n39\\x02sh\"\\x14\\nCATnoun\\nAGR\\x02\\x03\\x15#\\nit\\x14\\nCATnoun\\nAGRsg\\x15\\n;;;verbs\\nlike\\x14\\nCATverb\\nAGRpl\\x15\\nlikes\\x14\\nCATverb\\nAGRsg\\x15\\nRoot structure:\\x02\\nCATS\\x03\\nConsider parsing theylikeitwith thisgrammar .Thelexical structures forlikeanditareuni\\x02ed with thecorresponding\\nstructure totheright hand side ofrule2.Both uni\\x02cations succeed, andthestructure corresponding tothemother of\\ntheruleis:\\n\\x14\\nCATVP\\nAGRpl\\x15\\nThe agreement value isplbecause ofthecoinde xation with theagreement value oflike.This structure canunify\\nwith therightmost daughter ofrule1.Thestructure fortheyisuni\\x02ed with theleftmost daughter .Rule 1says that\\nboth daughters havetohavethesame agreement value, which isthecase inthisexample. Rule application therefore\\nsucceeds andsince theresult uni\\x02es with therulestructure, there isavalidparse.\\nToseewhat isgoing onabitmore precisely ,weneed toshowtherules asFSs. There areseveralwaysofencoding\\nthis, butforcurrent purposes Iwillassume thatrules havefeatures MOTHER,DTR1,DTR2...DTRN.SoRule2, which\\nIinformally wrote as:\\n\\x14\\nCATVP\\nAGR1\\x15\\n!\\x14\\nCATV\\nAGR1\\x15\\n,\\x14\\nCATNP\\nAGR\\x02\\x03\\x15\\nisactually:\\n2\\n66666664MOTHER\\x14\\nCATVP\\nAGR1\\x15\\nDTR1\\x14\\nCATV\\nAGR1\\x15\\nDTR2\\x14\\nCATNP\\nAGR\\x02\\x03\\x153\\n77777775\\nThestructure forlikecanbeuni\\x02ed with thevalue ofDTR1intherule. Uni\\x02cation means allinformation isretained,\\nsotheresult includes theagreement value from like:\\n2\\n66666664MOTHER\\x14\\nCATVP\\nAGR1pl\\x15\\nDTR1\\x14\\nCATV\\nAGR1\\x15\\nDTR2\\x14\\nCATNP\\nAGR\\x02\\x03\\x153\\n77777775\\nThestructure foritisuni\\x02ed with thevalue forDTR2:\\n402\\n66666664MOTHER\\x14\\nCATVP\\nAGR1pl\\x15\\nDTR1\\x14\\nCATV\\nAGR1\\x15\\nDTR2\\x14\\nCATNP\\nAGRsg\\x153\\n77777775\\nTheruleapplication thus succeeds. The MOTHERvalue actsastheDTR2ofRule 1.That is:\\n\\x14\\nCATVP\\nAGRpl\\x15\\nisuni\\x02ed with theDTR2value of:\\n2\\n66666664MOTHER\\x14\\nCATS\\nAGR1\\x15\\nDTR1\\x14\\nCATNP\\nAGR1\\x15\\nDTR2\\x14\\nCATVP\\nAGR1\\x153\\n77777775\\nThis gives:\\n2\\n66666664MOTHER\\x14',\n",
              " '402\\n66666664MOTHER\\x14\\nCATVP\\nAGR1pl\\x15\\nDTR1\\x14\\nCATV\\nAGR1\\x15\\nDTR2\\x14\\nCATNP\\nAGRsg\\x153\\n77777775\\nTheruleapplication thus succeeds. The MOTHERvalue actsastheDTR2ofRule 1.That is:\\n\\x14\\nCATVP\\nAGRpl\\x15\\nisuni\\x02ed with theDTR2value of:\\n2\\n66666664MOTHER\\x14\\nCATS\\nAGR1\\x15\\nDTR1\\x14\\nCATNP\\nAGR1\\x15\\nDTR2\\x14\\nCATVP\\nAGR1\\x153\\n77777775\\nThis gives:\\n2\\n66666664MOTHER\\x14\\nCATS\\nAGR1pl\\x15\\nDTR1\\x14\\nCATNP\\nAGR1\\x15\\nDTR2\\x14\\nCATVP\\nAGR1\\x153\\n77777775\\nTheFSfortheyis:\\n\\x14\\nCATNP\\nAGRpl\\x15\\nTheuni\\x02cation ofthiswith thevalue ofDTR1succeeds butadds nonewinformation:\\n2\\n66666664MOTHER\\x14\\nCATS\\nAGR1pl\\x15\\nDTR1\\x14\\nCATNP\\nAGR1\\x15\\nDTR2\\x14\\nCATVP\\nAGR1\\x153\\n77777775\\nSimilarly ,thisstructure uni\\x02es with therootstructure, sothisisavalidparse.\\nNote however,thatifwehadtried toparse itlikeit,auni\\x02cation failure would haveoccurred, since the AGRonthe\\nlexical entry forithasthevalue sgwhich clashes with thevalue pl.\\nIhavedescribed these uni\\x02cations asoccurring inaparticular order ,butitisveryimportant tonote thatorder isnot\\nsigni\\x02cant andthatthesame overall result would havebeen obtained ifanother order hadbeen used. This means that\\ndifferent parsing algorithms areguaranteed togivethesame result. Theoneproviso isthatwith some FSgrammars,\\njustlikeCFGs, some algorithms may terminate while others donot.\\n5.3 Featur estructur esindetail\\nSofar,Ihavebeen using arather informal description ofFSs. Thefollowing section givesmore formal de\\x02nitions.\\n41FSscanbethought ofasgraphs which havelabelled arcs connecting nodes (except forthecase ofthesimplest FSs,\\nwhich consist ofasingle node with noarcs) Thelabels onthearcs arethefeatures. Arcs areregarded ashaving a\\ndirection, conventionally regarded aspointing intothestructure, awayfrom thesingle root node. Thesetoffeatures\\nandthesetofatomic values areassumed tobe\\x02nite.\\nProperties ofFSs\\nConnectedness andunique rootAFSmust haveaunique rootnode: apart from therootnode, allnodes haveoneor\\nmore parent nodes.\\nUnique featur esAnynode may havezero ormore arcs leading outofit,butthelabel oneach (that is,thefeature)\\nmust beunique.\\nNocycles Nonode may haveanarcthatpoints back totheroot node ortoanode thatinterv enes between itandthe\\nrootnode. (Some variants ofFSformalisms allowcycles.)\\nValues Anode which does nothaveanyarcsleading outofitmay haveanassociated atomic value.\\nFiniteness AnFSmust havea\\x02nite number ofnodes.\\nSequences offeatures areknownaspaths.\\nFeature structures canberegarded asbeing ordered byinformation content \\x97anFSissaidtosubsume another ifthe\\nlatter carries extrainformation. This isimportant because wede\\x02ne uni\\x02cation interms ofsubsumption.\\nProperties ofsubsumption FS1subsumes FS2ifandonly ifthefollowing conditions hold:\\nPathvalues Foreverypath PinFS1there isapath PinFS2. IfPhasavalue tinFS1, then Palsohasvalue tinFS2.\\nPathequivalences Everypairofpaths PandQwhich arereentrant inFS1 (i.e., which lead tothesame node inthe\\ngraph) arealsoreentrant inFS2.\\nUni\\x02cation corresponds toconjunction ofinformation, andthus canbede\\x02ned interms ofsubsumption, which isa\\nrelation ofinformation containment. Theuni\\x02cation oftwoFSsisde\\x02ned tobethemost general FSwhich contains\\nalltheinformation inboth oftheFSs. Uni\\x02cation willfailifthetwoFSscontain con\\x03icting information. Aswesaw',\n",
              " \"Pathequivalences Everypairofpaths PandQwhich arereentrant inFS1 (i.e., which lead tothesame node inthe\\ngraph) arealsoreentrant inFS2.\\nUni\\x02cation corresponds toconjunction ofinformation, andthus canbede\\x02ned interms ofsubsumption, which isa\\nrelation ofinformation containment. Theuni\\x02cation oftwoFSsisde\\x02ned tobethemost general FSwhich contains\\nalltheinformation inboth oftheFSs. Uni\\x02cation willfailifthetwoFSscontain con\\x03icting information. Aswesaw\\nwith thesimple grammar above,thisprevented itlikeitgetting ananalysis, because theAGRvalues con\\x03icted.\\nProperties ofuni\\x02cation Theuni\\x02cation oftwoFSs, FS1andFS2, isthemost general FSwhich issubsumed byboth\\nFS1andFS2, ifitexists.\\n5.4 Agrammar enforcing subcategorization\\nAlthough thegrammar shownaboveimpro vesonthesimple CFG, itstill doesn' tencode subcate gorization. The\\ngrammar shownoverleaf does this. Itmovesfurther awayfrom theCFG. Inparticular ,intheprevious grammar thecat\\nfeature encoded both thepart-of-speech (i.e., noun orverb)andthedistinction between thelexical sign andthephrase\\n(i.e., NvsNPandVvsVP). Inthegrammar below,theCATfeature justencodes themajor category (noun vsverb)and\\nthephrasal distinction isencoded interms ofwhether thesubcate gorization requirements havebeen satis\\x02ed. The CAT\\nand AGRfeatures arenowinside another feature head .Signs havethree features atthetop-le vel:HEAD,COMPand\\nSPR.This re\\x03ects aportion ofalinguistic frame workwhich isdescribed ingreat detail inSagandWasow(1999).15\\nBrie\\x03y ,HEADcontains information which isshared between thelexical entries andphrases ofthesame category:\\ne.g., nouns share thisinformation with thenoun phrase which dominates them inthetree, while verbs share head\\ninformation with verbphrases andsentences. SoHEADisused foragreement information andforcategory information\\n(i.e. noun, verbetc). Incontrast, COMPand SPRareabout subcate gorization: theycontain information about what\\ncancombine with thissign. Inthegrammar below,thespeci\\x02er isthesubject ofaverb,butinalargergrammar a\\ndeterminer would bethespeci\\x02er ofanoun. Forinstance, anintransiti veverbwillhaveaSPRcorresponding toits\\nsubject `slot' andavalue of\\x02lled foritsCOMP.16\\n15Youaren' texpected toknowanydetails ofthelinguistic frame workfortheexam, andyoudonothavetoremember thefeature structure\\narchitecture described here. Thepoint ofgiving thismore complicated grammar isthatitstarts todemonstrate thepowerofthefeature structure\\nframe work,inawaythatthesimple grammar using agreement does not.\\n16There' samore elegantwayofdoing thisusing lists, butsince thiscomplicates thegrammar quite alot,Iwon'tshowthishere.\\n42Thegrammar belowhasjusttworules, oneforcombining asign with itscomplement, another forcombining asign\\nwith itsspeci\\x02er .Rule 1says that, when building thephrase, the COMPvalue ofthe\\x02rstdaughter istobeequated\\n(uni\\x02ed) with thewhole structure ofthesecond daughter (indicated by2).Thehead ofthemother isequated with\\nthehead ofthe\\x02rstdaughter (1).The SPRofthemother isalsoequated with theSPRofthe\\x02rstdaughter (3).The\\nCOMPvalue ofthemother isstipulated asbeing \\x02lled :thismeans themother can'tactasthe\\x02rstdaughter inanother\",\n",
              " '(uni\\x02ed) with thewhole structure ofthesecond daughter (indicated by2).Thehead ofthemother isequated with\\nthehead ofthe\\x02rstdaughter (1).The SPRofthemother isalsoequated with theSPRofthe\\x02rstdaughter (3).The\\nCOMPvalue ofthemother isstipulated asbeing \\x02lled :thismeans themother can\\'tactasthe\\x02rstdaughter inanother\\napplication oftherule, since \\x02lled won\\'tunify with acomple xfeature structure. Thespeci\\x02er ruleisfairly similar ,in\\nthataSPR`slot\\' isbeing instantiated, although inthiscase it\\'sthesecond daughter thatcontains theslotandissharing\\nitshead information with themother .The rule also stipulates thatthe AGRvalues ofthetwodaughters havetobe\\nuni\\x02ed andthatthespeci\\x02er daughter hastohavea\\x02lled complement. These rules arecontrolled bythelexical entries\\ninthesense thatit\\'sthelexical entries which determine therequired complements andspeci\\x02er ofaword.\\nAsanexample, consider analysing they\\x02sh.Theverbentry for\\x02shcanbeuni\\x02ed with thesecond daughter position\\nofrule2,giving thefollowing partially instantiated rule:2\\n64HEAD 1\\x14\\nCATverb\\nAGR3pl\\x15\\nCOMP\\x02lled\\nSPR\\x02lled3\\n75! 22\\n64HEAD\\x14\\nCATnoun\\nAGR3\\x15\\nCOMP\\x02lled\\nSPR\\x02lled3\\n75,\"\\nHEAD 1\\nCOMP\\x02lled\\nSPR2#\\nThe\\x02rstdaughter ofthisresult canbeuni\\x02ed with thestructure forthey,which inthiscase returns thesame structure,\\nsince itadds nonewinformation. Theresult canbeuni\\x02ed with therootstructure, sothisisavalidparse.\\nOntheother hand, thelexical entry forthenoun \\x02shdoes notunify with thesecond daughter position ofrule2. The\\nentry fortheydoes notunify with the\\x02rstdaughter position ofrule1. Hence there isnoother parse.\\nSimple FSgrammar fragment encoding subcategorization\\nRule1 ;;;comp \\x02lling\"\\nHEAD 1\\nCOMP\\x02lled\\nSPR3#\\n!\"\\nHEAD 1\\nCOMP2\\nSPR3#\\n,2\\x02\\nCOMP\\x02lled\\x03\\nRule2 ;;;spr\\x02lling:\"\\nHEAD 1\\nCOMP\\x02lled\\nSPR\\x02lled#\\n! 22\\n4HEAD\\x02\\nAGR3\\x03\\nCOMP\\x02lled\\nSPR\\x02lled3\\n5,2\\n4HEAD 1\\x02\\nAGR3\\x03\\nCOMP\\x02lled\\nSPR23\\n5\\nLexicon:\\n;;;noun phrases\\nthey2\\n64HEAD\\x14\\nCATnoun\\nAGRpl\\x15\\nCOMP\\x02lled\\nSPR\\x02lled3\\n75\\n\\x02sh2\\n64HEAD\\x14\\nCATnoun\\nAGR\\x02\\x03\\x15\\nCOMP\\x02lled\\nSPR\\x02lled3\\n75\\nit2\\n64HEAD\\x14\\nCATnoun\\nAGRsg\\x15\\nCOMP\\x02lled\\nSPR\\x02lled3\\n75\\n;;;verbs\\n\\x02sh2\\n6664HEAD\\x14\\nCATverb\\nAGRpl\\x15\\nCOMP\\x02lled\\nSPRh\\nHEAD\\x02\\nCATnoun\\x03i3\\n7775\\n43can2\\n666664HEAD\\x14\\nCATverb\\nAGR\\x02\\x03\\x15\\nCOMPh\\nHEAD\\x02\\nCATverb\\x03i\\nSPRh\\nHEAD\\x02\\nCATnoun\\x03i3\\n777775;;;auxiliary verb\\ncan2\\n6666664HEAD\\x14\\nCATverb\\nAGRpl\\x15\\nCOMP\\x14\\nHEAD\\x02\\nCATnoun\\x03\\nCOMP\\x02lled\\x15\\nSPRh\\nHEAD\\x02\\nCATnoun\\x03i3\\n7777775;;;transiti veverb\\nRoot structure:2\\n4HEAD\\x02\\nCATverb\\x03\\nCOMP\\x02lled\\nSPR\\x02lled3\\n5\\n5.5 Parsing with featur estructur egrammars\\nFormally wecantreat feature structure grammars interms ofsubsumption. Iwon\\'tgivedetails here, buttheintuition\\nisthattherule FSs, thelexical entry FSsandtheroot FSallactasconstraints ontheparse, which havetobesat-\\nis\\x02ed simultaneously .This means thesystem hastobuildaparse structure which issubsumed byalltheapplicable\\nconstraints. However,thisdescription ofwhat itmeans forsomething tobeavalidparse doesn\\' tgiveanyhintofa\\nsensible algorithm.\\nThestandard approach toimplementation istousechart parsing, asdescribed intheprevious lecture, butthenotion',\n",
              " \"isthattherule FSs, thelexical entry FSsandtheroot FSallactasconstraints ontheparse, which havetobesat-\\nis\\x02ed simultaneously .This means thesystem hastobuildaparse structure which issubsumed byalltheapplicable\\nconstraints. However,thisdescription ofwhat itmeans forsomething tobeavalidparse doesn' tgiveanyhintofa\\nsensible algorithm.\\nThestandard approach toimplementation istousechart parsing, asdescribed intheprevious lecture, butthenotion\\nofagrammar rulematching anedge inthechart ismore comple x.Inanaiveimplementation, when application ofa\\ngrammar ruleischeck ed,allthefeature structures intheedges inthechart thatcorrespond tothepossible daughters\\nhavetobecopied, andthegrammar rule feature structure itself isalso copied. The copied daughter structures are\\nuni\\x02ed with thedaughter positions inthecopyoftherule, andifuni\\x02cation succeeds, thecopied structure isassociated\\nwith anewedge onthechart.\\nThe need forcopying isoften discussed interms ofthedestructi venature ofthestandard algorithm foruni\\x02cation\\n(which Iwon'tdescribe here), butthisisperhaps alittle misleading. Uni\\x02cation, howeverimplemented, involves\\nsharing information between structures. Assume, forinstance, thattheFSrepresenting thelexical entry ofthenoun\\nfor\\x02shisunderspeci\\x02ed fornumber agreement. When weparse asentence like:\\nthe\\x02shswims\\nthepartoftheFSintheresult thatcorresponds totheoriginal lexical entry willhaveitsAGRvalue instantiated. This\\nmeans thatthestructure corresponding toaparticular edge cannot bereused inanother analysis, because itwillcontain\\n`extra' information. Consider ,forinstance, parsing:\\nthe\\x02shinthelakewhich isnear thetownswim\\nApossible analysis of:\\n\\x02shinthelakewhich isnear thetown\\nis:\\n(\\x02sh (inthelake)(which isnear thetown))\\ni.e.,the\\x02sh(sg)isnear thetown.Ifweinstantiate theAGRvalue intheFSfor\\x02shassgwhile constructing thisparse,\\nandthen trytoreuse thatsame FSfor\\x02shintheother parses, analysis willfail.Hence theneed forcopying, sowecan\\nuseafresh structure each time. Copying ispotentially extremely expensi ve,because realistic grammars involveFSs\\nwith manyhundreds ofnodes.\\n44So,although uni\\x02cation isverynear tolinear incomple xity,naiveimplementations ofFSformalisms areveryin-\\nef\\x02cient. Furthermore, packing isnotstraightforw ard, because twostructures arerarely identical inrealgrammars\\n(especially ones thatencode semantics).\\nReasonably ef\\x02cient implementations ofFSformalisms cannevertheless bedeveloped. Copying canbegreatly re-\\nduced:\\n1.bydoing anef\\x02cient pretest before uni\\x02cation, sothatcopies areonly made when uni\\x02cation islikelytosucceed\\n2.bysharing parts ofFSsthataren' tchanged\\n3.bytaking advantage oflocality principles inlinguistic formalisms which limit theneed topercolate information\\nthrough structures\\nPacking canalsobeimplemented: thetesttoseeifanewedge canbepackedinvolvessubsumption rather than equality .\\nAswith CFGs, forrealef\\x02cienc yweneed tocontrol thesearch space soweonly getthemost likelyanalyses. De\\x02ning\\nprobabilistic FSgrammars inawaywhich istheoretically well-moti vated ismuch more dif\\x02cult than de\\x02ning aPCFG.\\nPractically itseems toturnoutthattreating aFSgrammar much asthough itwere aCFG works fairly well, butthisis\\nanactiveresearch issue.\\n5.6 Templates\\nThelexicon outlined abovehasthepotential tobeveryredundant. Forinstance, aswell astheintransiti veverb\\x02sh,\\nafulllexicon would haveentries forsleep ,snoreandsoon,which would beessentially identical. Weavoidthis\",\n",
              " 'Practically itseems toturnoutthattreating aFSgrammar much asthough itwere aCFG works fairly well, butthisis\\nanactiveresearch issue.\\n5.6 Templates\\nThelexicon outlined abovehasthepotential tobeveryredundant. Forinstance, aswell astheintransiti veverb\\x02sh,\\nafulllexicon would haveentries forsleep ,snoreandsoon,which would beessentially identical. Weavoidthis\\nredundanc ybyassociating names with particular feature structures andusing those names inlexical entries. For\\ninstance:\\n\\x02shINTRANS VERB\\nsleep INTRANS VERB\\nsnore INTRANS VERB\\nwhere thetemplate isspeci\\x02ed as:\\nINTRANS VERB2\\n6664HEAD\\x14\\nCATverb\\nAGRpl\\x15\\nCOMP\\x02lled\\nSPRh\\nHEAD\\x02\\nCATnoun\\x03i3\\n7775\\nThelexical entry may havesome speci\\x02c information associated with it(e.g., semantic information, seenextlecture)\\nwhich willbeexpressed asaFS:inthiscase, thetemplate andthelexical feature structure arecombined byuni\\x02cation.\\n5.7 Interface tomorphology\\nSofarwehaveassumed afull-form lexicon, butwecannowreturn totheapproach tomorphology thatwesawin\\nlecture 2,andshowhowthisrelates tofeature structures. Recall thatwehavespelling rules which canbeused to\\nanalyse awordform toreturn astem andlistofaf\\x02xesandthateach af\\x02xisassociated with anencoding ofthe\\ninformation itcontrib utes. Forinstance, theaf\\x02xsisassociated with thetemplatePLURAL_NOUN ,which would\\ncorrespond tothefollowing information inourgrammar fragment:\\n\"\\nHEAD\\x14\\nCATnoun\\nAGRpl\\x15#\\nAstem foranoun isgenerally assumed tobeuninstantiated fornumber (i.e., neutral between sgandpl).Sothelexical\\nentry forthenoun doginourfragment would be:\\n2\\n64HEAD\\x14\\nCATnoun\\nAGR\\x02\\x03\\x15\\nCOMP\\x02lled\\nSPR\\x02lled3\\n75\\n45One simple wayofimplementing in\\x03ectional morphology inFSsissimply tounify thecontrib ution oftheaf\\x02xwith\\nthatofthestem. Ifweunify theFScorresponding tothestem fordogtotheFSforPLURAL_NOUN ,weget:\\n2\\n64HEAD\\x14\\nCATnoun\\nAGRpl\\x15\\nCOMP\\x02lled\\nSPR\\x02lled3\\n75\\nThis approach assumes thatwealsohaveatemplateSINGULAR_NOUN ,where thisisassociated with a`null\\' af\\x02x.\\nInthecase ofanexample such asfeed incorrectly analysed asfee-ed,discussed inx2.5,theaf\\x02xinformation willfail\\ntounify with thestem, ruling outthatanalysis.\\nThere areother waysofencoding in\\x03ectional morphology with FS,which Iwon\\'tdiscuss here. Note thatthissimple\\napproach isnot,ingeneral, adequate forderivational morphology .Forinstance, theaf\\x02x-ize,which combines with\\nanoun toform averb(e.g., lemmatization ),cannot berepresented simply byuni\\x02cation, because ithastochange a\\nnominal form intoaverbal one. This canbeimplemented bysome form oflexical rule(which areessentially grammar\\nrules with single daughters), butIwon\\'tdiscuss thisinthiscourse. Note, however,thatthisre\\x03ects thedistinction\\nbetween in\\x03ectional andderivational morphology thatwesawinx2.2:while in\\x03ectional morphology canbeseen as\\nsimple addition ofinformation, derivational morphology convertsfeature structures into newstructures. However,\\nderivational morphology isoften nottreated asproducti ve,especially inlimited domain systems.\\n5.8 Further reading\\nJ&M describe feature structures asaugmenting aCFG rather than replacing it,butmost oftheir discussion applies\\nequally totheFSformalism I\\'veoutlined here.',\n",
              " 'simple addition ofinformation, derivational morphology convertsfeature structures into newstructures. However,\\nderivational morphology isoften nottreated asproducti ve,especially inlimited domain systems.\\n5.8 Further reading\\nJ&M describe feature structures asaugmenting aCFG rather than replacing it,butmost oftheir discussion applies\\nequally totheFSformalism I\\'veoutlined here.\\nLinGO (Linguistic Grammars Online:http://lingo.stanford.edu) distrib utes Open Source FSgrammars foravariety\\noflanguages. The LinGO English Resource Grammar (ERG) isprobably thelargest freely available bidirectional\\ngrammar .\\n466Lectur e6:Compositional andlexical semantics\\nThis lecture willgivearather super\\x02cial account ofsemantics andsome ofitscomputational aspects:\\n1.Compositional semantics infeature structure grammars\\n2.Meaning postulates\\n3.Classical lexical relations: hypon ymy,meron ymy,synon ymy,anton ymy\\n4.Taxonomies andWordNet\\n5.Classes ofpolysemy: homon ymy,regular polysemy ,vagueness\\n6.Wordsense disambiguation\\n6.1 Simple semantics infeatur estructur es\\nThegrammar fragment belowisbased ontheoneintheprevious lecture. Itisintended asarough indication ofhow\\nitispossible tobuildupsemantic representations using feature structures. Thelexical entries havebeen augmented\\nwith pieces offeature structure re\\x03ecting predicate-ar gument structure. Withthisgrammar ,theFSfortheycan\\x02sh\\nwillhaveaSEMvalue of: 2\\n666666666664PREDand\\nARG1\\x14\\nPREDpron\\nARG11\\x15\\nARG22\\n666664PREDand\\nARG1\"\\nPREDcanv\\nARG11\\nARG22#\\nARG2\\x14\\nPRED\\x02shn\\nARG12\\x153\\n7777753\\n777777777775\\nThis canbetakentobeequivalent tothelogical expression pron(x)^(canv(x;y)^\\x02shn(y))bytranslating the\\nreentranc ybetween argument positions intovariable equivalence.\\nThe most important thing tonotice ishowthesyntactic argument positions inthelexical entries arelinkedtotheir\\nsemantic argument positions. This means, forinstance, thatforthetransiti veverbcan,thesyntactic subject willalways\\ncorrespond tothe\\x02rstargument position, while thesyntactic object willcorrespond tothesecond position.\\nSimple FSgrammar with crude semantic composition\\nRule1 ;;;comp \\x02lling2\\n6664HEAD 1\\nCOMP\\x02lled\\nSPR3\\nSEM\"\\nPREDand\\nARG14\\nARG25#3\\n7775!2\\n4HEAD 1\\nCOMP2\\nSPR3\\nSEM 43\\n5,2\\x14\\nCOMP\\x02lled\\nSEM 5\\x15\\nRule2 ;;;spr\\x02lling:2\\n6664HEAD 1\\nCOMP\\x02lled\\nSPR\\x02lled\\nSEM\"\\nPREDand\\nARG14\\nARG25#3\\n7775! 22\\n64HEAD\\x02\\nAGR3\\x03\\nCOMP\\x02lled\\nSPR\\x02lled\\nSEM 43\\n75,2\\n64HEAD 1\\x02\\nAGR3\\x03\\nCOMP\\x02lled\\nSPR2\\nSEM 53\\n75\\nLexicon:\\n47can2\\n66666666666666664HEAD\\x14\\nCATverb\\nAGRpl\\x15\\nCOMP2\\n4HEAD\\x02\\nCATnoun\\x03\\nCOMP\\x02lled\\nSEM\\x02\\nINDEX 2\\x033\\n5\\nSPR\"\\nHEAD\\x02\\nCATnoun\\x03\\nSEM\\x02\\nINDEX 1\\x03#\\nSEM\"\\nPREDcanv\\nARG11\\nARG22#3\\n77777777777777775;;;transiti veverb\\n\\x02sh2\\n6666664HEAD\\x14\\nCATnoun\\nAGR\\x15\\nCOMP\\x02lled\\nSPR\\x02lled\\nSEM\"\\nINDEX 1\\nPRED\\x02shn\\nARG11#3\\n7777775;;;noun phrase\\nthey2\\n6666664HEAD\\x14\\nCATnoun\\nAGRpl\\x15\\nCOMP\\x02lled\\nSPR\\x02lled\\nSEM\"\\nINDEX 1\\nPREDpron',\n",
              " 'HEAD\\x02\\nCATnoun\\x03\\nSEM\\x02\\nINDEX 1\\x03#\\nSEM\"\\nPREDcanv\\nARG11\\nARG22#3\\n77777777777777775;;;transiti veverb\\n\\x02sh2\\n6666664HEAD\\x14\\nCATnoun\\nAGR\\x15\\nCOMP\\x02lled\\nSPR\\x02lled\\nSEM\"\\nINDEX 1\\nPRED\\x02shn\\nARG11#3\\n7777775;;;noun phrase\\nthey2\\n6666664HEAD\\x14\\nCATnoun\\nAGRpl\\x15\\nCOMP\\x02lled\\nSPR\\x02lled\\nSEM\"\\nINDEX 1\\nPREDpron\\nARG11#3\\n7777775;;;noun phrase\\nAnalternati veapproach toencoding semantics istowrite thesemantic composition rules inaseparate formalism\\nsuch astyped lambda calculus .This corresponds more closely totheapproach most commonly assumed informal\\nlinguistics: variants oflambda calculus aresometimes used inNLP,butIwon\\'tdiscuss thisfurther here.\\nIngeneral, asemantic representation constructed forasentence iscalled thelogical form ofthesentence. The se-\\nmantics shownabovecanbetakentobeequivalent toaform ofpredicate calculus without variables orquanti\\x02ers:\\ni.e.the`variables\\' intherepresentation actually correspond toconstants. Itturns outthatthisveryimpo verished\\nform ofsemantic representation isadequate formanyNLP applications: template representations, used ininformation\\nextraction orsimple dialogue systems canbethought ofasequivalent tothis. Butforafully adequate representation\\nweneed something richer \\x97forinstance, todonegation properly .Minimally weneed full\\x02rst-order predicate cal-\\nculus (FOPC). FOPC logical forms canbepassed totheorem-pro versinorder todoinference about themeaning ofa\\nsentence. However,although thisapproach hasbeen extensi velyexplored inresearch work,especially inthe1980s, it\\nhasn\\' tsofarledtopractical systems. There aremanyreasons forthis, butperhaps themost important isthedif\\x02culty\\nofacquiring detailed domain knowledge expressed inFOPC. There isalsoatheoretical AIproblem, because weseem\\ntoneed some form ofprobabilistic reasoning formanyapplications. So,although most researchers who areworking in\\ncomputational compositional semantics takesupport forinference asadesideratum, manysystems actually usesome\\nform ofshallo winference (e.g., semantic transfer inMT,mentioned inlecture 8).\\nFOPC alsohasthedisadv antage thatitforces quanti\\x02ers tobeinaparticular scopal relationship, andthisinformation\\nisnot(generally) overtinNLsentences. One classic example is:\\nEveryman lovesawoman\\nwhich isambiguous between:\\n8x[man0(x))9y[woman0(y)^love0(x;y)]]\\nandtheless-lik ely,`one speci\\x02c woman\\' reading:\\n9y[woman0(y)^8x[man0(x))love0(x;y)]]\\nMost current systems construct anunderspeci\\x02ed representation which isneutral between these readings, ifthey\\nrepresent quanti\\x02er scope atall.There areseveraldifferent alternati veformalisms forunderspeci\\x02cation.\\n486.2 Generation\\nWecangenerate from asemantic representation with asuitable FSgrammar .Producing anoutput string givenaninput\\nlogical form isgenerally referred toastactical generation orrealization ,asopposed tostrategicgeneration ortext\\nplanning ,which concerns howyoumight buildthelogical form inthe\\x02rstplace. Strate gicgeneration isanopen-ended',\n",
              " \"represent quanti\\x02er scope atall.There areseveraldifferent alternati veformalisms forunderspeci\\x02cation.\\n486.2 Generation\\nWecangenerate from asemantic representation with asuitable FSgrammar .Producing anoutput string givenaninput\\nlogical form isgenerally referred toastactical generation orrealization ,asopposed tostrategicgeneration ortext\\nplanning ,which concerns howyoumight buildthelogical form inthe\\x02rstplace. Strate gicgeneration isanopen-ended\\nproblem: itdepends verymuch ontheapplication andIwon'thavemuch tosayabout ithere. Tactical generation is\\nmore tractable, andisuseful without astrate giccomponent insome conte xts,such asthesemantic transfer approach\\ntoMT,which I'llbrie\\x03y discuss inlecture 8.\\nTactical generation canusesimilar techniques toparsing: forinstance oneapproach ischart generation which uses\\nmanyofthesame techniques aschart parsing. There hasbeen much lessworkongeneration than onparsing ingeneral,\\nandbuilding bidirectional grammars ishard: most grammars forparsing allowthrough manyungrammatical strings.\\nRecently there hasbeen some workonstatistical generation, where n-grams areused tochoose between realisations\\nconstructed byagrammar thatovergenerates. Butevenrelati vely`tight' bidirectional grammars may need touse\\nstatistical techniques inorder togenerate natural sounding utterances.\\n6.3 Meaning postulates\\nInference rules canbeused torelate open class predicates: i.e.,predicates thatcorrespond toopen class words. This is\\ntheclassic wayofrepresenting lexical meaning informal semantics within linguistics:17\\n8x[bachelor(x)$man(x)^unmarried(x)]\\nLinguistically andphilosophically ,thisgets pretty dubious. Isthecurrent Pope abachelor? Technically presumably\\nyes, butbachelor seems toimply someone who could bemarried: it'sastrange wordtoapply tothePope under\\ncurrent assumptions about celibac y.Meaning postulates arealso toounconstrained: Icould construct apredicate\\n`bachelor -weds-thurs' tocorrespond tosomeone who wasunmarried onWednesday andmarried onThursday ,butthis\\nisn'tgoing tocorrespond toawordinanynatural language. Inanycase, veryfewwords areassimple tode\\x02ne as\\nbachelor :consider howyoumight start tode\\x02ne table ,tomato orthought ,forinstance.18\\nForcomputational semantics, perhaps thebestwayofregarding meaning postulates issimply asonereasonable wayof\\nlinking compositionally constructed semantic representations toaspeci\\x02c domain. InNLP,we'renormally concerned\\nwith implication rather than de\\x02nition andthisislessproblematic philosophically:\\n8x[bachelor(x)!man(x)^unmarried(x)]\\nHowever,thebigcomputational problems with meaning postulates aretheir acquisition andthecontrol ofinference\\nonce theyhavebeen obtained. Building meaning postulates foranything other than asmall, bounded domain isan\\nAI-complete problem.\\nThemore general, shallo wer,relationships thatareclassically discussed inlexical semantics arecurrently more useful\\ninNLP,especially forbroad-co verage processing.\\n6.4 Hyponymy: IS-A\\nHypon ymy istheclassical IS-A relation: e.g. dogisahyponym ofanimal .That is,therelevantsense ofdogisthe\",\n",
              " \"AI-complete problem.\\nThemore general, shallo wer,relationships thatareclassically discussed inlexical semantics arecurrently more useful\\ninNLP,especially forbroad-co verage processing.\\n6.4 Hyponymy: IS-A\\nHypon ymy istheclassical IS-A relation: e.g. dogisahyponym ofanimal .That is,therelevantsense ofdogisthe\\nhypon ymofanimal :asnearly everything said inthislecture isabout wordsenses rather than words, Iwill avoid\\nexplicitly qualifying allstatements inthisway,butthisshould beglobally understood.\\nanimal isthehypernym ofdog.Hypon yms canbearranged intotaxonomies :classically these aretree-structured: i.e.,\\neach term hasonly onehypernym .\\nDespite thefactthathypon ymy isbyfarthemost important meaning relationship assumed inNLP,manyquestions\\narise which don'tcurrently haveverygood answers:\\n17Generally ,linguists don'tactually write meaning postulates foropen-class words, butthisisthestandard assumption about howmeaning would\\nberepresented ifanyone could bebothered todoit!\\n18There hasbeen acourt case thathinged ontheprecise meaning oftable andalso onethatdepended onwhether tomatoes were fruits or\\nvegetables.\\n491.What classes ofwords canbecategorised byhypon ymy? Some nouns, classically biological taxonomies, but\\nalsohuman artifacts, professions etcworkreasonably well. Abstract nouns, such astruth ,don'treally workvery\\nwell (theyareeither notinhypon ymic relationships atall,orveryshallo wones). Some verbs canbetreated as\\nbeing hypon yms ofoneanother \\x97e.g. murderisahyponym ofkill,butthisisnotnearly asclear asitisfor\\nconcrete nouns. Event-denoting nouns aresimilar toverbs inthisrespect. Hypon ymy isessentially useless for\\nadjecti ves.\\n2.Dodifferences inquantisation andindividuation matter? Forinstance, ischair ahypon ymoffurnitur e?isbeer\\nahypon ymofdrink ?iscoin ahypon ymofmone y?\\n3.Ismultiple inheritance allowed? Intuiti vely,multiple parents might bepossible: e.g. coin might bemetal (or\\nobject ?)andalso mone y.Artif acts ingeneral canoften bedescribed either interms oftheir form ortheir\\nfunction.\\n4.What should thetopofthehierarch ylook like?Thebest answer seems tobetosaythatthere isnosingle top\\nbutthatthere areaseries ofhierarchies.\\n6.5 Other lexical semantic relations\\nMeronymy i.e.,PART-OF\\nThe standard examples ofmeron ymy apply tophysical relationships: e.g., arm ispart ofabody (arm isa\\nmeronym ofbody );steering wheel isameron ymofcar.Note thedistinction between `part' and`piece': ifI\\nattack acarwith achainsa w,Igetpieces rather than parts!\\nSynonymy i.e.,twowords with thesame meaning (ornearly thesame meaning)\\nTruesynon yms arerelati velyuncommon: most cases oftruesynon ymy arecorrelated with dialect differences\\n(e.g., eggplant /auber gine,boot /trunk ).Often synon ymy involvesregister distinctions, slang orjargons: e.g.,\",\n",
              " 'Synonymy i.e.,twowords with thesame meaning (ornearly thesame meaning)\\nTruesynon yms arerelati velyuncommon: most cases oftruesynon ymy arecorrelated with dialect differences\\n(e.g., eggplant /auber gine,boot /trunk ).Often synon ymy involvesregister distinctions, slang orjargons: e.g.,\\npoliceman ,cop,rozzer ...Near -synon yms conveynuances ofmeaning: thin,slim,slender ,skinny .\\nAntonymy i.e.,opposite meaning\\nAnton ymy ismostly discussed with respect toadjecti ves:e.g., big/little,though it\\'sonly relevantforsome\\nclasses ofadjecti ves.\\n6.6 WordNet\\nWordNet isthemain resource forlexical semantics forEnglish thatisused inNLP \\x97primarily because ofitsvery\\nlargecoverage andthefactthatit\\'sfreely available. WordNets areunder development formanyother languages,\\nthough sofarnone areasextensi veastheoriginal.\\nTheprimary organisation ofWordNet isintosynsets :synon ymsets(near -synon yms). Toillustrate this, thefollowing\\nispartofwhat WordNet returns asan`overvie w\\'ofred:\\nwnred-over\\nOverview ofadjred\\nTheadjredhas6senses(first5fromtaggedtexts)\\n1.(43)red,reddish, ruddy,blood-red, carmine,\\ncerise, cherry, cherry-red, crimson, ruby,ruby-red,\\nscarlet --(having anyofnumerous brightorstrong\\ncolorsreminiscent ofthecolorofbloodorcherries\\nortomatoes orrubies)\\n2.(8)red,reddish --((usedofhairorfur)ofa\\nreddish browncolor;\"reddeer\";reddish hair\")\\n50Nouns inWordNet areorganised byhypon ymy,asillustrated bythefragment below:\\nSense6\\nbigcat,cat\\n=>leopard, Panthera pardus\\n=>leopardess\\n=>panther\\n=>snowleopard, ounce,Panthera uncia\\n=>jaguar, panther, Panthera onca,Felisonca\\n=>lion,kingofbeasts, Panthera leo\\n=>lioness\\n=>lionet\\n=>tiger,Panthera tigris\\n=>Bengaltiger\\n=>tigress\\n=>liger\\n=>tiglon, tigon\\n=>cheetah, chetah, Acinonyx jubatus\\n=>saber-toothed tiger,sabertooth\\n=>Smiledon californicus\\n=>falsesaber-toothed tiger\\nThefollowing isanovervie woftheinformation available inWordNet forthevarious POS classes:\\n\\x0fallclasses\\n1.synon yms (ordered byfrequenc y)\\n2.familiarity /polysemy count\\n3.compound words (done byspelling)\\n\\x0fnouns\\n1.hypon yms /hypern yms (also sisters)\\n2.holon yms /meron yms\\n\\x0fadjecti ves\\n1.anton yms\\n\\x0fverbs\\n1.anton yms\\n2.hypon yms /hypern yms (also sisters)\\n3.syntax (verysimple)\\n\\x0fadverbs\\nTaxonomies havealso been automatically orsemi-automatically extracted from machine-readable dictionaries, but\\nthese arenotdistrib uted. Microsoft\\' sMindNet isthebest knownexample (ithasmanymore relationships than just',\n",
              " \"2.holon yms /meron yms\\n\\x0fadjecti ves\\n1.anton yms\\n\\x0fverbs\\n1.anton yms\\n2.hypon yms /hypern yms (also sisters)\\n3.syntax (verysimple)\\n\\x0fadverbs\\nTaxonomies havealso been automatically orsemi-automatically extracted from machine-readable dictionaries, but\\nthese arenotdistrib uted. Microsoft' sMindNet isthebest knownexample (ithasmanymore relationships than just\\nhypon ymy). There areother collections ofterms, generally hierarchically ordered, especially medical ontologies.\\nThere havebeen anumber ofattempts tobuildanontology forworldknowledge: none ofthemore elaborate ones are\\ngenerally available. There isanongoing attempt atstandardisation ofontologies. Ontology support isanimportant\\ncomponent ofthesemantic web.\\n516.7 Using lexical semantics\\nByfarthemost commonly used lexical relation ishypon ymy.Hypon ymy relations canbeused inmanyways:\\n\\x0fSemantic classi\\x02cation: e.g., forselectional restrictions (e.g., theobject ofeathastobesomething edible) and\\nfornamed entity recognition\\n\\x0fShallo winference: `Xmurdered Y'implies `Xkilled Y'etc\\n\\x0fBack-of ftosemantic classes insome statistical approaches\\n\\x0fWord-sense disambiguation\\n\\x0fMT:ifyoucan'ttranslate aterm, substitute ahypern ym\\n\\x0fQuery expansion forinformation retrie val:ifasearch doesn' treturn enough results, oneoption istoreplace an\\nover-speci\\x02c term with ahypern ym\\nSynon ymy ornear-synon ymy isrelevantforsome ofthese reasons andalsoforgeneration. (Howeverdialect andreg-\\nister haven'tbeen investig ated much inNLP,sothepossible relevance ofdifferent classes ofsynon ymforcustomising\\ntexthasn' treally been lookedat.)\\n6.8 Polysemy\\nPolysemy refers tothestate ofawordhaving more than onesense: thestandard example isbank (riverbank) vsbank\\n(\\x02nancial institution).\\nThis ishomonymy \\x97thetwosenses areunrelated (not entirely trueforbank ,actually ,buthistorical relatedness isn't\\nactually important \\x97it'swhether ordinary speak ersofthelanguage feelthere' sarelationship). Homon ymy isthe\\nmost obvious case ofpolysemy ,butisactually relati velyinfrequent compared touses which havedifferent butrelated\\nmeanings, such asbank (\\x02nancial institution) vsbank (inacasino).\\nIfpolysemy were alwayshomon ymy,wordsenses would bediscrete: twosenses would benomore likelytoshare\\ncharacteristics than would morphologically unrelated words. Butmost senses areactually related. Regular orsys-\\ntematic polysemy concerns related butdistinct usages ofwords, often with associated syntactic effects. Forinstance,\\nstrawberry ,cherry (fruit /plant), rabbit, turkey,halib ut(meat /animal), tango, waltz (dance (noun) /dance (verb)).\\nThere arealotofcomplicated issues indeciding whether awordispolysemous orsimply general/v ague. Forinstance,\\nteacherisintuiti velygeneral between male andfemale teachers rather than ambiguous, butgiving good criteria asa\\nbasis ofthisdistinction isdif\\x02cult. Dictionaries arenotmuch help, since their decisions astowhether tosplit asense\",\n",
              " \"There arealotofcomplicated issues indeciding whether awordispolysemous orsimply general/v ague. Forinstance,\\nteacherisintuiti velygeneral between male andfemale teachers rather than ambiguous, butgiving good criteria asa\\nbasis ofthisdistinction isdif\\x02cult. Dictionaries arenotmuch help, since their decisions astowhether tosplit asense\\nortoprovide ageneral de\\x02nition areveryoften contingent onexternal factors such asthesizeofthedictionary orthe\\nintended audience, andevenwhen these factors arerelati velyconstant, lexicographers often makedifferent decisions\\nabout whether andhowtosplit upsenses.\\n6.9 Wordsense disambiguation\\nWordsense disambiguation (WSD) isneeded formost NLapplications thatinvolvesemantics (explicitly orimplicitly).\\nInlimited domains, WSD isnottoobigaproblem, butforlargecoverage textprocessing it'saserious bottleneck.\\nWSD needs depend ontheapplication \\x97there isnoobjecti venotion ofwordsense (dictionaries differextensi vely)and\\nit'sveryhard tocome upwith good criteria tojudge whether ornottodistinguish senses. Butinorder toexperiment\\nwith WSD asastandalone module, there hastobeastandard: most commonly WordNet, because itistheonly\\nextensi vemodern resource forEnglish with noproblematic IPRissues. This iscontro versial, because WordNet hasa\\nvery\\x02negranularity ofsenses \\x97it'salsoobvious thatitssenses often overlap. However,theonly current alternati ve\\nisapre-1920 version ofWebster' s.Recently WSD `competitions' havebeen organised: SENSEV ALandSENSEV AL\\n2.\\nWSD uptotheearly 1990s wasmostly done byhand-constructed rules (still used insome MTsystems). Dahlgren\\ninvestig ated WSD inafairly broad domain inthe1980s. Reasonably broad-co verage WSD generally depends on:\\n52\\x0ffrequenc y\\n\\x0fcollocations\\n\\x0fselectional restrictions/preferences\\nWhat' schanged since the1980s isthatvarious statistical ormachine-learning techniques havebeen used toavoid\\nhand-crafting rules.\\n\\x0fsupervised learning. Requires asense-tagged corpus, which isextremely time-consuming toconstruct systemat-\\nically (examples aretheSemcor andSENSEV ALcorpora, butboth arereally toosmall). Most experimentation\\nhasbeen done with asmall setofwords which canbesense-tagged bytheexperimenter (e.g., plant ).Supervised\\nlearning techniques donotcarry overwell from onecorpus toanother .\\n\\x0funsupervised learning (seebelow)\\n\\x0fMachine readable dictionaries (MRDs). Disambiguating dictionary de\\x02nitions according totheinternal data in\\ndictionaries isnecessary tobuildtaxonomies from MRDs. MRDs havealsobeen used asasource ofselectional\\npreference andcollocation information forgeneral WSD (quite successfully).\\nUntil recently ,most ofthestatistical ormachine-learning techniques havebeen evaluated onhomon yms: these are\\nrelati velyeasy todisambiguate. So95% disambiguation ine.g., Yarowsky'sexperiments sounds good (see below),\\nbutdoesn' ttranslate intohigh precision onallwords when targetisWordNet senses (inSENSEV AL2thebestsystem\\nwasaround 70%).\\nThere havealsobeen some attempts atautomatic sense induction ,where anattempt ismade todetermine theclusters\\nofusages intextsthatcorrespond tosenses. Inprinciple, thisisaverygood idea, since thewhole notion ofaword\",\n",
              " \"butdoesn' ttranslate intohigh precision onallwords when targetisWordNet senses (inSENSEV AL2thebestsystem\\nwasaround 70%).\\nThere havealsobeen some attempts atautomatic sense induction ,where anattempt ismade todetermine theclusters\\nofusages intextsthatcorrespond tosenses. Inprinciple, thisisaverygood idea, since thewhole notion ofaword\\nsense isfuzzy: wordsenses canbeargued tobeartifactsofdictionary publishing. However,sofarsense induction\\nhasnotbeen much explored inmonolingual conte xts,though itcould beconsidered asaninherent partofstatistical\\napproaches toMT.\\n6.10 Collocations\\nInformally ,acollocation isagroup oftwoormore words thatoccur together more often than would beexpected by\\nchance (there areother de\\x02nitions \\x97thisisnotreally aprecise notion). Collocations havealwaysbeen themost useful\\nsource ofinformation forWSD, eveninDahlgren' searly experiments. Forinstance:\\n(23) Striped bass arecommon.\\n(24) Bass guitars arecommon.\\nstriped isagood indication thatwe'retalking about the\\x02sh(because it'saparticular sortofbass), similarly with guitar\\nandmusic. Inboth bass guitar andstriped bass,we'vearguably gotamultiw ordexpression (i.e., aconventional phrase\\nthatmight belisted inadictionary), buttheprinciple holds foranysortofcollocation. Thebest collocates forWSD\\ntend tobesyntactically related inthesentence tothewordtobedisambiguated, butmanytechniques simply usea\\nwindo wofwords.\\nJ&M makeauseful (though non-standard) distinction between collocation andco-occurrence: co-occurrence refers to\\ntheappearance ofanother wordinalargerwindo woftextthan acollocation. Forinstance, troutmight co-occur with\\nthe\\x02shsense ofbass.\\n6.11 Yarowsky'sunsuper vised learning appr oach toWSD\\nYarowsky(1995) describes atechnique forunsupervised learning using collocates (collocates andco-occurrences in\\nJ&M' sterms). Afewseed collocates arechosen foreach sense (manually orviaanMRD), then these areused\\ntoaccurately identify distinct senses. The sentences inwhich thedisambiguated senses occur canthen beused to\\nlearn other discriminating collocates automatically ,producing adecision list. Theprocess canthen beiterated. The\\n53algorithm allowsbadcollocates tobeoverridden. This works because ofthegeneral principle of`one sense per\\ncollocation' (experimentally demonstrated byYarowsky\\x97it'snotabsolute, butthere areverystrong preferences).\\nInabitmore detail, using Yarowsky'sexample ofdisambiguating plant (which ishomon ymous between factory vs\\nvegetation senses):\\n1.Identify allexamples ofthewordtobedisambiguated inthetraining corpus andstore their conte xts.\\nsense training example\\n? compan ysaidthattheplant isstilloperating\\n? although thousands ofplant andanimal species\\n? zonal distrib ution ofplant life\\n? compan ymanuf acturing plant isinOrlando\\netc\\n2.Identify some seeds which reliably disambiguate afewofthese uses. Tagthedisambiguated senses andcount\\ntherestasresidual. Forinstance, choosing `plant life' asaseed forthevegetation sense ofplant (sense A)and\",\n",
              " \"? although thousands ofplant andanimal species\\n? zonal distrib ution ofplant life\\n? compan ymanuf acturing plant isinOrlando\\netc\\n2.Identify some seeds which reliably disambiguate afewofthese uses. Tagthedisambiguated senses andcount\\ntherestasresidual. Forinstance, choosing `plant life' asaseed forthevegetation sense ofplant (sense A)and\\n`manuf acturing plant' astheseed forthefactory sense (sense B):\\nsense training example\\n? compan ysaidthattheplant isstilloperating\\n? although thousands ofplant andanimal species\\nA zonal distrib ution ofplant life\\nB compan ymanuf acturing plant isinOrlando\\netc\\nThis disambiguated 2%ofuses inYarowsky'scorpus, leaving 98% residual.\\n3.Trainadecision listclassi\\x02er ontheSense A/Sense Bexamples. Adecision listapproach givesalistofcriteria\\nwhich aretried inorder until anapplicable testisfound: thisisthen applied. Thetests areeach associated with\\nareliability metric. Theoriginal seeds arelikelytobeatthetopoftheinitial decision list,followed byother\\ndiscriminating terms. e.g.thedecision listmight include:\\nreliability criterion sense\\n8.10 plant life A\\n7.58 manuf acturing plant B\\n6.27 animal within 10words ofplant A\\netc\\n4.Apply thedecision listclassi\\x02er tothetraining setandaddallexamples which aretagged with greater than a\\nthreshold reliability totheSense AandSense Bsets.\\nsense training example\\n? compan ysaidthattheplant isstilloperating\\nA although thousands ofplant andanimal species\\nA zonal distrib ution ofplant life\\nB compan ymanuf acturing plant isinOrlando\\netc\\n5.Iterate theprevious steps 3and4until convergence\\n6.Apply theclassi\\x02er totheunseen testdata\\nYarowskyalso demonstrated theprinciple of`one sense perdiscourse' (again,averystrong, butnotabsolute effect).\\nThis canbeused asanadditional re\\x02nement forthealgorithm above.\\nYarowskyargues thatdecision listsworkbetter than manyother statistical frame works because noattempt ismade to\\ncombine probabilities. This would becomple x,because thecriteria arenotindependent ofeach other .\\nYarowsky'sexperiments were nearly allonhomon yms: these principles probably don'thold aswell forsense exten-\\nsion.\\n546.12 Evaluation ofWSD\\nThebaseline forWSD isgenerally `pick themost frequent' sense: thisishard tobeat! However,inmanyapplications,\\nwedon'tknowthefrequenc yofsenses.\\nSENSEV ALandSENSEV AL-2 evaluated WSD inmultiple languages, with various criteria, butgenerally using Word-\\nNetsenses forEnglish. Thehuman ceiling forthistaskvaries considerably between words: probably partly because of\\ninherent differences insemantic distance between groups ofuses andpartly because ofWordNet itself, which some-\\ntimes makesvery\\x02ne-grained distinctions. Aninteresting variant inSENSEV AL-2 wastodooneexperiment onWSD\\nwhere thedisambiguation waswith respect touses requiring different translations intoJapanese. This hastheadvan-\\ntage thatitisuseful andrelati velyobjecti ve,butsometimes thistaskrequires splitting terms which aren' tpolysemous\\ninEnglish (e.g., water \\x97hotvscold). Performance ofWSD onthistask seems abitbetter than thegeneral WSD\\ntask.\",\n",
              " \"where thedisambiguation waswith respect touses requiring different translations intoJapanese. This hastheadvan-\\ntage thatitisuseful andrelati velyobjecti ve,butsometimes thistaskrequires splitting terms which aren' tpolysemous\\ninEnglish (e.g., water \\x97hotvscold). Performance ofWSD onthistask seems abitbetter than thegeneral WSD\\ntask.\\n6.13 Further reading\\nJ&M gointoquite alotofdetail about compositional semantics including underspeci\\x02cation.\\nWordNet isfreely downloadable: thewebsite haspointers toseveralpapers which provide agood introduction.\\nForalotmore detail ofWSD than provided byJ&M, seeManning andSch¨utze who haveaverydetailed account of\\nWSD andword-sense induction:\\nManning, Christopher andHinrich Sch¨utze (1999), Foundations ofStatistical Natur alLangua geProcessing ,MIT\\nPress\\nYarowsky'spaper iswell-written andshould beunderstandable:\\nYarowsky,David(1995)\\nUnsupervised wordsense disambiguation rivalling supervised methods ,\\nProceedings ofthe33rd Annual Meeting oftheAssociation forComputational Linguistics (ACL-95) MIT, 189\\x96196\\nLikemanyother recent NLP papers, thiscanbedownloaded viawww .citeseer .com\\n557Lectur e7:Discourse\\nUtterances arealwaysunderstood inaparticular conte xt.Conte xt-dependent situations include:\\n1.Referring expressions: pronouns, de\\x02nite expressions etc.\\n2.Universe ofdiscourse: every dogbarked,doesn' tmean everydogintheworld butonly everydoginsome\\nexplicit orimplicit conte xtual set.\\n3.Responses toquestions, etc:only makesense inaconte xt:Who came totheparty? NotSandy .\\n4.Implicit relationships between events: Max fell.Johnpushed him\\x97thesecond sentence is(usually) understood\\nasproviding acausal explanation.\\nInthe\\x02rst part ofthislecture, Igiveabrief overvie wofrhetorical relations which canbeseen asstructuring text\\natalevelabovethesentence. I'llthen goontotalkabout oneparticular case ofconte xt-dependent interpretation \\x97\\nanaphor resolution. Iwilldescribe analgorithm foranaphor resolution which uses arelati velybroad-co verage shallo w\\nparser andthen discuss avariant ofitthatrelies onPOS-tagging andregular expression matching rather than parsing.\\n7.1 Rhetorical relations andcoher ence\\nConsider thefollowing discourse:\\nMax fell. John pushed him.\\nThis discourse canbeinterpreted inatleast twoways:\\n1.Max fellbecause John pushed him.\\n2.Max fellandthen John pushed him.\\nThere seems tobeanimplicit relationship between thetwooriginal sentences: adiscour serelation orrhetorical\\nrelation .(Iwillusetheterms interchangeably here, though different theories usedifferent terminology ,andrhetorical\\nrelation tends torefer toamore surfacyconcept than discourse relation.) In1thelinkisaform ofexplanation, but2is\\nanexample ofnarration. Theories ofdiscourse/rhetorical relations reify linktypes such asExplanation andNarr ation .\\nTherelationship ismade more explicit in1and2than itwasintheoriginal sentence: because andandthen aresaid\\ntobecuephrases.\\n7.2 Coher ence\\nDiscourses havetohaveconnecti vitytobecoherent:\\nKim gotintohercar.Sandy likesapples.\\nBoth ofthese sentences makeperfect sense inisolation, buttakentogether theyareincoherent. Adding conte xtcan\\nrestore coherence:\",\n",
              " \"Therelationship ismade more explicit in1and2than itwasintheoriginal sentence: because andandthen aresaid\\ntobecuephrases.\\n7.2 Coher ence\\nDiscourses havetohaveconnecti vitytobecoherent:\\nKim gotintohercar.Sandy likesapples.\\nBoth ofthese sentences makeperfect sense inisolation, buttakentogether theyareincoherent. Adding conte xtcan\\nrestore coherence:\\nKim gotintohercar.Sandy likesapples, soKim thought she'dgotothefarmshop andseeifshecould\\ngetsome.\\nThesecond sentence canbeinterpreted asanexplanation ofthe\\x02rst. Inmanycases, thiswillalsoworkiftheconte xt\\nisknown,evenifitisn'texpressed.\\nStrate gicgeneration requires awayofimplementing coherence. Forexample, consider asystem thatreports share\\nprices. This might generate:\\nIntrading yesterday: Dell wasup4.2%, Safewaywasdown3.2%, Compaq wasup3.1%.\\n56This ismuch lessacceptable than aconnected discourse:\\nComputer manuf acturers gained intrading yesterday: Dell wasup4.2% andCompaq wasup3.1%. But\\nretail stocks suffered: Safewaywasdown3.2%.\\nHere butindicates aContrast. Notmuch actual information hasbeen added (assuming weknowwhat sortofcompan y\\nDell, Compaq andSafewayare), butthediscourse iseasier tofollow.\\nDiscourse coherence assumptions canaffectinterpretation:\\nJohn likesBill. Hegavehimanexpensi veChristmas present.\\nIfweinterpret thisasExplanation, then `he' ismost likelyBill. ButifitisJusti\\x02cation (i.e., thespeak erisjustifying\\nthe\\x02rstsentence), then `he'isJohn.\\n7.3 Factors in\\x03uencing discourse inter pretation\\n1.Cuephrases. These aresometimes unambiguous, butnotusually .e.g.andisacuephrase when used insentential\\norVPconjunction.\\n2.Punctuation (also prosody) andtextstructure. Forinstance, parenthetical information cannot berelated toa\\nmain clause byNarration, butalistisoften interpreted asNarration:\\nMax fell(John pushed him) andKim laughed.\\nMax fell,John pushed himandKim laughed.\\nSimilarly ,enumerated listscanindicate aform ofnarration.\\n3.Real worldcontent:\\nMax fell. John pushed himashelayontheground.\\n4.Tense andaspect.\\nMax fell. John hadpushed him.\\nMax wasfalling. John pushed him.\\nItshould beclear thatitispotentially veryhard toidentify rhetorical relations. Infact,recent research thatsimply\\nuses cuephrases andpunctuation isproving quite promising. This canbedone byhand-coding aseries of\\x02nite-state\\npatterns, orbyaform ofsupervised learning.\\n7.4 Discourse structur eandsummarization\\nIfweconsider adiscourse relation asarelationship between twophrases, wegetabinary branching treestructure for\\nthediscourse. Inmanyrelationships, such asExplanation, onephrase depends ontheother: e.g., thephrase being\\nexplained isthemain oneandtheother issubsidiary .Infactwecangetridofthesubsidiary phrases andstillhave\\nareasonably coherent discourse. (The main phrase issometimes called thenucleus andthesubsidiary oneisthe\\nsatellite .)This canbeexploited insummarization.\\nForinstance:\\nWegetabinary branching treestructure forthediscourse. Inmanyrelationships onephrase depends on\\ntheother .Infactwecangetridofthesubsidiary phrases andstillhaveareasonably coherent discourse.\\nOther relationships, such asNarration, giveequal weight toboth elements, sodon'tgiveanyclues forsummarization.\",\n",
              " \"satellite .)This canbeexploited insummarization.\\nForinstance:\\nWegetabinary branching treestructure forthediscourse. Inmanyrelationships onephrase depends on\\ntheother .Infactwecangetridofthesubsidiary phrases andstillhaveareasonably coherent discourse.\\nOther relationships, such asNarration, giveequal weight toboth elements, sodon'tgiveanyclues forsummarization.\\nRather than trying to\\x02ndrhetorical relations forarbitrary text,genre-speci\\x02c cues canbeexploited, forinstance for\\nscienti\\x02c texts.This allowsmore detailed summaries tobeconstructed.\\n577.5 Referring expr essions\\nI'llnowmoveontotalking about another form ofdiscourse structure, speci\\x02cally thelinkbetween referring expres-\\nsions. Thefollowing example willbeused toillustrate referring expressions andanaphora resolution:\\nNiall Ferguson isproli\\x02c, well-paid andasnapp ydresser .Stephen Moss hated him\\x97atleast until he\\nspent anhour being charmed inthehistorian' sOxford study .(quote takenfrom theGuardian)\\nSome terminology:\\nreferentarealworld entity thatsome piece oftext(orspeech) refers to.e.g., thetwopeople who arementioned in\\nthisquote.\\nreferring expr essions bitsoflanguage used toperform reference byaspeak er.In,theparagraph above,Niall Fergu-\\nson,himandthehistorian areallbeing used torefer tothesame person (theycorefer).\\nantecedent thetextevoking areferent. Niall Ferguson istheantecedent ofhimandthehistorian\\nanaphora thephenomenon ofreferring toanantecedent: himandthehistorian areanaphoric because theyrefer toa\\npreviously introduced entity .\\nWhat about asnappy dresser ?Traditionally ,thiswould bedescribed aspredicati ve:thatis,itisapredicate, likean\\nadjecti ve,rather than being areferring expression itself.\\nGenerally ,entities areintroduced inadiscourse (technically ,evoked)byinde\\x02nite noun phrases orproper names.\\nDemonstrati vesandpronouns aregenerally anaphoric. De\\x02nite noun phrases areoften anaphoric (asabove),butoften\\nused tobring amutually knownanduniquely identi\\x02able entity intothecurrent discourse. e.g., thepresident ofthe\\nUS.\\nSometimes, pronouns appear before their referents areintroduced: thisiscataphor a.E.g., atthestart ofadiscourse:\\nAlthough shecouldn' tseeanydogs, Kim wassure she'dheard barking.\\nboth cases ofsherefer toKim -the\\x02rstisacataphor .\\n7.6 Pronoun agreement\\nPronouns generally havetoagree innumber andgender with their antecedents. Incases where there' sachoice of\\npronoun, such ashe/sheoritforananimal (orababy,insome dialects), then thechoice hastobeconsistent.\\n(25) Alittle girlisatthedoor \\x97seewhat shewants, please?\\n(26) Mydoghashurthisfoot\\x97heisinalotofpain.\\n(27) *Mydoghashurthisfoot\\x97itisinalotofpain.\\nComplications include thegender neutral they(some dialects), useoftheywith everybody ,group nouns, conjunctions\\nanddiscontinuous sets:\\n(28) Somebody' satthedoor \\x97seewhat theywant,willyou?\\n(29) Idon'tknowwho thenewteacher willbe,butI'msure they'llmakechanges tothecourse.\\n(30) Everybody' scoming totheparty ,aren' tthey?\",\n",
              " \"anddiscontinuous sets:\\n(28) Somebody' satthedoor \\x97seewhat theywant,willyou?\\n(29) Idon'tknowwho thenewteacher willbe,butI'msure they'llmakechanges tothecourse.\\n(30) Everybody' scoming totheparty ,aren' tthey?\\n(31) Theteam played really well, butnowtheyareallverytired.\\n(32) Kim andSandy areasleep: theyareverytired.\\n(33) Kim issnoring andSandy can'tkeephereyesopen: theyareboth exhausted.\\n587.7 Re\\x03exi ves\\n(34) Johnicuthimselfishaving. (himself =John, subscript notation used toindicate this)\\n(35) #Johnicuthimjshaving. (i6=j\\x97averyoddsentence)\\nTheinformal andnotfully adequate generalisation isthatre\\x03exivepronouns must beco-referential with apreceding\\nargument ofthesame verb(i.e., something itsubcate gorizes for), while non-re\\x03e xivepronouns cannot be.Inlinguis-\\ntics,thestudy ofinter-sentential anaphora isknownasbinding theory :Iwon'tdiscuss thisfurther ,since theconstraints\\nonreference involvedarequite different from those with intra-sentential anaphora.\\n7.8 Pleonastic pronouns\\nPleonastic pronouns aresemantically empty ,anddon'trefer:\\n(36) Itissnowing\\n(37) Itisnoteasy tothink ofgood examples.\\n(38) Itisobvious thatKim snores.\\n(39) Itbothers Sandy thatKim snores.\\nNote also:\\n(40) Theyaredigging upthestreet again\\nThis isan(informal) useoftheywhich, though probably nottechnically pleonastic, doesn' tapparently refer toa\\ndiscourse referent inthestandard way(they=`theauthorities'??).\\n7.9 Salience\\nThere areanumber ofeffects which cause particular pronoun referents tobepreferred, after allthehard constraints\\ndiscussed abovearetakenintoconsideration.\\nRecency More recent referents arepreferred. Only relati velyrecently referred toentities areaccessible.\\n(41) Kim hasafastcar.Sandy hasanevenfaster one. Leelikestodriveit.\\nitpreferentially refers toSandy' scar,rather than Kim' s.\\nGrammatical roleSubjects>objects>everything else:\\n(42) Fred went totheGrafton Centre with Bill. Hebought aCD.\\nheismore likelytobeinterpreted asFred than asBill.\\nRepeated mention Entities thathavebeen mentioned more frequently arepreferred:\\n(43) Fred wasgetting bored. Hedecided togoshopping. Billwent totheGrafton Centre with Fred. He\\nbought aCD.\\nHe=Fred (maybe) despite thegeneral preference forsubjects.\\nParallelism Entities which share thesame roleasthepronoun inthesame sortofsentence arepreferred:\\n(44) Billwent with Fred totheGrafton Centre. Kim went with himtoLion Yard.\\nHim=Fred, because theparallel interpretation ispreferred.\\n59Coher ence effects Thepronoun resolution may depend ontherhetorical/discourse relation thatisinferred.\\n(45) BilllikesFred. Hehasagreat sense ofhumour .\",\n",
              " \"(44) Billwent with Fred totheGrafton Centre. Kim went with himtoLion Yard.\\nHim=Fred, because theparallel interpretation ispreferred.\\n59Coher ence effects Thepronoun resolution may depend ontherhetorical/discourse relation thatisinferred.\\n(45) BilllikesFred. Hehasagreat sense ofhumour .\\nHe=Fred preferentially ,possibly because thesecond sentence isinterpreted asanexplanation ofthe\\x02rst, and\\nhaving asense ofhumour isseen asareason tolikesomeone.\\n7.10 Algorithms forresolving anaphora\\nMost workhasgone intotheproblem ofresolving pronoun referents. Aswell asdiscourse understanding, thisisoften\\nimportant inMT.Forinstance, English ithastoberesolv edtotranslate intoGerman because German hasgrammatical\\ngender (though note, ifthere aretwopossible antecedents, butboth havethesame gender ,weprobably donotneed\\ntoresolv ebetween thetwoforMT). Iwilldescribe oneapproach toanaphora resolution andamodi\\x02cation ofitthat\\nrequires fewerresources.\\n7.11 Lappin andLeass (1994)\\nThe algorithm relies onparsed text(from afairly shallo w,verybroad-co verage parser ,which unfortunately isn't\\ngenerally available). Thetextthesystem wasdeveloped andtested onwasallfrom online computer manuals. The\\nfollowing description isalittle simpli\\x02ed:\\nThediscourse model consists ofasetofreferring NPs arranged intoequivalence classes, each class having aglobal\\nsalience value.\\nForeach sentence:\\n1.Divide bytwotheglobal salience factors foreach existing equivalence class.\\n2.Identify referring NPs (i.e., exclude pleonastic itetc)\\n3.Calculate global salience factors foreach NP(seebelow)\\n4.Update thediscourse model with thereferents andtheir global salience scores.\\n5.Foreach pronoun:\\n(a)Collect potential referents (cutoffisfour sentences back).\\n(b)Filter referents according tobinding theory andagreement constraints.\\n(c)Calculate theperpronoun adjustments foreach referent (seebelow).\\n(d)Select thereferent with thehighest salience value foritsequivalence class plus itsper-pronoun adjustment.\\nIncase ofatie,prefer theclosest referent inthestring.\\n(e)Add thepronoun intotheequivalence class forthatreferent, andincrement thesalience factor bythe\\nnon-duplicate salience factors pertaining tothepronoun.\\nThe salience factors were determined experimentally .Global salience factors mostly takeaccount ofgrammatical\\nfunction \\x97theyencode thehierarch ymentioned previously .Theygivelowest weight toanNPinanadverbial\\nposition, such asinside anadjunct PP.This isachie vedbygiving everynon-adv erbial anextrapositi vescore, because\\nwewantallglobal salience scores tobepositi veintegers. Embedded NPs arealsodownweighted bygiving apositi ve\\nscore tonon-embedded NPs. Recenc yweights mean thatintra-sentential binding ispreferred.\\nGlobal salience factors.\\nrecenc y 100\\nsubject 80\\nobjects ofexistential sentences 70\\ndirect object 50\\nindirect object 40\\noblique complement 40\\nnon-embedded noun 80\\nother non-adv erbial 50\\n`Existential objects' refers toNPs which areinsyntactic object position insentences such as:\\n60There isacatinthegarden.\",\n",
              " \"Global salience factors.\\nrecenc y 100\\nsubject 80\\nobjects ofexistential sentences 70\\ndirect object 50\\nindirect object 40\\noblique complement 40\\nnon-embedded noun 80\\nother non-adv erbial 50\\n`Existential objects' refers toNPs which areinsyntactic object position insentences such as:\\n60There isacatinthegarden.\\nHere acatissyntactically anobject, butfunctions more likeasubject, while there,which issyntactically thethe\\nsubject, does notrefer.Anoblique complement isacomplement other than anoun phrase, such asaPP.\\nTheper-pronoun modi\\x02cations havetobecalculated each time acandidate pronoun isbeing evaluated. Themodi\\x02-\\ncations strongly disprefer cataphora andslightly prefer referents which are`parallel', where parallel here justmeans\\nhaving thesame syntactic role.\\nPerpronoun salience factors:\\ncataphora -175\\nsame role 35\\nApplying thistothesample discourse:\\nNiall Ferguson isproli\\x02c, well-paid andasnapp ydresser .\\nStephen Moss hated him\\x97atleast until hespent anhour being charmed inthehistorian' sOxford study .\\nAssume wehaveprocessed upto`\\x97' andareresolving he.Discourse referents:\\nN Niall Ferguson ,him 435\\nS Stephen Moss 310\\nIamassuming thatasnappy dresser isignored, although itmight actually betreated asanother potential referent,\\ndepending ontheparser .\\nNhasscore 155+280((subject +non-embedded +non-adv erbial +recenc y)/2+(direct object +head +non-adv erbial\\n+recenc y))\\nShasscore 310(subject +non-embedded +non-adv erbial +recenc y)+same roleper-pronoun 35\\nSointhiscase, thewrong candidate wins.\\nWenowaddhetothediscourse referent equivalence class. Theadditional weight isonly 80,forsubject, because we\\ndon'taddweights foragivenfactor more than once inasentence.\\nN Niall Ferguson ,him,he 515\\nNote thatthewrong result isquite plausible:\\nNiall Ferguson isproli\\x02c, well-paid andasnapp ydresser .\\nStephen Moss hated him\\x97atleast until hespent anafternoon being intervie wed atveryshort notice.\\nTheoverall performance ofthealgorithm reported byLappin andLeass was86% butthiswasoncomputer manuals\\nalone. Their results can'tbedirectly replicated, duetotheir useofaproprietary parser ,butother experiments suggest\\nthattheaccurac yonother types oftextcould belower.\\n7.12 Anaphora foreveryone\\nItispotentially important toresolv eanaphoric expressions, evenfor`shallo w'NLP tasks, such asWebsearch, where\\nfullparsing isimpractical. Anarticle which mentions thename `Niall Ferguson' once, butthen hasmultiple uses of\\n`he', `thehistorian' etcreferring tothesame person ismore relevanttoasearch for`Niall Ferguson' than onewhich\\njustmentions thename once. Itistherefore interesting toseewhether analgorithm canbedeveloped which does not\\nrequire parsed text.Kennedy andBogurae v(1996) describe avariant ofLappin andLeass which wasdeveloped for\\ntextwhich hadjustbeen tagged forpart-of-speech.\",\n",
              " \"justmentions thename once. Itistherefore interesting toseewhether analgorithm canbedeveloped which does not\\nrequire parsed text.Kennedy andBogurae v(1996) describe avariant ofLappin andLeass which wasdeveloped for\\ntextwhich hadjustbeen tagged forpart-of-speech.\\nTheinput textwastagged with theLingsoft tagger (averyhigh precision andrecall tagger thatuses manually developed\\nrules: seehttp://www.lingsoft.fi/demos.html ).Besides POS tags, thisgivessome grammatical function\\ninformation: e.g., itnotates subjects (forEnglish, thisisquite easy todoonthebasis ofPOS-tagged textwith some\\nsimple regular expressions). Thetextwasthen runthrough aseries ofregular expression \\x02lters toidentify NPs and\\nmark expleti veit.Heuristics de\\x02ned asregular expressions arealsoused toidentify theNPs grammatical role. Global\\nsalience factors areasinLappin andLeass, butKennedy andBogurae vaddafactor forconte xt(asdetermined bya\\ntextsegmentation algorithm). Theyalsouseadistinct factor forpossessi veNPs.\\nBecause thisalgorithm doesn' thaveaccess toaparser ,theimplementation ofbinding theory hastorelyonheuristics\\nbased ontherolerelationships identi\\x02ed. Otherwise, thealgorithm ismuch thesame asforLappin andLeass.\\n61Overall accurac yisquoted as75%, measured onamixture ofgenres (soitisn'tpossible todirectly compare with\\nLappin andLeass, since thatwasonly tested oncomputer manual information). Fewerrors were caused bythelack\\nofdetailed syntactic information. 35% oferrors were caused byfailure toidentify gender correctly ,14% were caused\\nbecause quoted conte xtsweren' thandled.\\n7.13 Another note onevaluation\\nThesituation with respect toevaluation ofanaphora resolution islesssatisf actory than POS tagging orWSD. This is\\npartly because oflackofevaluation materials such asindependently mark ed-up corpora. Another factor isthedif\\x02culty\\ninreplication: e.g., Lappin andLeass' salgorithm can'tbefully replicated because oflack ofavailability oftheparser .\\nThis canbepartially circumv ented byevaluating algorithms ontreebanks, butexisting treebanks arerelati velylimited\\ninthesortoftexttheycontain. Alternati vely,different parsers canbecompared according totheaccurac ywith which\\ntheysupply thenecessary information, butagainthisrequires asuitable testing environment.\\n7.14 Further reading\\nJ&M discuss themost popular approach torhetorical relations, rhetorical structur etheory orRST.Ihaven'tdiscussed\\nitindetail here, partly because I\\x02ndthetheory veryunclear: attempts toannotate textusing RST approaches tend\\nnottoyield good interannotator agreement (see comments onevaluation inlecture 3),although tobefair,thisisa\\nproblem with allapproaches torhetorical relations. Thediscussion ofthefactors in\\x03uencing anaphora resolution and\\nthedescription oftheLappin andLeass algorithm thatI'vegivenhere arepartly based onJ&M' saccount.\\nThereferences belowareforcompleteness rather than suggested reading:\\nLappin, Shalom andHerb Leass (1994)\\nAnalgorithm forpronominal anaphor aresolution ,\\nComputational Linguistics 20(4), 535\\x96561\\nKennedy ,Christopher andBranimir Bogurae v(1996)\\nAnaphor aforeveryone: pronominal anaphor aresolution without aparser,\",\n",
              " \"Thereferences belowareforcompleteness rather than suggested reading:\\nLappin, Shalom andHerb Leass (1994)\\nAnalgorithm forpronominal anaphor aresolution ,\\nComputational Linguistics 20(4), 535\\x96561\\nKennedy ,Christopher andBranimir Bogurae v(1996)\\nAnaphor aforeveryone: pronominal anaphor aresolution without aparser,\\nProceedings ofthe16th International Conference onComputational Linguistics (COLING 96), Copenhagen, Den-\\nmark, 113\\x96118\\n628Lectur e8:Applications\\nThis lecture considers three applications ofNLP: machine translation, spok endialogue systems andemail response.\\nThis isn'tintended asacomplete overvie wofthese areas, butjustasawayofdescribing howsome ofthetechniques\\nwe'veseen intheprevious lectures arebeing used incurrent systems orhowtheymight beused inthefuture.\\nMachine translation\\n8.1 Methodology forMT\\nThere arefour main classical approaches toMT:\\n\\x0fDirect transfer: map between morphologically analysed structures.\\n\\x0fSyntactic transfer: map between syntactically analysed structures.\\n\\x0fSemantic transfer: map between semantics structures.\\n\\x0fInterlingua: construct alanguage-neutral representation from parsing andusethisforgeneration.\\nThe standard illustration ofthedifferent classical approaches toMTistheVauquois triangle. This issupposed to\\nillustrate theamount ofeffortrequired foranalysis andgeneration asopposed totransfer inthedifferent approaches.\\ne.g., direct transfer requires verylittle effortforanalysis orgeneration, since itsimply involvesmorphological analysis,\\nbutitrequires more effortontransfer than syntactic orsemantic transfer do.\\n-direct-syntactic transfer-semantic transfer\\x1e\\nanalysis\\n^generation\\nSource Language TargetLanguageInterlingua\\nTheVauquois triangle ispotentially misleading, because itsuggests asimple trade-of fineffort. Itisatleast asplausi-\\nblethatthecorrect geometry isasbelow(theVauquois inverted funnel with verylong spout):\\n63-direct-syntactic transfer-semantic\\ntransfer\\nunderspeci\\x02ed semanticsresolv edlogical form\\nSource Language TargetLanguageLanguage Neutral Utterance Representation\\nThis diagram isintended toindicate thatthegoal ofproducing alanguage-neutral representation may beextremely\\ndif\\x02cult!\\nStatistical MTinvolveslearning translations from aparallel corpus :i.e.acorpus consisting ofmultiple versions of\\nasingle textindifferent languages. Theclassic workwasdone ontheproceedings oftheCanadian parliament (the\\nCanadian Hansard). Itisnecessary toalign thetexts,sothatsentences which aretranslations ofeach other arepaired:\\nthisisnon-tri vial(the mapping may notbeone-to-one). The original statistical MTapproach canbethought ofas\\ninvolving direct transfer ,with some more recent workbeing closer tosyntactic (orevensemantic) transfer .\\nExample-based MTinvolvesusing adatabase ofexisting translation pairs andtrying to\\x02nd theclosest matching\\nphrase. Itisveryuseful aspartofmachine-aided translation.\\n8.2 MTusing semantic transfer\\nSemantic transfer isanapproach toMTwhich involves:\\n1.Parsing asourcelangua gestring toproduce ameaning representation\\n2.Transforming thatrepresentation tooneappropriate forthetargetlangua ge\\n3.Generating from thetransformed representation\\nConstraint-based grammars arepotentially well suited tosemantic transfer .\\nForinstance:\\nInput: Kim singt\\nSource LF: named (x;\\x93Kim\\x94 );singen (e;x)\",\n",
              " \"1.Parsing asourcelangua gestring toproduce ameaning representation\\n2.Transforming thatrepresentation tooneappropriate forthetargetlangua ge\\n3.Generating from thetransformed representation\\nConstraint-based grammars arepotentially well suited tosemantic transfer .\\nForinstance:\\nInput: Kim singt\\nSource LF: named (x;\\x93Kim\\x94 );singen (e;x)\\nTargetLF: named (x;\\x93Kim\\x94 );sing(e;x)\\nOutput: Kim sings\\nTransfer rules:\\nsingen (e;x)$sing(e;x)\\n$indicates transfer equivalence ortranslation equivalence :thedouble arrowindicates reversibility:\\nInput: Kim sings\\nSource LF: named (x;\\x93Kim\\x94 );sing(e;x)\\nTargetLF: named (x;\\x93Kim\\x94 );singen (e;x)\\nOutput: Kim singt\\n64`named' canberegarded asalanguage-neutral predicate, sonotransfer isnecessary .Wealsoassume wedon'tchange\\nstrings like\\x93Kim\\x94.\\nSEMANTIC TRANSFER\\n*\\nj\\nPARSING\\n6\\nMORPHOLOGY\\n6\\nINPUT PROCESSING\\n6\\nsourcelanguage inputTACTICAL GENERA TION\\n?\\nMORPHOLOGY GENERA TION\\n?\\nOUTPUT PROCESSING\\n?\\ntargetlanguage output\\nSemantic transfer rules areaform ofquasi-inference: theymap between meaning representations. Obviously the\\nexample abovewastrivial: more generally some form ofmismatc hislikelytobeinvolved,although theidea of\\nsemantic transfer isthat there isless mismatch atthesemantic levelthan atasyntactic level.Semantic transfer\\ndoes notrequire thatquanti\\x02er scope beresolv ed.Semantic transfer requires detailed bidirectional grammars forthe\\nlanguages involved,which currently makesitmore suitable forhigh-precision, limited domain systems.\\nAnanaphora resolution module ispotentially needed when translating between languages likeEnglish andGerman,\\nsince English itcancorrespond toGerman er,sieores,forinstance. Buttheresolution should bedone onan`as-\\nneeded' basis, triggered bytransfer ,since itsome conte xtsthere isnoambiguity .\\nSome deplo yedMTsystems useaform ofsemantic transfer ,butsyntactic transfer ismore common. Inthese systems,\\ngeneration isusually aform oftextreconstruction, rather than `proper' tactical generation. Direct transfer isused asa\\nfallback ifsyntactic analysis fails. Systran uses amixture ofdirect transfer andsyntactic transfer: itworks reasonably\\nwell because ithasanenormous lexicon ofphrases. Handling multiw ordexpressions isamajor problem inMT.\\nStatistical MTisthecommonest approach intheresearch community ,followed bysemantic transfer .\\nAllMTsystems require some form ofWSD: potentially bigimpro vements could bemade inthisarea. One dif\\x02culty ,\\nhowever,isthatMTsystems often havetooperate with rather small amounts oftext,which limits theavailability of\\ncues.\\nDialogue systems\\n8.3 Human dialogue basics\\nTurn-taking: generally there arepoints where aspeak erinvites someone else totakeaturn (possibly choosing a\",\n",
              " \"however,isthatMTsystems often havetooperate with rather small amounts oftext,which limits theavailability of\\ncues.\\nDialogue systems\\n8.3 Human dialogue basics\\nTurn-taking: generally there arepoints where aspeak erinvites someone else totakeaturn (possibly choosing a\\nspeci\\x02c person), explicitly (e.g., byasking aquestion) orotherwise.\\nPauses: pauses between turns aregenerally veryshort (afewhundred milliseconds, buthighly culture speci\\x02c).\\nLonger pauses areassumed tobemeaningful: example from Levinson (1983: 300)\\nA:Isthere something bothering youornot? (1.0secpause)\\nA:Yesorno?(1.5secpause)\\n65A:Eh?\\nB:No.\\nTurn-taking disruption isverydif\\x02cult toadjust to.This isevident insituations such asdelays onphone lines\\nandpeople using speech prostheses, aswell asslowautomatic systems.\\nOverlap: Utterances canoverlap (theacceptability ofthisisdialect/culture speci\\x02c butunfortunately humans tend to\\ninterrupt automated systems \\x97thisisknownasbargein).\\nBackchannel: Utterances likeUh-huh ,OKcanoccur during other speak er'sutterance asasign thatthehearer is\\npaying attention.\\nAttention: The speak erneeds reassurance thatthehearer isunderstanding/paying attention. Often eyecontact is\\nenough, butthisisproblematic with telephone conversations, dark sunglasses, etc. Dialogue systems should\\ngiveexplicit feedback.\\nCooperati vity: Because participants assume theothers arecooperati ve,wegeteffects such asindirect answers to\\nquestions.\\nWhen doyouwanttoleave?\\nMymeeting starts at3pm.\\nAllofthese phenomena mean thattheproblem ofspok endialogue understanding isverycomple x.This together with\\ntheunreliability ofspeech recognition means thatspok endialogue systems arecurrently only usable forverylimited\\ninteractions.\\n8.4 Spok endialogue systems\\n1.Single initiati vesystems (also knownassystem initiati vesystems): system controls what happens when.\\nSystem: Which station doyouwanttoleavefrom?\\nUser: King' sCross\\nGenerally verylimited: forinstance, intheexample abovethesystem won'taccept anything that'snotastation\\nname. Soitwouldn' taccept either King' sCrossorLiverpool Street,depending onwhen thenexttrainto\\nCambridg eis.Designing such systems tends toinvolveHCI issues (persuading theuser nottocomplicate\\nthings), rather than language related ones.\\n2.Mixedinitiati vedialogue. Both participants cancontrol thedialogue tosome extent.\\nSystem: Which station doyouwanttoleavefrom?\\nUser: Idon'tknow,tellmewhich station Ineed forCambridge.\\nThe user hasresponded toaquestion with aquestion oftheir own, thereby taking control ofthedialogue.\\nUnfortunately ,getting systems likethistoworkproperly isverydif\\x02cult andalthough research systems anda\\nsmall number ofpractical systems havebeen built, performance isoften better ifyoudon'tallowthissortof\\ninteraction. Theterm `mixed-initiati ve'isoften used (some what misleadingly) forsystems which simply allow\\nusers tooptionally specify more than onepiece ofinformation atonce:\",\n",
              " \"Unfortunately ,getting systems likethistoworkproperly isverydif\\x02cult andalthough research systems anda\\nsmall number ofpractical systems havebeen built, performance isoften better ifyoudon'tallowthissortof\\ninteraction. Theterm `mixed-initiati ve'isoften used (some what misleadingly) forsystems which simply allow\\nusers tooptionally specify more than onepiece ofinformation atonce:\\nSystem: Which daydoyouwanttoleave?\\nUser: thetwenty-third\\nOR\\nUser: thetwenty-third ofFebruary\\n3.Dialogue tracking. Explicit dialogue models may impro veperformance inother tasks such asspok enlanguage\\nmachine translation orsummarising ahuman-to-human dialogue. Generally it'slesscritical togeteverything\\nright insuch cases, which means broader domains arepotentially realistic.\\n66The useofFSAs incontrolling dialogues wasmentioned inlecture 2.Initial versions ofsimple SDSs cannowbe\\nbuiltinafewweeks using toolkits developed byNuance andother companies: CFGs aregenerally hand-b uiltforeach\\ndialogue state. This istime-consuming, buttesting theSDS with realusers andre\\x02ning ittoimpro veperformance is\\nprobably amore serious bottleneck indeplo ying systems.\\nEmail response using deep grammars\\n8.5 Alargecoverage grammar\\nTheemail response application thatImentioned inlecture 1might beaddressed using domain-speci\\x02c grammars, but\\nunlik eindialogue systems, itismuch more dif\\x02cult tomakethelimitations inthegrammar obvious totheuser (and\\nifthecoverage isverylimited amenu-dri vensystem might well workbetter). Itistooexpensi vetomanually builda\\nnewbroad-co verage grammar foreach newapplication andgrammar induction isgenerally notfeasible because the\\ndata thatisavailable istoolimited. TheLinGO ERG constraint-based grammar mentioned inlecture 5hasbeen used\\nforparsing incommercially-deplo yedemail response systems. The grammar wasslightly tailored forthedifferent\\ndomains, butthismostly involvedadding lexical entries. TheERG hadpreviously been used ontheVerbmobil spok en\\nlanguage MTtask: theexamples belowaretakenfrom this.\\nIndication ofcoverage oftheERG:\\n1. Theweek ofthetwenty second, Ihavetwohour blocks available.\\n2. Ifyougivemeyour name andyour address wewillsend youtheticket.\\n3. Okay ,actually Iforgottosaythatwhat weneed isatwohour meeting.\\n4. Themorning isgood, butnine o'clock might bealittle toolate, asI\\nhaveaseminar atteno'clock.\\n5. Well,Iamgoing onvacation forthenexttwoweeks, sothe\\x02rstday\\nthatIwould beable tomeet would betheeighteenth\\n6. Didyousaythatyouwere freefrom three to\\x02vep.m. onWednesday ,\\nthethird, because ifsothatwould beaperfect time forme.\\nCoverage wasaround 80% onVerbmobil.\\nEf\\x02cienc y(with thePET system onan850MHz CPU):\\nItem Word Lexical Readings First All Passive\\nLength Entries Reading Readings Edges\\n1 12 33 15 150ms 270ms 1738\\n2 15 41 2 70ms 110ms 632\\n3 15 63 8 70ms 140ms 779\\n4 21 76 240 90ms 910ms 5387\\n5 26 87 300 1460 ms 8990 ms 41873\\n6 27 100 648 1080 ms 1450 ms 7850\",\n",
              " '1 12 33 15 150ms 270ms 1738\\n2 15 41 2 70ms 110ms 632\\n3 15 63 8 70ms 140ms 779\\n4 21 76 240 90ms 910ms 5387\\n5 26 87 300 1460 ms 8990 ms 41873\\n6 27 100 648 1080 ms 1450 ms 7850\\nTheERG andother similar systems havedemonstrated thatitispossible touseageneral purpose grammar inmultiple\\napplications. However,itiscrucial thatthere isafallback strate gywhen aparse fails. Foremail response, thefallback\\nistosend theemail toahuman. Reliability oftheautomated system isextremely important: sending aninappropriate\\nresponse canbeverycostly .\\nAbigdif\\x02culty foremail response isconnecting thesemantics produced bythegeneral purpose grammar tothe\\nunderlying knowledge base ordatabase. This isexpensi veinterms ofmanpo wer,butdoes notrequire much linguistic\\nexpertise. Hence, thissortofapproach ispotentially commercially viable fororganisations thathavetodeal with\\nalotoffairly routine email. Although tailoring thegrammar byadding lexical entries isnottoohard, itismuch\\nmore dif\\x02cult tomanually adjust theweights ongrammar rules andlexical entries sothatthebest parse ispreferred:\\nautomatic methods arede\\x02nitely required here. Much lesstraining data isrequired totune agrammar than toinduce\\none.\\n8.6 Further reading\\nJ&M discuss MTandspok endialogue systems.\\n67Aglossary/index ofsome oftheterms used inthelectur es\\nThis isprimarily intended tocoverconcepts which arementioned inmore than onelecture. Thelecture where theterm\\nisexplained inmost detail isgenerally indicated. Insome cases, Ihavejustgivenapointer tothesection inthelectures\\nwhere theterm isde\\x02ned. Note thatIGE stands forTheInternet Grammar ofEnglish ,http://www .ucl.ac.uk/internet-\\ngrammar/home.htm There areafewcases where thisuses aterm inaslightly different wayfrom these course notes: I\\nhavetried toindicate these.\\nactivechart Seex4.10.\\nadjecti veSeeIGE ornotes forprelecture exercises inlecture 3.\\nadjunct Seeargument andalsoIGE.\\nadverb SeeIGE ornotes forprelecture exercises inlecture 3.\\naf\\x02x Amorpheme which canonly occur inconjunction with other morphemes (lecture 2).\\nAI-complete Ahalf-joking term, applied toproblems thatwould require asolution totheproblem ofrepresenting the\\nworldandacquiring worldknowledge (lecture 1).\\nagreement Therequirement fortwophrases tohavecompatible values forgrammatical features such asnumber and\\ngender .Forinstance, inEnglish, dogsbark isgrammatical butdogbark anddogsbarks arenot. SeeIGE.\\n(lecture 5)\\nambiguity Thesame string (orsequence ofsounds) meaning different things. Contrasted with vagueness .\\nanaphora The phenomenon ofreferring tosomething thatwasmentioned previously inatext.Ananaphor isan\\nexpression which does this, such asapronoun (seex7.5).\\nantonymy Opposite meaning: such asclean anddirty (x6.5).\\nargument Insyntax, thephrases which arelexically required tobepresent byaparticular word(prototypically a\\nverb). This isasopposed toadjunct s,which modify awordorphrase butarenotrequired. Forinstance, in:',\n",
              " \"antonymy Opposite meaning: such asclean anddirty (x6.5).\\nargument Insyntax, thephrases which arelexically required tobepresent byaparticular word(prototypically a\\nverb). This isasopposed toadjunct s,which modify awordorphrase butarenotrequired. Forinstance, in:\\nKim sawSandy onTuesday\\nSandy isanargument butonTuesday isanadjunct. Arguments arespeci\\x02ed bythesubcategorization ofaverb\\netc.Also seetheIGE. (lecture 5)\\naspect Aterm used tocoverdistinctions such aswhether averbsuggests aneventhasbeen completed ornot(as\\nopposed totense, which refers tothetime ofanevent). Forinstance, shewas writing abook vsshewrotea\\nbook .\\nback offUsually used torefer totechniques fordealing with data sparseness inprobabilistic systems: using amore\\ngeneral classi\\x02cation rather than amore speci\\x02c one. Forinstance, using unigram probabilities instead of\\nbigrams; using wordclasses instead ofindividual words (lecture 3).\\nbaseline Inevaluation, theperformance produced byasimple system against which theexperimental technique is\\ncompared (x3.6).\\nbidir ectional Usable forboth analysis andgeneration (lecture 2).\\ncase Distinctions between nominals indicating their syntactic roleinasentence. InEnglish, some pronouns showa\\ndistinction: e.g., sheisused forsubjects, while herisused forobjects. e.g., shelikeshervs*herlikesshe.\\nLanguages such asGerman andLatin mark case much more extensi vely.\\nceiling Inevaluation, theperformance produced bya`perfect' system (such ashuman annotation) against which the\\nexperimental technique iscompared (x3.6).\\nchart parsing Seex4.6.\\n68Chomsk yNoam Chomsk y,professor atMIT.Thefounder ofmodern theories ofsyntax inlinguistics.\\nclosed class Refers toparts ofspeech, such asconjunction, forwhich allthemembers could potentially beenumerated\\n(lecture 3).\\ncoher ence Seex7.2\\ncollocation Seex6.10\\ncomplement Forthepurposes ofthiscourse, anargument other than thesubject.\\ncompositionality The idea thatthemeaning ofaphrase isafunction ofthemeaning ofitsparts. compositional\\nsemantics isthestudy ofhowmeaning canbebuiltupbysemantic rules which mirror syntactic structure\\n(lecture 6).\\nconstituent Asequence ofwords which isconsidered asaunitinaparticular grammar (lecture 4).\\nconstraint-based grammar Aformalism which describes alanguage using asetofindependently stated constraints,\\nwithout imposing anyconditions onprocessing orprocessing order (lecture 5).\\ncontext Thesituation inwhich anutterance occurs: includes prior utterances, thephysical environment, background\\nknowledge ofthespeak erandhearer(s), etcetc.\\ncorpus Abody oftextused inexperiments (plural corpor a).Seex3.1.\\ncuephrases Phrases which indicates particular rhetorical relations .\\ndenominal Something derivedfrom anoun: e.g., theverbtango isadenominal verb.\\nderivational morphology Seex2.2\\ndeterminer SeeIGE ornotes forprelecture exercises inlecture 3.\",\n",
              " \"knowledge ofthespeak erandhearer(s), etcetc.\\ncorpus Abody oftextused inexperiments (plural corpor a).Seex3.1.\\ncuephrases Phrases which indicates particular rhetorical relations .\\ndenominal Something derivedfrom anoun: e.g., theverbtango isadenominal verb.\\nderivational morphology Seex2.2\\ndeterminer SeeIGE ornotes forprelecture exercises inlecture 3.\\ndeverbal Something derivedfrom averb:e.g., theadjecti vesurprised .\\ndirectobject SeeIGE. Contrast indir ectobject .\\ndiscourse InNLP,apiece ofconnected text.\\ndiscourse relations Seerhetorical relations .\\ndomain Notaprecise term, butIuseittomean some restricted setofknowledge appropriate foranapplication.\\nerroranalysis Inevaluation, working outwhat sortoferrors arefound foragivenapproach (x3.6).\\nfeatur estructur eSeeLecture 5.\\nfull-f orm lexicon Alexicon where allmorphological variants areexplicitly listed (lecture 2).\\ngeneration Theprocess ofconstructing strings from some input representation. Withbidirectional grammars using\\ncompositional semantics, generation canbesplit intostrategic generation ,which istheprocess ofdeciding on\\nthelogical form (also knownastextplanning ),andtactical generation which istheprocess ofgoing from the\\nlogical form tothestring (also knownasrealization ).x6.2.\\ngenerati vegrammar Thefamily ofapproaches tolinguistics where anatural language istreated asgoverned byrules\\nwhich canproduce allandonly thewell-formed utterances. Lecture 4.\\ngrammar Formally ,inthegenerati vetradition, thesetofrules andthelexicon. Lecture 4.\\nhead Insyntax, themost important element ofaphrase.\\nhear erAnyone ontherecei ving endofanutterance (spok en,written orsigned).x1.3.\\nhomonymy Instances ofpolysemy where thetwosenses areunrelated (x6.8).\\n69hyponymy An`IS-A 'relationship (x6.4)More general terms arehyper nym s,more speci\\x02c hyponym s.\\nindir ectobject Thebene\\x02ciary inverbphrases likegive apresent toSandy orgive Sandy apresent .Inthiscase the\\nindirect object isSandy andthedirectobject isapresent .\\ninterannotator agreement Thedegree ofagreement between thedecisions oftwoormore humans with respect to\\nsome categorisation (x3.6).\\nlanguage model Aterm generally used inspeech recognition, forastatistical model ofanatural language (lecture 3).\\nlemmatization Finding thestem andaf\\x02xesforwords (lecture 2).\\nlexical ambiguity Ambiguity caused because ofmultiple senses foraword.\\nlexicon ThepartofanNLP system thatcontains information about individual words (lecture 1).\\nlinking Relating syntax andsemantics inlexical entries (x6.1).\\nlocal ambiguity Ambiguity thatarises during analysis etc,butwhich will beresolv edwhen theutterance iscom-\\npletely processed.\\nlogical form Thesemantic representation constructed foranutterance (x6.1).\\nmeaning postulates Inference rules thatcapture some aspects ofthemeaning ofaword.\\nmeronymy The`part-of 'lexical semantic relation (x6.5).\",\n",
              " \"local ambiguity Ambiguity thatarises during analysis etc,butwhich will beresolv edwhen theutterance iscom-\\npletely processed.\\nlogical form Thesemantic representation constructed foranutterance (x6.1).\\nmeaning postulates Inference rules thatcapture some aspects ofthemeaning ofaword.\\nmeronymy The`part-of 'lexical semantic relation (x6.5).\\nmorpheme Minimal information carrying units within aword(x2.1).\\nmorphology Seex1.2\\nMT Machine translation\\nmultiw ordexpr ession Aconventional phrase thathassomething idiosyncratic about itandtherefore might belisted\\ninadictionary .\\nmumble input Anyunrecognised input inaspok endialogue system (lecture 2).\\nn-gram Asequence ofnwords (x3.2).\\nnamed entity recognition Recognition andcategorisation ofperson names, names ofplaces, dates etc(lecture 4).\\nnoun SeeIGE ornotes forprelecture exercises inlecture 3.\\nnoun phrase (NP) Aphrase which hasanoun assyntactic head .SeeIGE.\\nontology InNLP andAI,aspeci\\x02cation oftheentities inaparticular domain and(sometimes) therelationships\\nbetween them. Often hierarchically structured.\\nopen class Opposite ofclosed class .\\northographic rules spelling rules (x2.3)\\novergenerate Ofagrammar ,toproduce strings which areinvalid, e.g., because theyarenotgrammatical according\\ntohuman judgements.\\npacking Seex4.9\\npassi vechart parsing Seex4.7\\nparse treeSeex4.4\\npart ofspeech Themain syntactic categories: noun, verb,adjecti ve,adverb,preposition, conjunction etc.\\n70part ofspeech tagging Automatic assignment ofsyntactic categories tothewords inatext.The setofcategories\\nused isactually generally more \\x02ne-grained than traditional parts ofspeech.\\npolysemy Thephenomenon ofwords having different senses (x6.8).\\npragmatics Seex1.2\\npredicate Inlogic, something thattakeszero ormore arguments andreturns atruth value. (Used inIGE fortheverb\\nphrase following thesubject inasentence, butIdon'tusethatterminology .)\\npre\\x02x Anaf\\x02x thatprecedes thestem .\\nprobabilistic context freegrammars (PCFGs) CFGs with probabilities associated with rules (lecture 4).\\nrealization Another term fortactical generation \\x97seegeneration .\\nreferring expr ession Seex7.5\\nrelativeclause SeeIGE.\\nArestricti verelativeclause isonewhich limits theinterpretation ofanoun toasubset: e.g.thestudents who\\nsleep inlectur esareobviously overworking refers toasubset ofstudents. Contrast non-r estricti ve,which isa\\nform ofparenthetical comment: e.g. thestudents, who sleep inlectur es,areobviously overworking means all\\n(ornearly all)aresleeping.\\nselectional restrictions Constraints onthesemantic classes ofarguments toverbs etc(e.g., thesubject ofthink is\\nrestricted tobeing sentient). Theterm selectional preference isused fornon-absolute restrictions.\\nsemantics Seex1.2\\nsign Asused inlecture 5,thebundle ofproperties representing awordorphrase.\\nsmoothing Redistrib uting observ edprobabilities toallowforsparse data ,especially togiveanon-zero probability\",\n",
              " \"selectional restrictions Constraints onthesemantic classes ofarguments toverbs etc(e.g., thesubject ofthink is\\nrestricted tobeing sentient). Theterm selectional preference isused fornon-absolute restrictions.\\nsemantics Seex1.2\\nsign Asused inlecture 5,thebundle ofproperties representing awordorphrase.\\nsmoothing Redistrib uting observ edprobabilities toallowforsparse data ,especially togiveanon-zero probability\\ntounseen events(lecture 2).\\nsparse data Especially instatistical techniques, data concerning rareeventswhich isn'tadequate togivegood proba-\\nbility estimates (lecture 2).\\nspeak erSomeone who makesanutterance (x1.3).\\nspelling rulesx2.3\\nstem Amorpheme which isacentral component ofaword(contrast af\\x02x ).x2.1.\\nstemming Stripping af\\x02x es(seex2.4).\\nstrongequivalence Ofgrammars, accepting/rejecting exactly thesame strings andassigning thesame brack etings\\n(contrast weak equivalence ).Lecture 4.\\nstructural ambiguity Thesituation where thesame string corresponds tomultiple brack etings.\\nsubcategorization Thelexical property thattells ushowmanyargument saverbetccanhave.\\nsuf\\x02x Anaf\\x02x thatfollowsthestem .\\nsummarization Producing ashorter piece oftext(orspeech) thatcaptures theessential information intheoriginal.\\nsynonymy Having thesame meaning (x6.5).\\nsyntax Seex1.2\\ntaxonomy Traditionally ,thescheme ofclassi\\x02cation ofbiological organisms. Extended inNLP tomean ahierarchical\\nclassi\\x02cation ofwordsenses. Theterm ontology issometimes used inarather similar way,butontologies tend\\ntobeclassi\\x02cations ofdomain-kno wledge, without necessarily having adirect linktowords, andmay havea\\nricher structure than ataxonomy .\\n71template Infeature structure grammars, see5.6\\ntense Past,present, future etc.\\ntextplanning Another term forstrategic generation :seegeneration .\\ntraining data Data used totrain anysortofmachine-learning system. Must beseparated from testdata which iskept\\nunseen. Manually-constructed systems should ideally alsousestrictly unseen data forevaluation.\\ntransfer InMT,theprocess ofgoing from arepresentation appropriate totheoriginal (source) language toone\\nappropriate forthetargetlanguage.\\ntreebank acorpus annotated with trees (lecture 4).\\nuni\\x02cation SeeLecture 5,especiallyx5.3.\\nweak equivalence Ofgrammars, accepting/rejecting exactly thesame strings (contrast strongequivalence ).Lecture\\n4.\\nWizard ofOzexperiment Anexperiment where data iscollected, generally foradialogue system, byasking users\\ntointeract with amock-up ofarealsystem, where some orallofthe`processing' isactually being done bya\\nhuman rather than automatically .\\nWordNet Seex6.6\\nword-sense disambiguation Seex6.9\\nutterance Apiece ofspeech ortext(sentence orfragment) generated byaspeak erinaparticular conte xt.\\nverb SeeIGE ornotes forprelecture exercises inlecture 3.\\nverbphrase (VP) Aphrase headed byaverb.\\n72Exer cises forNLP course, 2004\\nNotes onexercises\\nThese exercises areorganised bylecture. Theyaredivided intotwoclasses: prelecture andpostlecture. Theprelecture\",\n",
              " \"utterance Apiece ofspeech ortext(sentence orfragment) generated byaspeak erinaparticular conte xt.\\nverb SeeIGE ornotes forprelecture exercises inlecture 3.\\nverbphrase (VP) Aphrase headed byaverb.\\n72Exer cises forNLP course, 2004\\nNotes onexercises\\nThese exercises areorganised bylecture. Theyaredivided intotwoclasses: prelecture andpostlecture. Theprelecture\\nexercises areintended toreviewthebasic concepts thatyou'llneed tofully understand thelecture. Depending onyour\\nbackground, youmay \\x02ndthese trivialoryoumay need toread thenotes, butineither case theyshouldn' ttakemore\\nthan afewminutes. The\\x02rstoneortwoexamples generally come with answers, other answers areattheend(where\\nappropriate).\\nAnswers tothepostlecture exercises arewith thesupervision notes (where appropriate). These aremostly intended as\\nquick exercises tocheck understanding ofthelecture, though some aremore open-ended.\\nALectur e1\\nA.1 Postlectur eexercises\\nIfyouuseawordprocessor with aspelling andgrammar check er,trylooking atitstreatment ofagreement andsome of\\ntheother phenomena discussed inthelecture. Ifpossible, tryswitching settings between British English andAmerican\\nEnglish.\\nBLectur e2\\nB.1 Prelectur eexercises\\n1.Split thefollowing words into morphological units, labelling each asstem, suf\\x02xorpre\\x02x. Ifthere isany\\nambiguity ,giveallpossible splits.\\n(a)dries\\nanswer: dry(stem), -s(suf\\x02x)\\n(b)cartwheel\\nanswer: cart(stem), wheel (stem)\\n(c)carries\\n(d)running\\n(e)uncaring\\n(f)intruders\\n(g)bookshelv es\\n(h)reattaches\\n(i)anticipated\\n2.Listthesimple pastandpast/passi veparticiple forms ofthefollowing verbs:\\n(a)sing\\nAnswer: simple pastsang ,participle sung\\n(b)carry\\n(c)sleep\\n(d)see\\nNote thatthesimple pastisused byitself (e.g., Kim sang well)while theparticiple form isused with anauxiliary (e.g.,\\nKim hadsung well).Thepassi veparticiple isalwaysthesame asthepast participle inEnglish: (e.g., Kim beganthe\\nlectur eearly ,Kim hadbegunthelectur eearly ,Thelectur ewasbegunearly ).\\n73B.2 Post-lectur eexercises\\n1.Foreach ofthefollowing surfaceforms, givealistofthestates thattheFST giveninthelecture notes for\\ne-insertion passes through, andthecorresponding underlying forms:\\n(a)cats\\n(b)corpus\\n(c)asses\\n(d)assess\\n(e)axes\\n2.Modify theFSA fordates sothatitonly accepts validmonths. Turnyour revised FSA intoaFST which maps\\nbetween thenumerical representation ofmonths andtheir abbre viations (Jan ...Dec).\\nCLectur e3\\nC.1 Pre-lectur e\\nLabel each ofthewords inthefollowing sentences with their partofspeech, distinguishing between nouns, proper\",\n",
              " \"(e)axes\\n2.Modify theFSA fordates sothatitonly accepts validmonths. Turnyour revised FSA intoaFST which maps\\nbetween thenumerical representation ofmonths andtheir abbre viations (Jan ...Dec).\\nCLectur e3\\nC.1 Pre-lectur e\\nLabel each ofthewords inthefollowing sentences with their partofspeech, distinguishing between nouns, proper\\nnouns, verbs, adjecti ves,adverbs, determiners, prepositions, pronouns andothers. (Traditional classi\\x02cations often\\ndistinguish between alargenumber ofadditional parts ofspeech, butthe\\x02ner distinctions won'tbeimportant here.)\\nThere arenotes onpartofspeech distinctions below,ifyouhaveproblems.\\n1.Thebrownfoxcould jump quickly overthedog, Rover.Answer: The/Det brown/Adj fox/Noun could/V erb(modal)\\njump/V erbquickly/Adv erbover/Preposition the/Determiner dog/Noun, Rover/Proper noun.\\n2.Thebigcatchased thesmall dogintothebarn.\\n3.Those barns haveredroofs.\\n4.Dogs often bark loudly .\\n5.Further discussion seems useless.\\n6.Kim didnotlikehim.\\n7.Time\\x03ies.\\nNotes onparts ofspeech. These notes areEnglish-speci\\x02c andarejustintended tohelp with thelectures andtheexer-\\ncises: seealinguistics textbook forde\\x02nitions! Some categories havefuzzy boundaries, butnone ofthecomplicated\\ncases willbeimportant forthiscourse.\\nNoun prototypically ,nouns refer tophysical objects orsubstances: e.g., aardvark ,chainsaw ,rice.Buttheycanalso\\nbeabstract (e.g. truth ,beauty )orrefer toevents, states orprocesses (e.g., decision ).IfyoucansaytheXand\\nhaveasensible phrase, that'sagood indication thatXisanoun.\\nPronoun something thatcanstand inforanoun: e.g., him,his\\nProper noun /Proper name aname ofaperson, place etc:e.g., Elizabeth ,Paris\\nVerb Verbs refer toevents, processes orstates butsince nouns andadjecti vescandothisaswell, thedistinction\\nbetween thecategories isbased ondistrib ution, notsemantics. Forinstance, nouns canoccur with determiners\\nlikethe(e.g., thedecision )whereas verbs can't(e.g., *thedecide ).InEnglish, verbs areoften found with\\nauxiliaries (be,have ordo)indicating tense andaspect, andsometime occur with modals, likecan,could etc.\\nAuxiliaries andmodals arethemselv esgenerally treated assubclasses ofverbs.\\n74Adjecti veawordthatmodi\\x02es anoun: e.g., big,loud.Most adjecti vescanalso occur after theverbbeandafew\\nother verbs: e.g., thestudents areunhappy .Numbers aresometimes treated asatype ofadjecti vebylinguists\\nbutgenerally giventheir owncategory intraditional grammars. Pastparticiple forms ofverbs canalsooften be\\nused asadjecti ves(e.g., worried inthevery worried man).Sometimes it'simpossible totellwhether something\\nisaparticiple oranadjecti ve(e.g., theman wasworried ).\\nAdverb awordthatmodi\\x02es averb:e.g.quickly,probably .\\nDeterminer these precede nouns e.g., the,every,this.Itisnotalwaysclear whether awordisadeterminer orsome\\ntype ofadjecti ve.\",\n",
              " 'isaparticiple oranadjecti ve(e.g., theman wasworried ).\\nAdverb awordthatmodi\\x02es averb:e.g.quickly,probably .\\nDeterminer these precede nouns e.g., the,every,this.Itisnotalwaysclear whether awordisadeterminer orsome\\ntype ofadjecti ve.\\nPreposition e.g., in,at,with\\nNouns, proper nouns, verbs, adjecti vesandadverbs aretheopen classes :newwords canoccur inanyofthese cate-\\ngories. Determiners, prepositions andpronouns areclosed classes (asareauxiliary andmodal verbs).\\nC.2 Post-lectur e\\nTryoutoneormore ofthefollowing POS tagging sites:\\nhttp://www.coli.uni-sb.de/\\x98thorsten/tnt/\\nhttp://www.comp.lancs.ac.uk/computing/research/ucrel/claws/trial.ht ml\\nhttp://l2r.cs.uiuc.edu/\\x98cogcomp/eoh/posdemo.html\\nOnly the\\x02rst siteuses anapproach comparable tothatdescribed inthelecture. Find twoshort pieces ofnaturally\\noccurring English text,oneofwhich youthink should berelati velyeasy totagcorrectly andonewhich youpredict to\\nbedif\\x02cult. Look atthetagged output andestimate thepercentage ofcorrect tags ineach case, concentrating onthe\\nopen-class words. Youmight liketogetanother student tolook atthesame output andseeifyouagree onwhich tags\\narecorrect.\\nDLectur e4\\nD.1Pre-lectur e\\nPutbrack etsround thenoun phrases andtheverbphrases inthefollowing sentences (ifthere isambiguity ,givetwo\\nbrack etings):\\n1.Thecatwith white furchased thesmall dogintothebarn.\\nAnswer: ((The cat)npwith (white fur)np)npchased (thesmall dog)npinto(thebarn)np\\nThecatwith white fur(chased thesmall dogintothebarn)vp\\n2.Thebigcatwith black furchased thedogwhich barked.\\n3.Three dogs barkedathim.\\n4.Kim sawthebirdw atcher with thebinoculars.\\nNote that noun phrases consist ofthenoun, thedeterminer (ifpresent) andanymodi\\x02ers ofthenoun (adjecti ve,\\nprepositional phrase, relati veclause). This means thatnoun phrases may benested. Verbphrases include theverband\\nanyauxiliaries, plus theobject andindirect object etc(ingeneral, thecomplements oftheverb\\x97discussed inlecture\\n5)andanyadverbial modi\\x02ers. Theverbphrase does notinclude thesubject.\\n75D.2Post-lectur e\\nUsing theCFG giveninthelecture notes (section 4.3):\\n1.showtheedges generated when parsing they\\x02shinriver sinDecember with thesimple chart parser in4.7\\n2.showtheedges generated forthissentence ifpacking isused (asdescribed in4.9)\\n3.showtheedges generated forthey\\x02shinriver sifanactivechart parser isused (asin4.10)\\nELectur e5\\nE.1 Pre-lectur e\\n1.Averysimple form ofsemantic representation corresponds tomaking verbs one-, two-orthree- place logical\\npredicates. Proper names areassumed tocorrespond toconstants. The\\x02rstargument should alwayscorrespond\\ntothesubject oftheactivesentence, thesecond totheobject (ifthere isone) andthethird totheindirect object\\n(i.e., thebene\\x02ciary ,ifthere isone). Giverepresentations forthefollowing examples:\\n(a)Kim likesSandy\\nAnswer: like(Kim, Sandy)\\n(b)Kim sleeps\\n(c)Sandy adores Kim',\n",
              " 'tothesubject oftheactivesentence, thesecond totheobject (ifthere isone) andthethird totheindirect object\\n(i.e., thebene\\x02ciary ,ifthere isone). Giverepresentations forthefollowing examples:\\n(a)Kim likesSandy\\nAnswer: like(Kim, Sandy)\\n(b)Kim sleeps\\n(c)Sandy adores Kim\\n(d)Kim isadored bySandy (note, thisispassi ve:thebyshould notberepresented)\\n(e)Kim gaveRovertoSandy (thetoisnotrepresented)\\n(f)Kim gaveSandy Rover\\n2.Listthree verbs thatareintransiti veonly,three which aresimple transiti veonly,three which canbeintransiti ve\\nortransiti veandthree which areditransiti ves.\\nThedistinction between intransiti ve,transiti veandditransiti veverbs canbeillustrated byexamples such as:\\nsleep \\x97intransiti ve.Noobject is(generally) possible: *Kim slept theevening .\\nadore \\x97transiti ve.Anobject isoblig atory: *Kim adored.\\ngive\\x97-ditransiti ve.These verbs haveanobject andanindirect object. Kim gave Sandy anapple (orKim gave\\nanapple toSandy ).\\nE.2 Post-lectur e\\n1.Givetheuni\\x02cation ofthefollowing feature structures:\\n(a)\\x14\\nCAT\\x02\\x03\\nAGRpl\\x15\\nuni\\x02ed with\\x14\\nCATVP\\nAGR\\x02\\x03\\x15\\n(b)2\\n66666664MOTHER\\x14\\nCATVP\\nAGR1\\x15\\nDTR1\\x14\\nCATV\\nAGR1\\x15\\nDTR2\\x14\\nCATNP\\nAGR\\x02\\x03\\x153\\n77777775uni\\x02ed with\"\\nDTR1\\x14\\nCATV\\nAGRsg\\x15#\\n(c)\\x14\\nF1\\nG 1\\x15\\nuni\\x02ed with2\\n64F\\x02\\nJa\\x03\\nG\\x14\\nJ\\x02\\x03\\nKb\\x153\\n75\\n76(d)\\x14\\nF1a\\nG 1\\x15\\nuni\\x02ed with\\x02\\nGb\\x03\\n(e)\\x14\\nF1\\nG 1\\x15\\nuni\\x02ed with2\\n64F\\x02\\nJa\\x03\\nG\\x14\\nJb\\nKb\\x153\\n75\\n(f)\\x14\\nF\\x02\\nG 1\\x03\\nH 1\\x15\\nuni\\x02ed with\\x14\\nF1\\nH 1\\x15\\n(g)2\\n4F1\\nG 1\\nH 2\\nJ23\\n5uni\\x02ed with\\x14\\nF1\\nJ1\\x15\\n(h)\\x14\\nF\\x02\\nG 1\\x03\\nH 1\\x15\\nuni\\x02ed with\\x14\\nF2\\nH\\x02\\nJ2\\x03\\x15\\n2.Add case totheinitial FSgrammar inorder topreventsentences such astheycantheyfrom parsing.\\n3.Workthough parses ofthefollowing strings forthesecond FSgrammar ,deciding whether theyparse ornot:\\n(a)\\x02sh\\x02sh\\n(b)theycan\\x02sh\\n(c)it\\x02sh\\n(d)theycan\\n(e)they\\x02shit\\n4.Modify thesecond FSgrammar toallowforverbs which taketwocomplements. Also addalexical entry for\\ngive (just dothevariant which takestwonoun phrases).\\nFLectur e6\\nF.1Pre-lectur e\\nWithout looking atadictionary ,write downbrief de\\x02nitions forasmanysenses asyoucanthink offorthefollowing\\nwords:\\n1.plant\\n2.shower\\n3.bass\\nIfpossible, compare your answers with another student\\' sandwith adictionary .\\nF.2Post-lectur e\\n1.Ifyoudidtheexercise associated with theprevious lecture toaddditransiti veverbs tothegrammar ,amend your\\nmodi\\x02ed grammar sothatitproduces semantic representations.\\n2.Givehypern yms and(ifpossible) hypon yms forthenominal senses ofthefollowing words:',\n",
              " \"words:\\n1.plant\\n2.shower\\n3.bass\\nIfpossible, compare your answers with another student' sandwith adictionary .\\nF.2Post-lectur e\\n1.Ifyoudidtheexercise associated with theprevious lecture toaddditransiti veverbs tothegrammar ,amend your\\nmodi\\x02ed grammar sothatitproduces semantic representations.\\n2.Givehypern yms and(ifpossible) hypon yms forthenominal senses ofthefollowing words:\\n(a)horse\\n(b)rice\\n(c)curtain\\n3.List some possible seeds forYarowsky'salgorithm thatwould distinguish between thesenses ofshower and\\nbass thatyougaveintheprelecture exercise.\\n77GLectur e7\\nG.1 Pre-lectur e\\nNosuggested exercises.\\nG.2 Post-lectur e\\nWorkthrough theLappin andLeass algorithm with ashort piece ofnaturally occurring text.Ifthere arecases where\\nthealgorithm getstheresults wrong, suggest thesorts ofknowledge thatwould beneeded togivethecorrect answer .\\nHLectur e8\\nH.1 Exer cises (pre-orpost- lectur e)\\nIfyouhaveneverused aspok endialogue system, tryoneout.One example isBritish Airw ays\\x03ight arrivals(0870 551\\n1155 \\x97chargedatstandard national rate). Think ofarealistic taskbefore youphone: forinstance, to\\x02ndarrivaltimes\\nforamorning \\x03ight from Edinb urghtoHeathro w.Another example istheTransco Meter Helpline (0870 6081524 \\x97\\nstandard national rate) which tells customers who their gassupplier is.This system uses averysimple dialogue model\\nbutallowsforinterruptions etc.\\nUse Systran (viahttp://world.altavista.com/ )totranslate some textandinvestig atewhether thetextit\\noutputs isgrammatical andwhether itdeals well with issues discussed inthecourse, such aslexical ambiguity and\\npronoun resolution. Ideally youwould getthehelp ofsomeone who speaks alanguage other than English forthisif\\nyou'renotfairly \\x03uent inanother language yourself: thelanguage pairs thatSystran deals with arelisted onthesite.\\n78IAnswers tosome ofthepre-lectur eexercises\\nI.1 Lectur e2\\n1.(a)carries\\ncarry (stem) s(suf\\x02x)\\n(b)running\\nrun(stem) ing(suf\\x02x)\\n(c)uncaring\\nun(pre\\x02x) care (stem) ing(suf\\x02x)\\n(d)intruders\\nintrude (stem) er(suf\\x02x)s(suf\\x02x)\\nNote thatin-isnotarealpre\\x02x here\\n(e)bookshelv es\\nbook (stem) shelf (stem) s(suf\\x02x)\\n(f)reattaches\\nre(pre\\x02x) attach (stem) s(suf\\x02x)\\n(g)anticipated\\nanticipate (stem) ed(suf\\x02x)\\n2.(a)carry\\nAnswer: simple pastcarried ,pastparticiple carried\\n(b)sleep\\nAnswer: simple pastslept ,pastparticiple slept\\n(c)see\\nAnswer: simple pastsaw,pastparticiple seen\\nI.2 Lectur e3\\n1.The/Det big/Adj cat/Noun chased/V erbthe/Det small/Adj dog/Noun into/Prep the/Det barn/Noun.\",\n",
              " '2.(a)carry\\nAnswer: simple pastcarried ,pastparticiple carried\\n(b)sleep\\nAnswer: simple pastslept ,pastparticiple slept\\n(c)see\\nAnswer: simple pastsaw,pastparticiple seen\\nI.2 Lectur e3\\n1.The/Det big/Adj cat/Noun chased/V erbthe/Det small/Adj dog/Noun into/Prep the/Det barn/Noun.\\n2.Those/Det barns/Noun have/Verbred/Adj roofs/Noun.\\n3.Dogs/Noun often/Adv erbbark/V erbloudly/Adv erb.\\n4.Further/Adj discussion/Noun seems/V erbuseless/Adj.\\n5.Kim/Proper noun did/V erb(aux) not/Adv erb(or Other) like/Verbhim/Pronoun.\\n6.Time/Noun \\x03ies/V erb.\\nTime/V erb\\x03ies/Noun. (theimperati ve!)\\nI.3 Lectur e4\\n1.Thebigcatwith black furchased thedogwhich barked.\\n((The bigcat)npwith (black fur)np)npchased (thedogwhich barked)np\\nThebigcatwith black fur(chased thedogwhich barked)vp\\n2.Three dogs barkedathim. (Three dogs)npbarkedat(him)npThree dogs (bark edathim)vp\\n3.Kim sawthebirdw atcher with thebinoculars.\\nAnalysis 1(thebirdw atcher hasthebinoculars) (Kim)npsaw((the birdw atcher)npwith (thebinoculars) np)np\\nKim (sawthebirdw atcher with thebinoculars) vp\\nAnalysis 2(theseeing waswith thebinoculars) (Kim)npsaw(thebirdw atcher)npwith (thebinoculars) np\\nKim (sawthebirdw atcher with thebinoculars) vp\\n79I.4 Lectur e5\\n1.Kim sleeps\\nsleep(Kim)\\n2.Sandy adores Kim\\nadore(Sandy ,Kim)\\n3.Kim isadored bySandy\\nadore(Sandy ,Kim)\\n4.Kim gaveRovertoSandy\\ngive(Kim, Rover,Sandy)\\n5.Kim gaveSandy Rover\\ngive(Kim, Rover,Sandy)\\nSome examples ofdifferent classes ofverb(obviously youhavealmost certainly come upwith different ones!)\\nsleep, snore, sneeze, cough \\x97intransiti veonly\\nadore, comb, rub\\x97simple transiti veonly eat,wash,shave,dust \\x97transiti veorintransiti ve\\ngive,hand, lend \\x97ditransiti ve\\n80']"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GX5BECsdSUUM",
        "outputId": "f8de8684-e55d-4a4d-b0a6-6ed4833a00d6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Inserted 50 headlines.\n"
          ]
        }
      ],
      "source": [
        "# Load the dataset into the vector store\n",
        "astra_vs.add_texts(texts[:50])\n",
        "# Print the number of headlines inserted\n",
        "print(\"Inserted %i headlines.\" % len(texts[:50]))\n",
        "# Create an index wrapper for the vector store\n",
        "# creating an index wrapper improves the efficiency of searching and retrieving similar vectors\n",
        "astra_v_index = VectorStoreIndexWrapper(vectorstore=astra_vs)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**6.  Run the QA cycle**\n",
        "\n",
        "Run the cells and ask a question -- or `exit` to stop. (you can also stop execution with the \"▪\" button on the top toolbar)\n",
        "\n",
        "Sample questions questions:\n",
        "- What distinguished NLP from other data processing applications?\n",
        "- What makes NLP applications unique?\n",
        "- List some NLP applications\n"
      ],
      "metadata": {
        "id": "pbCMIHkRV1lE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b2c6576f-6274-478f-fefd-05bc863c0bae",
        "id": "Zb_LFqXxDIWV"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Enter your question (or type 'quit' to exit): What distinguished NLP from other data processing applications?\n",
            "\n",
            "QUESTION: \"What distinguished NLP from other data processing applications?\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:cassandra.protocol:Server warning: Top-K queries can only be run with consistency level ONE / LOCAL_ONE / NODE_LOCAL. Consistency level LOCAL_QUORUM was requested. Downgrading the consistency level to LOCAL_ONE.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ANSWER: \"NLP applications require a lot of hand-coded linguistic knowledge, and researchers were interested in developing symbolic and statistical techniques for processing and understanding language.\"\n",
            "\n",
            "FIRST DOCUMENTS BY RELEVANCE:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:cassandra.protocol:Server warning: Top-K queries can only be run with consistency level ONE / LOCAL_ONE / NODE_LOCAL. Consistency level LOCAL_QUORUM was requested. Downgrading the consistency level to LOCAL_ONE.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    [0.9349] \"forprocessing, with some researchers downplaying syntax, inparticular ,infavourofwor ...\"\n",
            "    [0.9346] \"forprocessing, with some researchers downplaying syntax, inparticular ,infavourofwor ...\"\n",
            "    [0.9346] \"forprocessing, with some researchers downplaying syntax, inparticular ,infavourofwor ...\"\n",
            "    [0.9311] \"centrating onthe`agent-lik e'applications andneglecting theuser aids. Although thesy ...\"\n",
            "\n",
            "What's your next question (or type 'quit' to exit): List some NLP applications\n",
            "\n",
            "QUESTION: \"List some NLP applications\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:cassandra.protocol:Server warning: Top-K queries can only be run with consistency level ONE / LOCAL_ONE / NODE_LOCAL. Consistency level LOCAL_QUORUM was requested. Downgrading the consistency level to LOCAL_ONE.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ANSWER: \"NLP applications include document classification, information extraction, reading comprehension, translation, summarization, knowledge acquisition, question-answering, conversation agents, tutoring systems, problem solving, speech processing, and language generation.\"\n",
            "\n",
            "FIRST DOCUMENTS BY RELEVANCE:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:cassandra.protocol:Server warning: Top-K queries can only be run with consistency level ONE / LOCAL_ONE / NODE_LOCAL. Consistency level LOCAL_QUORUM was requested. Downgrading the consistency level to LOCAL_ONE.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    [0.9401] \"Modern software development methods play an important role.Applications of NLP\n",
            "•Text ...\"\n",
            "    [0.9401] \"Modern software development methods play an important role.Applications of NLP\n",
            "•Text ...\"\n",
            "    [0.9401] \"Modern software development methods play an important role.Applications of NLP\n",
            "•Text ...\"\n",
            "    [0.9368] \"1Natural Language Processing\n",
            "CS 6320  \n",
            "Lecture 1\n",
            "Introduction to NLP\n",
            "Instructor: San ...\"\n",
            "\n",
            "What's your next question (or type 'quit' to exit): exit\n"
          ]
        }
      ],
      "source": [
        "my_question = True\n",
        "while True:\n",
        "    if my_question:\n",
        "        query_text = input(\"\\nEnter your question (or type 'quit' to exit): \").strip()\n",
        "    else:\n",
        "        query_text = input(\"\\nWhat's your next question (or type 'quit' to exit): \").strip()\n",
        "\n",
        "    if query_text.lower() == \"exit\":\n",
        "        break\n",
        "\n",
        "    if query_text == \"\":\n",
        "        continue\n",
        "\n",
        "    my_question = False\n",
        "\n",
        "    print(\"\\nQUESTION: \\\"%s\\\"\" % query_text)\n",
        "    answer = astra_v_index.query(query_text, llm=llm).strip()\n",
        "    print(\"ANSWER: \\\"%s\\\"\\n\" % answer)\n",
        "\n",
        "    print(\"FIRST DOCUMENTS BY RELEVANCE:\")\n",
        "    for doc, score in astra_vs.similarity_search_with_score(query_text, k=4):\n",
        "        print(\"    [%0.4f] \\\"%s ...\\\"\" % (score, doc.page_content[:84]))"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "E93iJ7oXGE3H"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}